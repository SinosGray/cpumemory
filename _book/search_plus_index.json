{"./":{"url":"./","title":"关于","keywords":"","body":"每位程序开发者都该有的memory知识 本文翻译自 Ulrich Drepper 于 2007 年撰写的论文《What Every Programmer Should Know About Memory》(版次: 1.0)，原文共 114 页。 在 CPU 核 (core) 在速度和数量增长的同时，memory存取限制著当今大多数程序的效率，甚至未来一段时间也会如此。 尽管硬件设计者已提出日趋复杂的memory处理与加速机制 —— 例如 CPU cache —— 但若程序开发者无法善用，仍无法有效发挥硬件作用。 不幸的是，论及电脑的memory子系统或 CPU cache时，无论是其内部的结构，抑或存取成本，对大多程序开发者仍相当陌生。 本文解释用于现代电脑硬件的memory子系统的结构、阐述 CPU cache发展的考量、它们如何运作，以及程序该如何针对memory操作调整，从而达到最佳的效能。 翻译资讯 译者: Chi-En Wu, Jim Huang [info] 关于繁体中文翻译内容的修正、改进建议，和贡献，请造访 sysprog21/cpumemory-zhtw "},"introduction.html":{"url":"introduction.html","title":"1. 引言","keywords":"","body":"1. 引言 早期的电脑简单许多。由于系统中的众多元件，如 CPU、memory、大容量储存装置（mass storage）、以及网卡（network interface），是一同被发展出来的，也因此在效能上相当平衡。举例来说，在提供资料时，memory与网卡并不比 CPU 来得快（非常多）。 这个情况随著电脑基础结构的稳定、以及硬件开发者专注于最佳化独立的子系统而改变了。某些电脑元件的效能突然大幅地落后并成为了瓶颈。尤其是大容量储存装置与memory –– 由于成本的缘故 –– 相比于其他系统，进步得十分缓慢。 大容量储存装置的效能问题主要由软件技术解决：操作系统（operating system）在主memory（main memory）–– 其在存取上比硬盘（hard disk）快了几个数量级 –– 中保存了最常使用（并且最有可能被使用）的资料。cache亦被加到储存装置中，其不需对操作系统作任何改变就能提升效能。1由于偏离本文的主旨，我们就不继续深入针对大型储存装置存取的软件最佳化细节了。 不若储存子系统一般，解决主memory的瓶颈已被证实更加困难，而且几乎所有的解决方法都必须改变硬件。现今这些改变主要有以下这些方式： RAM 的硬件设计（速度以及平行度〔parallelism〕）。 memory控制器（controller）的设计。 CPU cache。 装置的直接memory存取（Direct Memory Access，DMA）。 本文主要涉及 CPU cache以及memory控制器设计的一些影响。在探索这些主题的过程中，我们将对 DMA 有更深入的理解。不过，我们要先概观地从现今商用硬件的设计开始。这是理解高效使用memory子系统的问题与限制的必要条件。我们也会 –– 稍加详细地 –– 学到不同类型的 RAM，并阐述为何这些差异依旧存在。 这篇文件绝非最终且完整的。本文仅止于商用硬件，而且仅限这类硬件的一个子集。同时，许多主题的讨论仅点到为止，以能达到本文目的为主。对于这些主题，建议读者去寻找更详尽的文件。 当提及特定操作系统的细节与解法时，本文仅特指 Linux。无论何时都不会涵盖其他操作系统的任何资讯。作者无意著墨于其它操作系统。假如读者认为他必须使用不同的操作系统，那他该去寻求供应商撰写与本文相似的文件。 开始前的最后一点说明。本文包含许多「通常」以及其他类似的修饰语。在这里讨论的技术在现实世界中存在著非常非常多不同的变种，而本文仅针对最常见、最主流的版本。这些技术很少能下定论，是故以此言之。 文件结构 本文主要写给软件开发者，不会深入对硬件导向的读者有用的硬件技术细节。但在我们能够讨论对开发者实用的资讯之前，有许多基础得打。 为了达到这个目标，第二节会以技术细节面描述随机存取memory（Random-Access Memory，RAM）。这一节的内容值得一读，但对于理解后面几节并非必要。在需要此节内容之处都会加上合适的引用（back reference），所以心急的读者可以先略过本节的大部分内容。 第三节描述了许多 CPU cache行为的细节。本节会用上一些图表以避免文字变得枯燥乏味。本节内容是理解本文后续章节所不可或缺的。第四节简述了虚拟memory（virtual memory）如何实作。这也是其余部份的必要基础。 第五节描述了非均匀memory存取（Non-Uniform Memory Access，NUMA）系统的诸多细节。 第六节是本文的核心章节。其将先前几节的资讯总结在一起，并给予程序开发者如何能在不同情境中撰写良好运作的程序建议。非常不耐烦的读者可以从此节开始，并且 –– 必要的话 –– 回到先前的章节回顾基础技术的知识。 第七节介绍了一些能够帮助程序开发者做得更好的工具。即使完全理解了这些技术，距离明确复杂软件专案的问题所在仍十分遥远。某些工具是必要的。 最后在第八节，我们将展望可以在未来几年期待、或是希望拥有的技术。 回报问题 作者想要更新这份文件一段时间。这包含了不仅得随著技术推进而更新，也要修正错误。乐于回报问题的读者可以来信给作者。但请在回报中包含精确的版本资讯。版本资讯可以在文件的最后一页找到。 致谢 我要感谢 Johnray Fuller 以及 LWN 的夥伴们（尤其是 Jonathan Corbet 承担著将作者式的英文改成更加传统形式的艰巨任务）。Markus Armbruster 针对本文的问题与疏忽提供了诸多有价值的建议。 关于本文 本文标题致敬于 David Goldberg 的经典论文《What Every Computer Scientist Should Know About Floating-Point Arithmetic》。这篇论文仍鲜少人知，虽然这应该是任何勇于严谨地撰写程序而敲下键盘者的先决条件。 1. 然而，为了保证使用储存装置cache时的资料完整性（data integrity），改变是必要的。 ↩ "},"commodity-hardware-today.html":{"url":"commodity-hardware-today.html","title":"2. 现代商用硬件","keywords":"","body":"2. 现代商用硬件 由于专属的硬件正在退潮，理解商用硬件是很重要的。时至今日，水平发展比起垂直发展更为常见。意味著现今使用许多较小的、连结在一起的商用电脑，而非少数几个非常大型且异常迅速（且昂贵）的系统，是较符合成本效益的。这是因为快速且廉价的网络硬件随处可见。虽然那些大型专门系统仍在一些情况中占有一席之地，并仍旧有其商机，但整体市场已被商用硬件市场蚕食。Red Hat 于 2007 年预测，对于未来的产品，大多资料中心的「标准建构元件（building block）」将会是一台有著至多四个插槽（socket）的电脑，每个插槽插著一颗四核 CPU，这些 CPU ── 以 Intel CPU 而言 ── 都会采用超线程（hyper-threading，简称 HT）技术。2这表示资料中心的标准系统将会有至多 64 个虚拟处理器（virtual processor）译注。当然也能够支援更大的机器，但四槽、四核 CPU 是目前认为最适宜的配置，并且大多的最佳化都是针对这种机器。 由商用元件建构出的电脑，结构上也存在著巨大差异。即便如此，我们将专注于最重大的差异上，从而涵盖超过 90% 这类硬件。请注意，这些技术细节日新月异，因此奉劝读者将本文的撰写日期纳入考量。 这些年来，个人电脑以及小型服务器被标准化为一张晶片组（chipset），其具有两个部份：北桥（Northbridge）与南桥（Southbridge）。图 2.1 示意这个结构。 图 2.1：包含北桥与南桥的结构 所有（在前面的例子中有两颗，但可以有更多）CPU 都透过一条共用的总线（bus）── 前端总线（Front Side Bus，FSB）── 连接到北桥。**北桥包含memory控制器（memory controller）**，而它的实作(implementation)决定用在电脑中的 RAM 晶片类型。不同类型的 RAM ── 诸如 DRAM、Rambus、以及 SDRAM (synchronous DRAM)── 需要不同memory控制器。为了与其它系统装置联系，北桥必须与南桥沟通。**南桥 ── 经常被称作 I/O 桥** ── 借由各种不同的总线与各个装置沟通。 现今，PCI、PCI Express、SATA、与 USB 等最重要的总线，以及 PATA、IEEE 1394、序列埠（serial port）、与平行埠（parallel port）都被南桥所支援。较老旧的系统有附属于北桥的 AGP 槽。这源于南北桥连线速度不够快速的效能因素。然而现今的 PCI-E 槽都是连接到南桥上的。 这种系统结构有一些值得注意的结果： * 从一颗 CPU 到另一颗 CPU 的所有资料通讯都必须经过与北桥沟通用的同一条总线。 * 所有与 RAM 的沟通都必须通过北桥。 * RAM 只有单埠。[^3] (ram 可以有多个 port 同时支持读写) * 一颗 CPU 与一个依附于南桥的装置之间的沟通会路经北桥。 几个瓶颈立刻显露在这个设计上。其中一个瓶颈牵涉到**装置对 RAM 的存取**。在最早期的 PC 中，不管在南北桥上，所有装置的沟通都必须经过 CPU，负面地影响整体的系统效能。为了绕过这个问题，某些装置变得能够支援直接memory存取（Direct Memory Access，DMA）。**DMA 允许装置 ── 借由北桥的帮助 ── 在没有 CPU 介入（以及相应效能成本）的情况下直接储存并接收 RAM 中的资料。**现今所有依附于任何总线上的高效能装置都能使用 DMA。虽然这大幅地降低 CPU 的工作量，这也引起北桥频宽的争夺，由于 DMA 请求与来自 CPU 的 RAM 存取相互竞争的缘故。因此，这个问题必须被纳入考量。 第二个瓶颈涉及从**北桥到 RAM 的总线**。总线的确切细节视布署的memory类型而定。在较老旧的系统中，只有一条总线连接所有的 RAM 晶片，因此平行存取是不可能的。近来的memory类型需要两条分离的总线（或称**通道〔channel〕**，如同 DDR2 所称呼的，见图 2.8），其加倍可用的频宽。北桥交错地使用通道进行memory存取。更加近代的memory技术（举例来说，FB-DRAM）加入更多的通道。 由于有限的可用频宽，以延迟最小化的方式排程memory存取，对效能来说是很重要的。如同我们将会看到的，处理器比起memory快许多，而且必须等待存取memory ── 尽管使用 CPU cache。假如多个 HT 或多处理器核同时存取memory，那么memory存取的等待时间甚至会更长。对 DMA 操作依旧如此。 然而，除了并行（concurrency）之外，存取memory还有许多议题。存取模式（access pattern）本身也会大幅地影响memory子系统的效能，尤其是有多个memory通道的情况。在 2.2 节，我们将会涵盖更多 RAM 存取模式的细节。 在一些比较昂贵的系统上，北桥并不真的包含memory控制器。作为替代，北桥可以连接到多个外部memory控制器（在下例中，共有四个）。 图 2.2：包含外部控制器的北桥 这个架构的优点是，有多于一个memory总线，因而提升整体的可用频宽(bandwidth)。这个设计也支援多个memory。并行（concurrent）memory存取模式借由同时存取不同的记忆库（memory bank）来减少延迟。尤其是多个处理器都直接连接到北桥上的情况，如图 2.2。对于这种设计，主要的限制是北桥的内部频宽 ── 其对这种（来自于 Intel 的）架构而言是非常大的。4 使用多个外部memory控制器并不是提升memory频宽的唯一作法。另一个越来越受欢迎的方式是将memory控制器整合到 CPU，并将memory附加到每颗 CPU 上。这个架构因为基于 AMD 的 Opteron 处理器的 SMP 系统而流行起来。图 2.3 展示这样的系统。 Intel 将从 Nehalem 处理器开始支援通用系统介面（Common System Interface，CSI）；这基本上也是相同的方法：一个让每个处理器都能拥有区域（local）memory的整合式memory控制器。 图 2.3：整合式memory控制器 采用像这样的架构，有多少处理器，就有多少可用的记忆库。在一台四核 CPU 的机器上，不需有著巨大频宽的复杂北桥，memory频宽就能变成四倍。一个整合到 CPU 的memory控制器也有些额外的优点；但我们不会在这里继续深入这些技术。 这个架构也有缺点。首先，因为机器仍需要让系统上的所有memory都能被所有的处理器存取，memory就不再是均匀的（uniform）（于是这种系统便有了 NUMA ── 非均匀memory架构〔Non-Uniform Memory Architecture〕── 这个名字）。区域memory（附属于处理器的memory）能够以正常的速度存取。当存取附属于其他处理器的memory时，情况就不同。在这种情况下，就必须用到处理器之间的交互连线（interconnect）。要从 CPU1 存取附属于 CPU2 的memory，就需要通过一条交互连线。当同样的 CPU 存取附属于 CPU4 的memory就得通过两条交互连线。 每次这样的通讯都有其对应的成本。当我们在描述存取远端（remote）memory所需的额外时间时，我们会称之为「NUMA 因子（factor）」。图 2.3 中的范例架构中，每个 CPU 都有两个层级：紧邻的 CPU，以及一颗相隔两条交互连线的 CPU。在更加复杂的系统中，层级会显著地成长。还有一些机器架构（像是 IBM 的 x445 与 SGI 的 Altix 系列）有著不只一种连线类型。CPU 被组织成节点；存取同一节点内的memory的时间会是一致的、或是仅需很小的 NUMA 因子。然而，节点间的连线非常昂贵，而且 NUMA 因子非常高。 如今已有商用的 NUMA 机器，而且可能会在未来扮演著更加重要的角色。预计在 2008 年末，每台 SMP 机器都会使用 NUMA。当一支程序执行在一台 NUMA 机器上时，认识到 NUMA 相应的成本是很重要的。我们将会在第五节讨论更多机器架构，以及一些 Linux 核心（kernel）为这些程序提供的技术。 除了本节其余部分描述的技术细节之外，还有许多影响 RAM 效能的额外因素。它们无法被软件所控制，这也是其不会被涵盖于本节的原因。感兴趣的读者可以在 2.1 节学到其中一些因素。这其实仅是为了对 RAM 的技术有比较完整的理解，而且可能会在购买电脑时做出更好的选择。 接下来的两节会以逻辑门（gate）层级讨论硬件细节，并接触到memory控制器与 DRAM 晶片之间的通讯协议（protocol）。程序开发者或许会发现这些资讯令人豁然开朗，因为这些细节解释为何 RAM 的存取会如此运作。不过，这都是选读的知识，急著了解与日常生活更直接相关主题的读者可以往前跳到 2.2.5 节。 2. 超线程 (HT) 使得一颗处理器核仅需少量的额外硬件，就能被用来同时处理两个或多个任务。 ↩ 3. 我们不会在本文讨论到多埠 RAM，因为这种 RAM 并不见于商用硬件中，至少不在程序开发者得存取之处。它可以在仰赖极限速度的专门硬件 ── 像是网络路由器 ── 中找到。 ↩ 4. 完整起见，这里需要提到一下，这类memory控制器布局可以被用于其它用途，像是「memory RAID」，它很适合与热插拔（hotplug）memory组合使用。 ↩ 译注. 这句的原文是 \"the standard system in the data center will have up to 64 virtual processors\"，注意到本文发表的时间点在 2007 年，本句是 Red Hat 公司之前的推论，64 个虚拟处理器核意味著 4 个插槽、HT (即 2 个硬件执行绪)，和每个 CPU 要有 8 核，不过这样的硬件配置要到 2014 年的 POWER8 才出现，后者的每个 CPU 可有 6 或 12 核。 ↩ "},"commodity-hardware-today/ram-types.html":{"url":"commodity-hardware-today/ram-types.html","title":"2.1. RAM 的种类","keywords":"","body":"2.1. RAM 的种类 这些年来，已经有许多不同种类的 RAM，而每种类型都各有 ── 有时是非常显著的 ── 差异。只有历史学家会对那些较老旧的类型有兴趣。而我们将不会探究它们的细节。我们将会聚焦于现代的 RAM 类型；我们仅会触及其问题的表面，探究系统核心或是应用程序能透过其效能特性看见的一些细节。 第一个有趣的事情是，围绕于在同一台机器中会有不同种类的 RAM 的原因。更具体地说，为何既有静态 RAM（Static RAM，SRAM5）又有动态 RAM（Dynamic RAM，DRAM）。前者更加快速，而且提供了相同的功能。为何一台机器里的 RAM 不全是 SRAM？答案是 ── 也许正是你所预期的 ── 成本。生产与使用 SRAM 比起 DRAM 都更加昂贵。这两个成本因素都很重要，而且后者变得越来越重要。为了了解这些差异，我们要稍微研究一下 SRAM 与 DRAM 储存的实作方式。 在本节的其余部分，我们将会讨论到一些 RAM 实作的底层细节。我们将会让细节尽可能地底层。最后，我们将会从「逻辑层级」讨论讯号（signal），而非从硬件设计师所需的那种层级。那种细节层级对我们这里的目的来说是不必要的。 5. 根据不同前后文，SRAM 指的可能是「同步（synchronous）RAM」。 ↩ "},"commodity-hardware-today/ram-types/static-ram.html":{"url":"commodity-hardware-today/ram-types/static-ram.html","title":"2.1.1. 静态 RAM","keywords":"","body":"2.1.1. 静态 RAM 图 2.4：6-T 静态 RAM 图 2.4 展示了一组由 6 个晶体管 (transistor) 构成的 SRAM 记忆单元（cell）的结构。这个记忆单元的中心是四个晶体管 M1 \\mathbf{M_{1}} M​1​​ 到 M4 \\mathbf{M_{4}} M​4​​，其形成两个交叉耦合（cross-coupled）的反相器（inverter）。它们有两个稳定状态，分别表示 0 与 1。只要 Vdd \\mathbf{V_{dd}} V​dd​​ 维持通电，状态就是稳定的。 若是需要存取记忆单元的状态，就提高字组存取线路（word access line）WL \\mathbf{WL} WL 的电位。若是必须复写记忆单元的状态，则要先将 BL \\mathbf{BL} BL 与 BL‾ \\overline{\\mathbf{BL}} ​BL​​​ 线路设为想要的值，然后再提高 WL \\mathbf{WL} WL 的电位。由于外部的驱动者（driver）强于四个晶体管（M1 \\mathbf{M_{1}} M​1​​ 到 M4 \\mathbf{M_{4}} M​4​​），这使得旧的状态得以被覆写。 需要更多记忆单元运作方式的详细描述，请见 [20]。为了接下来的讨论，要注意的重点是 一个记忆单元需要六个晶体管。也有四个晶体管的变体，但其有些缺点。 维持记忆单元的状态需要持续供电。 当提高字组存取线路 WL \\mathbf{WL} WL 的电位时，几乎能立即取得记忆单元的状态。其讯号如同其它晶体管控制的讯号，是直角的（rectangular）（在两个二元状态间迅速地转变）。 记忆单元的状态是稳定的，不需要再充电周期（refresh cycle）。 也有其它可用的 SRAM 形式 –– 较慢且较省电。但因为我们寻求的是更快的 RAM，所以在此我们对它并不感兴趣。这些慢的变种引发关注的主要原因是，它们比起动态 RAM 更容易被用在系统中，因为它们的介面较为简单。 "},"commodity-hardware-today/ram-types/dynamic-ram.html":{"url":"commodity-hardware-today/ram-types/dynamic-ram.html","title":"2.1.2. 动态 RAM","keywords":"","body":"2.1.2. 动态 RAM 动态 RAM 在其结构上比静态 RAM 简单许多。图 2.5 示意了常见的 DRAM 记忆单元设计结构。它仅由一个晶体管以及一个电容（capacitor）组成。复杂度上的巨大差异，自然意味著其与静态 RAM 的运作方式非常不同。 图 2.5：1-T 动态 RAM 一个动态 RAM 的记忆单元在电容 C \\mathbf{C} C 中保存其状态。晶体管 M \\mathbf{M} M 用以控制状态的存取。为了读取记忆单元的状态，要提高存取线路 AL \\mathbf{AL} AL (access line)的电位；这要不使得电流流经资料线路（data line） DL \\mathbf{DL} DL、要不没有，取决于电容中的电量。要写到记忆单元中，则要适当地设置资料线路 DL \\mathbf{DL} DL，然后提高 AL \\mathbf{AL} AL 的电位一段足以让电容充电或放电的时间。 动态 RAM 有许多设计上的难题。使用电容意味著读取记忆单元时会对电容放电。这件事无法不断重复，必须在某个时间点上对电容重新充电。更糟的是，为了容纳大量的记忆单元（晶片有著 109 或者更多的记忆单元在现今是很普遍的），电容的电量必须很低（在飞〔femto，10-15〕法拉范围内或者更小）。完全充电后的电容容纳了数万个电子。尽管电容的电阻很高（几兆欧姆），耗尽电容仍旧只需要很短的时间。这个问题被称为「漏电（leakage）」。 这种泄漏是 DRAM 必须被持续充电的原因。对于现今大部分 DRAM 晶片，每 64ms 就必须重新充电一次。在重新充电的期间内是无法存取memory的，因为重新充电基本上就是直接丢弃结果的读取操作。对某些工作而言，这个额外成本可能会延误高达 50% 的memory存取（见 [3]）。 第二个 –– 因微小电量而造成 –– 的问题是，从记忆单元读取的资讯无法直接使用。资料线路必须被连接到感测放大器（sense amplifier），其能够根据仍需计作 1 的电量范围来分辨储存的 0 或 1。 第三个问题是，从记忆单元进行读取会消耗电容的电量。这代表每次的读取操作都必须接著进行重新对电容充电的操作。这能够透过将感测放大器的输出返回到电容里头来自动达成。虽然这代表读取memory内容需要额外的电力以及 –– 更为重要地 –– 时间。 第四个问题是，对电容充放电并不是立即完成的。由于感测放大器接收到的讯号并不是直角的，因此必须使用一个谨慎的估计，以得知何时可以使用记忆单元的输出。对电容充放电的公式为 QCharge(t)=Q0(1−e−tRC)QDischarge(t)=Q0e−tRC \\begin{aligned} Q_{\\text{Charge}}(t) &= Q_{0}(1 - e^{-\\frac{t}{RC}}) \\\\ Q_{\\text{Discharge}}(t) &= Q_{0} e^{-\\frac{t}{RC}} \\end{aligned} ​Q​Charge​​(t)​Q​Discharge​​(t)​​​=Q​0​​(1−e​−​RC​​t​​​​)​=Q​0​​e​−​RC​​t​​​​​​ 这代表让电容充电或放电需要一些时间（由电量 C 与电阻 R 决定）。这也代表无法立即使用能被感测放大器侦测的电流。图 2.6 显示了充电与放电的曲线。X 轴以 RC（电阻乘上电量）为单位，这是一种时间单位。 图 2.6：电容充电与放电时间 不像静态 RAM 能够在字组存取线路的电位提高时立即取得输出结果，它会花费一些时间以让电容被充分放电。这个延迟严重地限制了 DRAM 能够达到的速度。 简单的方法也有其优点。最主要的优点是大小。比起一个 SRAM 的记忆单元，一个 DRAM 的记忆单元所需的晶片面积要小好几倍。SRAM 记忆单元也需要个别的电力来维持晶体管的状态。DRAM 记忆单元的结构也较为简单，这代表能较轻易地将许多记忆单元紧密地塞在一起。 总体来说，赢在（极为戏剧性的）成本差异。除了在专门的硬件 –– 举例来说，网络路由器 –– 之外，我们必须采用基于 DRAM 的主memory。这对程序开发者有著巨大的影响，我们将会在本文的其余部分讨论它们。但首先，我们需要先多理解一些实际使用 DRAM 记忆单元的细节。 "},"commodity-hardware-today/ram-types/dram-access.html":{"url":"commodity-hardware-today/ram-types/dram-access.html","title":"2.1.3. DRAM 存取","keywords":"","body":"2.1.3. DRAM 存取 一支程序使用虚拟地址（virtual address）来选择memory位置。处理器将其转译（translate）成实体地址（physical address），最终由memory控制器选择对应于这个地址的 RAM 晶片。为了选择 RAM 晶片中的个别记忆单元，实体地址的一部分会以数条地址线（address line）的形式被传递进去。 由memory控制器个别定址（address）memory位置是极为不切实际的：4GB 的 RAM 会需要 232 条地址线。作为替代，地址会使用较小的一组地址线，编码成二进位数值传递。以这种方式传递到 DRAM 晶片的地址必须先被多路分解（demultiplex）。有 N N N 条地址线的多路分解器（demultiplexer）将会有 2N N N 条输出线（output line）。这些输出线能被用以选择记忆单元。对于小容量的晶片而言，使用这种直接的方法是没什么大问题的。 但假如记忆单元的数量增加，这个方法就不再合适。一个 1Gbit6 容量的晶片将会需要 30 条地址线以及 230 选择线（select line）。在不牺牲速度的前提下，多路分解器的大小会随著输入线（input line）的数量以指数成长。用于 30 条地址线的多路分解器需要大量的晶片空间，外加多路分解器的（尺寸与时间）复杂度。更重要的是，同时在地址线上传输 30 个脉冲（impulse）比「只」传输 15 个脉冲还要难得多。只有少数的地址线能够以长度完全相同或适当安排时间的方式排版。7 图 2.7：动态 RAM 示意图 图 2.7 显示了以极高阶角度示意的 DRAM 晶片。DRAM 记忆单元被组织在列（row）与行（column）中。虽然它们可以全都排成一列，但 DRAM 晶片会因而需要一个庞大的多路分解器。借由阵列（array）的方式，便能够以各为一半大小的一个多路分解器与一个多工器达到这种目的。8这从各方面来说都是个大大的节约。在这个例子中，地址线 a0 \\mathbf{a_{0}} a​0​​ 与 a1 \\mathbf{a_{1}} a​1​​ 透过行地址选择（row address selection）（RAS‾ \\overline{\\text{RAS}} ​RAS​​​）9多路分解器选择一整列记忆单元的地址线。在读取时，所有记忆单元的内容都能够被列地址选择（column address selection）多工器（multiplexer）（CAS‾ \\overline{\\text{CAS}} ​CAS​​​）取得。基于地址线 a2 \\mathbf{a_{2}} a​2​​ 与 a3 \\mathbf{a_{3}} a​3​​，其中一行的内容便能够提供给 DRAM 晶片的资料针脚（pin）。这会在许多 DRAM 晶片上平行地发生多次，以产生对应于资料总线宽度的所有位元。 对于写入操作，新的记忆单元的值会被置于资料总线中，然后 ── 当记忆单元借由 RAS‾ \\overline{\\text{RAS}} ​RAS​​​ 与 CAS‾ \\overline{\\text{CAS}} ​CAS​​​ 选取时 ── 储存到资料单元中。相当直观的设计。这实际上有著显然地更多的困难。需要规范发出讯号之后，在资料能够由资料总线读取之前有多少延迟。如同上节所述，电容无法立即充放电。来自于记忆单元的讯号太微弱了，以致于它非得被放大（amplify）不可。对于写入操作，必须指定设置完 RAS‾ \\overline{\\text{RAS}} ​RAS​​​ 与 CAS‾ \\overline{\\text{CAS}} ​CAS​​​ 之后，资料必须在总线维持多久，才能够成功地在记忆单元中储存新值（再提醒一次，电容不会立即被充放电）。这些时间常数（constant）对于 DRAM 晶片的效能而言是至关重要的。我们将会在下一节讨论这些。 一个次要的可扩展性问题是，令 30 条地址线都连接到每个 RAM 晶片也不大可行。一个晶片的针脚是个宝贵的资源。必须尽可能多地平行传输资料（像是一次 64 位元）已经够「糟」。memory控制器必须能够定址每个 RAM 模组（module）（RAM 晶片的集合）。假如因为效能因素，需要平行存取多个 RAM 模组，并且每个 RAM 模组需要它所拥有的一组 30 条或者更多条地址线，那么仅为了处理地址，以 8 个 RAM 模组而言，memory控制器就必须要有多达 240+ 根针脚。 为了克服这些次要的可扩展性问题，DRAM 晶片长期以来必须自行多工地址。这代表地址会被转变成两个部分。由地址位元（图 2.7 的例子中的 a0 \\mathbf{a_{0}} a​0​​ 与 a1 \\mathbf{a_{1}} a​1​​）组成的第一个部分选取列。这个选择直到撤销之前都会维持有效。接著第二个部分，地址位元 a2 \\mathbf{a_{2}} a​2​​ 与 a3 \\mathbf{a_{3}} a​3​​，选取行。决定性的差异在于，只需要两条外部的地址线。译注需要额外一些少量的线路来代表能否取得 RAS‾ \\overline{\\text{RAS}} ​RAS​​​ 与 CAS‾ \\overline{\\text{CAS}} ​CAS​​​ 的线路，但这对于减半地址线来说，这只是个很小的代价。不过，这种地址多工带来了一些自身的问题。我们将会在 2.2 节讨论这些问题。 6. 我厌恶这些 SI 前缀（prefix）。对我来说一个 giga-bit 永远是 230 而非 109 位元。 ↩ 7. 现代 DRAM 类型（如 DDR3）可以自动调整时序，但可以容忍的范围有限。 ↩ 8. 多工器与多路分解器是对等的，并且这里的多工器在写入时需要如多路分解器一般运作。所以从现在开始我们要忽略其差异。 ↩ 9. 名字上的线表示讯号是反相的（negated）。 ↩ 译注. 这里的意思单看字面可能比较难理解。以图 2.7 为例，memory由 4 × 4 的记忆单元组成。对外仅提供两条地址线。在选择记忆单元时，首先由两条外部地址线指定列，并将结果暂存起来。接著，再利用同样的两条外部地址线指定行。便能够只用两条外部地址线来指定 16 个记忆单元中的一个。详见 2.2 节。 ↩ "},"commodity-hardware-today/ram-types/conclusions.html":{"url":"commodity-hardware-today/ram-types/conclusions.html","title":"2.1.4. 结论","keywords":"","body":"2.1.4. 结论 若是本节中的细节稍微令人喘不过气，别担心。要从本节得到的重点为： 有著为何不是所有memory都为 SRAM 的理由 记忆单元需要被个别选取使用 地址线的数量直接反映memory控制器、主机板（motherboard）、DRAM 模组、与 DRAM 晶片的成本 在读取或写入操作的结果有效之前得花上一段时间 接下来的章节将会深入更多存取 DRAM memory的实际过程的细节。我们不会深入存取 SRAM 的细节，它通常是直接定址的。这是基于速度考量，并且因为 SRAM memory受限于其大小。SRAM 目前被用于 CPU cache并内建于晶片上（on-die），其连线较少、并且完全在 CPU 设计者的控制中。CPU cache是我们将会在之后谈及的主题，但我们所需知道的是，SRAM 记忆单元有著确切的最大速度，这取决于在 SRAM 上所花的努力。速度可以从略微慢于 CPU 核到慢于一或两个数量级。 "},"commodity-hardware-today/dram-access-technical-details.html":{"url":"commodity-hardware-today/dram-access-technical-details.html","title":"2.2. DRAM 存取技术细节","keywords":"","body":"2.2. DRAM 存取技术细节 在介绍 DRAM 的章节中，我们看到为了节省地址针脚，DRAM 晶片多工了地址。我们也看到因为记忆单元中的电容无法立即放电以产生稳定的讯号，存取 DRAM 记忆单元会花点时间；我们也看到 DRAM 记忆单元必须被重新充电。现在，是时候将这全都摆在一起，看看这些因子是如何决定 DRAM 存取是怎么运作的。 我们将会聚焦于当前的技术上；我们不会讨论非同步（asynchronous）DRAM 及其变种，因为它们完全与此无关。对这个主题感兴趣的读者请参见 [3] 与 [19]。我们也不会谈及 Rambus DRAM（RDRAM），纵使这项技术并不过时。只是它并没有被广泛使用于系统memory中。我们仅会聚焦于同步 DRAM（Synchronous DRAM，SDRAM）与其后继者双倍资料传输速率 DRAM（Double Data Rate DRAM，DDR）。 同步 DRAM，如同名称所暗示的，其运作与一种时间源（time source）有关。memory控制器提供一个时钟（clock），其频率决定前端总线（FSB）── DRAM 晶片使用的memory控制器介面 ── 的速度。在撰写此文时，频率可达 800MHz、1,066MHz、甚至到 1,333MHz，并宣称在下个世代会达到更高的频率（1,600MHz）。这不代表用于总线的频率真的这么高。而是现今的总线都是二或四倍频（double- or quad-pumped），代表资料在每个周期传输二或四次。数字越高、卖得越好，因此厂商惯于将四倍频 200MHz 总线宣传成「实质上的（effective）」800MHz 总线。 对于现今的 SDRAM 来说，每次资料传输以 64 位元 (即 8 位元组) 组成。FSB 的传输率因此为 8 位元组乘上实际的总线频率（以四倍频 200MHz 总线来说，6.4GB/s）。这听起来很多，但这是尖峰速度 ── 永远无法超越的最大速度。如同我们将会看到的，现今与 RAM 模组沟通的协议中有许多的閒置期（downtime），这时是没有资料可以被传输的。这个閒置期正是我们必须了解、并减到最小，以达到最佳效能的东西。 "},"commodity-hardware-today/dram-access-technical-details/read-access-protocol.html":{"url":"commodity-hardware-today/dram-access-technical-details/read-access-protocol.html","title":"2.2.1. 读取协议","keywords":"","body":"2.2.1. 读取协议 图 2.8：SDRAM 读取时序 图 2.8 显示 DRAM 模组上一些连线的活动，发生在三个标上不同颜色的阶段。像往常一样，时间从左到右流动。许多细节被省略。这里我们仅讨论总线时钟、RAS‾ \\overline{\\text{RAS}} ​RAS​​​ 与 CAS‾ \\overline{\\text{CAS}} ​CAS​​​ 讯号、以及地址与资料总线。读取周期从memory控制器在地址总线提供列地址、并降低 RAS‾ \\overline{\\text{RAS}} ​RAS​​​ 讯号的电位开始。所有讯号都会在时钟（CLK）的上升边沿（rising edge）被读取，因此若是讯号并不是完全的方波也无所谓，只要在读取的这个时间点是稳定的就行。设置列地址会使得 RAM 晶片锁上（latch）指定的列。 经过 tRCD（RAS‾ \\overline{\\text{RAS}} ​RAS​​​ 至 CAS‾ \\overline{\\text{CAS}} ​CAS​​​ 的延迟）个时脉周期之后，便能发出 CAS‾ \\overline{\\text{CAS}} ​CAS​​​ 讯号。这时行地址便能借由地址总线提供、以及降低 CAS‾ \\overline{\\text{CAS}} ​CAS​​​ 线路的电位来传输。这里我们可以看到，地址的两个部分（约莫是对半分，其余的情况并不合理）是如何透过同样的地址总线来传输。 现在定址已经完成，可以传输资料。为此 RAM 晶片需要一点时间准备。这个延迟通常被称作 CAS‾ \\overline{\\text{CAS}} ​CAS​​​ 等待时间（CAS‾ \\overline{\\text{CAS}} ​CAS​​​ Latency，CL）。在图 2.8 中，CAS‾ \\overline{\\text{CAS}} ​CAS​​​ 等待时间为 2。这个值可高可低，取决于memory控制器、主机板、以及 DRAM 模组的品质。等待时间也可以是半周期。以 CL=2.5 而言，资料将能够在蓝色区块的第一个下降边沿时取得。 对于取得资料的这些准备工作而言，仅传输一个字组的资料是很浪费的。这即是为何 DRAM 模组允许memory控制器指定要传输多少资料。通常选择在 2、4、或 8 字组之间。这便能在不提供新的 RAS‾ \\overline{\\text{RAS}} ​RAS​​​／CAS‾ \\overline{\\text{CAS}} ​CAS​​​ 序列的情况下填满cache中的整行（line）。memory控制器也能够在不重设列的选取的情况下发出新的 CAS‾ \\overline{\\text{CAS}} ​CAS​​​ 讯号。借由这种方式，能够非常快速地读取或写入连续的memory地址，因为不必发出 RAS‾ \\overline{\\text{RAS}} ​RAS​​​ 讯号，也不必将列无效化（deactivate）（见后文）。memory控制器必须决定是否让列保持「开启（open）」。一直任其开启，对实际的应用程序来说有些负面影响（见 [3]）。发出新的 CAS‾ \\overline{\\text{CAS}} ​CAS​​​ 讯号仅受 RAM 模组的命令速率（command rate）控制（通常设为 Tx，其中 x 为像是 1 或 2 的值；每个周期都接受命令的高效能 DRAM 模组会设为 1）。 在这个例子中，SDRAM 在每个周期吐出一个字组。第一世代就是这么做的。DDR 能够在每个周期传输两个字组。这减少传输时间，但没有改变等待时间。虽然在实务上看起来不同，但原理上 DDR2 运作如斯。这里没有再深入细节的必要。能够注意到 DDR2 可以变得更快、更便宜、更可靠、并且更省电（更多资讯见 [6]）就够。 "},"commodity-hardware-today/dram-access-technical-details/precharge-and-activation.html":{"url":"commodity-hardware-today/dram-access-technical-details/precharge-and-activation.html","title":"2.2.2. 预充电与有效化","keywords":"","body":"2.2.2. 预充电与有效化 图 2.8 未涵盖整个周期。它只显示出存取 DRAM 的完整循环的一部分。在能够发送新的 RAS‾ \\overline{\\text{RAS}} ​RAS​​​ 讯号之前，必须无效化（deactivate）目前锁上的列，并对新的列预充电（precharge）。这里我们仅聚焦在借由明确命令来执行的情况。有些协议上的改进 ── 在某些情况下 ── 能够避免这个额外步骤。不过由预充电引入的延迟仍然会影响操作。 图 2.9：SDRAM 预充电与有效化 图 2.9 示意从 CAS‾ \\overline{\\text{CAS}} ​CAS​​​ 讯号开始、到另一列的 CAS‾ \\overline{\\text{CAS}} ​CAS​​​ 讯号为止的活动。与先前一样，经过 CL 周期后，便能够取得以第一个 CAS‾ \\overline{\\text{CAS}} ​CAS​​​ 讯号请求的资料。在这个例子中，请求两个字组，其 ── 在一个简易的 SDRAM 上 ── 花了两个周期来传输。也可以想像成是在一张 DDR 晶片上传输四个字组。 即使在命令速率为 1 的 DRAM 模组上，也无法立即发出预充电命令。它必须等待与传输资料一样长的时间。在这个例子中，它花了两个循环。虽然与 CL 相同，但这只是巧合。预充电讯号没有专用的线路；有些实作是借由同时降低允写（Write Enable，WE‾ \\overline{\\text{WE}} ​WE​​​）与 RAS‾ \\overline{\\text{RAS}} ​RAS​​​ 的电位来发出这个命令。这个组合本身没什么特别意义（编码细节见 [18]）。 一旦发出预充电命令，它会花费 tRP（列预充电时间）个周期，直到列能被选取为止。在图 2.9 中，大部分的时间（以紫色标示）与memory传输时间（浅蓝）重叠。这满好的！但 tRP 比传输时间还长，所以下一个 RAS‾ \\overline{\\text{RAS}} ​RAS​​​ 讯号会被延误一个周期。 假使我们延伸图表的时间轴，我们会发现下一次资料传输发生在前一次停止的 5 个周期之后。这表示在七个周期中，只有两个周期有用到资料总线。将这乘上 FSB 的速度，对 800MHz 总线而言，理论上的 6.4GB/s 就变成 1.8GB/s。这太糟，而且必须避免。在第六节描述的技术能帮忙提升这个数字。程序开发者通常也得尽一份力。 对于 SDRAM 模组，还有一些没有讨论过的时间值。在图 2.9 中，预充电命令受限于资料传输时间。另一个限制是，在 RAS‾ \\overline{\\text{RAS}} ​RAS​​​ 讯号之后，SDRAM 模组需要一些时间才能够为另一列预充电（记作 tRAS）。这个数字通常非常大，为 tRP 值的两到三倍。假如 ── 在 RAS‾ \\overline{\\text{RAS}} ​RAS​​​ 讯号之后 ── 只有一个 CAS‾ \\overline{\\text{CAS}} ​CAS​​​ 讯号，并且资料传输在少数几个周期内就完成，这就是问题。假设在图 2.9 中，起始的 CAS‾ \\overline{\\text{CAS}} ​CAS​​​ 讯号是直接接在 RAS‾ \\overline{\\text{RAS}} ​RAS​​​ 讯号之后，并且 tRAS 为 8 个周期。预充电命令就必须要延迟一个额外的周期，因为 tRCD、CL、与 tRP（因为它比资料传输时间还长）的总和只有 7 个周期。 DDR 模组经常以一种特殊的标记法描述：w-x-y-z-T。举例来说：2-3-2-8-T1。这代表： w 2 CAS‾ \\overline{\\text{CAS}} ​CAS​​​ 等待时间（CL） x 3 RAS‾ \\overline{\\text{RAS}} ​RAS​​​ 至 CAS‾ \\overline{\\text{CAS}} ​CAS​​​ 等待时间（tRCD） y 2 RAS‾ \\overline{\\text{RAS}} ​RAS​​​ 预充电（tRP） z 8 有效化至预充电延迟（tRAS） T T1 命令速率 还有许多其它会影响命令的发送或处理方式的时间常数。不过在实务上，这五个常数就足以判定模组的效能。 知道这些关于电脑的资讯，有时有助于解释某些量测结果。购买电脑的时候，知道这些细节显然是有用的，因为它们 ── 以及 FSB 与 SDRAM 模组的速度 ── 是决定一台电脑速度的最重要因素。 非常大胆的读者也可以试著调校（tweak）系统。有时候 BIOS 允许修改某些或者全部的值。SDRAM 模组拥有能够设定这些值的可程序化暂存器（register）。通常 BIOS 会挑选最佳的预设值。如果 RAM 模组的品质很好，可能可以在不影响电脑稳定性的前提下降低某些延迟。网络上众多的超频网站提供大量的相关文件。尽管如此，请自行承担风险，可别说你没被警告过。 "},"commodity-hardware-today/dram-access-technical-details/recharging.html":{"url":"commodity-hardware-today/dram-access-technical-details/recharging.html","title":"2.2.3. 再充电","keywords":"","body":"2.2.3. 再充电 谈及 DRAM 存取时，一个最常被忽略的主题是再充电（recharging）。如同 2.1.2 节所述，DRAM 记忆单元必须被持续地重新充电。对于系统的其余部分来说，这并不那么容易察觉。在对列10重新充电的时候，是不能对它存取的。在 [3] 的研究中发现「出乎意料地，DRAM 再充电的安排可能大大地影响效能」。 根据 JEDEC（联合电子装置工程委员会，Joint Electron Device Engineering Council）规范，每个 DRAM 记忆单元每隔 64ms 都必须重新充电。假如一个 DRAM 阵列有 8,192 列，这代表memory控制器平均每 7.8125μs 都得发出再充电命令（再充电命令能够伫列等待〔queue〕，因此在实务上，两次请求间的最大间隔能更长一些）。为再充电命令排程是memory控制器的职责。DRAM 模组纪录最后一次再充电的列的地址，并且自动为新的请求增加地址计数器。 关于再充电与发出命令的时间点，程序开发者能做的真的不多。但在解释量测结果时，务必将 DRAM 生命周期的这个部分记在心上。假如必须从正在被重新充电的列中取得一个关键的字组，处理器会被延误很长一段时间。重新充电要花多久则视 DRAM 模组而定。 10. 不管 [3] 与其它文献怎么说，列都是这项操作的对象（见 [18]）。 ↩ "},"commodity-hardware-today/dram-access-technical-details/memory-types.html":{"url":"commodity-hardware-today/dram-access-technical-details/memory-types.html","title":"2.2.4. memory类型","keywords":"","body":"2.2.4. memory类型 值得花点时间来看看目前以及即将到来的memory类型。我们将从 SDR（单倍资料传输速率，Single Data Rate）SDRAM 开始，因为它们是 DDR（双倍资料传输速率，Double Data Rate）SDRAM 的基础。SDR 十分简单。记忆单元与资料传输速率是一致的。 图 2.10：SDR SDRAM 的运作 在图 2.10 中，记忆单元阵列能够以等同于经由memory总线传输的速率输出memory内容。假如 DRAM 记忆单元阵列能够以 100MHz 运作，单一记忆单元的总线的资料传输率便为 100Mb/s。所有元件的频率 f f f 都是一样的。由于耗能会随著频率增加而增加，因此提升 DRAM 晶片的吞吐量（throughput）的代价很高。由于大量的阵列单元，成本贵得吓人。11实际上，提升频率通常也需要提升电压，以维持系统的稳定性，这更是一个问题。DDR SDRAM（追溯地称为 DDR1）设法在不提高任何相关频率的情况下提升吞吐量。 图 2.11：DDR1 SDRAM 的运作 SDR 与 DDR1 之间的差异是 ── 如同在图 2.11 所见、以及从它们名称来猜测的 ── 每个周期传输两倍的资料量。也就是说，DDR1 晶片会在上升与下降边沿传输资料。这有时被称作一条「二倍频（double-pumped）」总线。为了不提升记忆单元阵列的频率，必须引入一个缓冲区（buffer）。这个缓冲区会持有每条资料线的两个位元。这转而要求令 ── 图 2.7 的记忆单元阵列中的 ── 资料总线由两条线路组成。实作的方式很直观：只要对两个 DRAM 记忆单元使用相同的行地址，并且平行存取它们就行。这个实作对记忆单元阵列的改变也非常小。 SDR DRAM 直接采用其频率来命名（例如 PC100 代表 100MHz SDR）。由于 DDR1 DRAM 的频率不会改变，厂商必须想出新的命名方式，让它听起来更厉害。他们提出的名字包含一个 DDR 模组（拥有 64 位元总线）能够维持、以位元组为单位的传输速率： 100MHz×64bit×2=1,600MB/s 100\\text{MHz} \\times 64\\text{bit} \\times 2 = 1,600\\text{MB/s} 100MHz×64bit×2=1,600MB/s 于是一个频率为 100MHz 的 DDR 模组就称为 PC1600。因为 1600 > 100，满足一切销售需求；这听起来更棒，纵使实际上只提升成两倍而已。12 图 2.12：DDR2 SDRAM 的运作 为了突破这些memory技术，DDR2 包含少许额外的革新。能从图 2.12 上看到的最明显的改变是，总线的频率加倍。频率加倍意味著频宽加倍。由于频率加倍对记忆单元阵列来说并不经济，因此现在需要由 I/O 缓冲区在每个时脉周期读取四个位元，然后才送到总线上。这代表 DDR2 模组的改变是，只让 DIMM 的 I/O 缓冲区元件拥有能以更快速度运转的能力。这当然是可能的，而且不需太多能量，因为这只是个小元件，而非整个模组。厂商为 DDR2 想出的名称与 DDR1 的名称相似，只是在计算值的时候，乘以二变成乘以四（我们现在有条四倍频〔quad-pumped〕总线）。表 2.1 显示现今使用的模组名称。 阵列频率 总线频率 资料速率 名称（速率） 名称（FSB） 133MHz 266MHz 4,256MB/s PC2-4200 DDR2-533 166MHz 333MHz 5,312MB/s PC2-5300 DDR2-667 200MHz 400MHz 6,400MB/s PC2-6400 DDR2-800 250MHz 500MHz 8,000MB/s PC2-8000 DDR2-1000 266MHz 533MHz 8,512MB/s PC2-8500 DDR2-1066 表 2.1：DDR2 模组名称 命名上还有个别扭之处。用于 CPU、主机板、以及 DRAM 模组的 FSB 速度是使用实质上的频率来指定的。也就是将时脉周期的两个边沿都纳入传输的因素，从而浮夸数字。因此，一个拥有 266MHz 总线的 133MHz 模组，它的 FSB「频率」为 533MHz。 沿著转变到 DDR2 的路线，DDR3（真正的 DDR3，而非用于显卡中的假 GDDR3）的规范寻求更多的改变。电压从 DDR2 的 1.8V 降至 DDR3 的 1.5V。由于功率消耗公式是使用电压的平方来算的，因此光这点就改善 30%。加上晶粒（die）尺寸的缩小以及其它电气相关的改进，DDR3 能够在相同的频率下降低一半的功率消耗。或者，在相同功率包络（envelope）的情况下达到更高的频率。又或者，在维持相同热能排放量的情况下加倍容量。 DDR3 模组的记忆单元阵列会以外部总线的四分之一速度运转，其需要将 DDR2 的 4 位元 I/O 缓冲区加大到 8 位元。示意图见图 2.13。 图 2.13：DDR3 SDRAM 的运作 起初，DDR3 模组的 CAS‾ \\overline{\\text{CAS}} ​CAS​​​ 等待时间可能会略高一些，因为 DDR2 技术更为成熟。这导致只在频率高于 DDR2 所能达到的情况，并且 ── 尽管如此 ── 一般在频宽比延迟时间更为重要的时候，DDR3 才有其用处。已有达到与 DDR2 相同 CAS‾ \\overline{\\text{CAS}} ​CAS​​​ 等待时间的 1.3V 模组的风声。无论如何，由于更快的总线，达到更高速度的可能性将会超过增加的等待时间。 一个 DDR3 的可能问题是，对于 1,600Mb/s 或更高的传输率，每个通道的模组数量可能会减至仅剩一个。在早期的版本中，对于任何频率都有这个要求，所以可以期待在某个时间点，这项要求会被剔除。否则会严重地限制系统的能力。 表 2.2 列出我们很可能会看到的 DDR3 模组名称。JEDEC 到目前为止接受前四种。鑑于 Intel 的 45nm 处理器拥有速度为 1,600Mb/s 的 FSB，1,866Mb/s 便为超频市场所需。随著 DDR3 的发展，我们大概会看到更多的类型。 阵列频率 总线频率 资料速率 名称（速率） 名称（FSB） 100MHz 400MHz 6,400MB/s PC3-6400 DDR3-800 133MHz 533MHz 8,512MB/s PC3-8500 DDR3-1066 166MHz 667MHz 10,667MB/s PC3-10667 DDR3-1333 200MHz 800MHz 12,800MB/s PC3-12800 DDR3-1600 233MHz 933MHz 14,933MB/s PC3-14900 DDR3-1866 表 2.2：DDR3 模组名称 所有的 DDR memory都有个问题：总线频率的提升，会使得建立平行资料总线变得困难。DDR2 模组有 240 根针脚。必须要规划所有连结到资料与地址针脚的布线，以让它们有大略相同的长度。还有个问题是，假如多过一个 DDR 模组被菊花链结（daisy-chain）在同一条总线上，对于每个附加的模组而言，讯号会变得越来越歪曲。DDR2 规范只允许在每个总线（亦称作通道）上有两个模组，DDR3 规范在高频时只能有一个。由于每个通道有 240 根针脚，使得单一北桥无法合理地驱动多于两个通道。替代方式是拥有外部的memory控制器（如图 2.2），但这代价不小。 这所代表的是，商用主机板受限于至多持有四个 DDR2 或 DDR3 模组。这大大地限制一个系统能够拥有的memory总量。即使是老旧的 32 位元 IA-32 处理器都能拥有 64GB 的 RAM，即使对于家用，memory需求也在持续增长，所以必须作点什么。 一个解决办法是将memory控制器加到每个处理器中，如同第二节所述。AMD 的 Opteron 系列就是这么做的，Intel 也将以他们的 CSI 技术来达成。只要处理器所能使用的、适当的memory容量都能被连接到单一处理器上，这会有所帮助。在某些情况下并非如此，这个设置会引入 NUMA 架构，伴随著其负面影响。对某些情况来说，则需要另外的解法。 Intel 针对大型服务器机器的解法 ── 至少在现在 ── 被称为全缓冲 DRAM （Fully Buffered DRAM，FB-DRAM）。FB-DRAM 模组使用与现今 DDR2 模组相同的memory晶片，这使得它的生产相对便宜。差异在连结到memory控制器的连线中。FB-DRAM 使用的并非平行资料总线，而是一条序列总线（也能追溯 Rambus DRAM、PATA 的后继者 SATA 以及 PCI/AGP 的后继者 PCI Express）。序列总线能以极高频驱动、恢复序列化的负面影响，甚至提升频宽。使用序列总线的主要影响为 每个通道能使用更多模组。 每个北桥／memory控制器能使用更多通道。 序列总线是被设计成全双工的（fully-duplex）（两条线）。 实作一条差动（differential）总线（每个方向两条线）足够便宜，因而能提高速度。 相比于 DDR2 的 240 根针脚，一个 FB-DRAM 模组只有 69 根针脚。由于能较为妥善地处理总线的电气影响，菊花链结 FB-DRAM 模组要简单许多。FB-DRAM 规范允许每通道至多 8 个 DRAM 模组。 以双通道北桥的连线需求来比较，现在可能以更少的针脚来驱动 FB-DRAM 的六个通道：2 × 240 根针脚对比于 6 × 69 根针脚。每个通道的布线更为简单，这也有助于降低主机板的成本。 对于传统 DRAM 模组来说，全双工平行总线贵得吓人，因为要将所有线路变为两倍的成本高昂。使用序列线路（即使是如 FB-DRAM 所需的差动式）情况就不同，序列总线因而被设计为全双工，意味著在某些情况下，理论上光是如此频宽就能加倍。但这并非唯一使用平行化来提升频宽之处。由于一个 FB-DRAM 控制器能够同时连接多达六个通道，即使对于较小 RAM 容量的系统，使用 FB-DRAM 也能够提升频宽。在具有四个模组的 DDR2 系统拥有两个通道的情况下，相同的能力得以使用一个普通的 FB-DRAM 控制器，经由四个通道来达成。序列总线的实际频宽取决于用在 FB-DRAM 模组的 DDR2（或 DDR3） 晶片类型。 我们能像这样总结优点： DDR2 FB-DRAM 针脚数 240 69 通道数 2 6 DIMM 数／通道数 2 8 最大memory13 16GB14 192GB 吞吐量15 ~10GB/s ~40GB/s 如果要在一个通道上使用多个 DIMM，FB-DRAM 有几个缺点。在链结的每个 DIMM 上，讯号会被延迟 ── 尽管很小，因而增加等待时间。第二个问题是，晶片驱动序列总线需要大量的能量，因为频率非常高、以及驱动总线的需求。但有著同样频率、同样memory容量的 FB-DRAM 总是比 DDR2 与 DDR3 还快，至多四个的 DIMM 每个都能拥有自己的通道；对于大型memory系统，DDR 完全没有任何使用商用元件的解决方法。 11. 功率 = 动态电容 × 电压2 × 频率 ↩ 12. 我会使用二作为倍率，但我不必喜欢浮夸的数字。 ↩ 13. 假设为 4GB 模组。 ↩ 14. 一份 Intel 的简报 ── 基于某些我不理解的原因 ── 说是 8GB... ↩ 15. 假设为 DDR2-800 模组。 ↩ "},"commodity-hardware-today/dram-access-technical-details/conclusions.html":{"url":"commodity-hardware-today/dram-access-technical-details/conclusions.html","title":"2.2.5. 结论","keywords":"","body":"2.2.5. 结论 这一节应该已经显示出存取 DRAM 并不是一个非常快速的过程。至少与处理器执行、以及存取暂存器与cache的速度相比并不怎么快。务必将 CPU 与memory频率之间的差异放在心上。一颗以 2.933GHz 运作的 Intel Core 2 处理器以及一条 1.066GHz FSB 的时脉比率（clock ratio）为 11:1（注：1.066GHz 总线是四倍频的）。每在memory总线延误一个周期，意味著延误处理器 11 个周期。对于大多数机器来说，实际使用的 DRAM 要更慢一些，也因此增加延迟。当我们在接下来的章节中谈论延误的时候，要把这些数字记在心中。 读取命令的时序图显示 DRAM 模组的持续资料传输速率（sustained data rate）很高。整个 DRAM 列能够在毫无延误的情况下传输。资料总线能够 100% 持续使用。对于 DDR 模组而言，这代表每个周期传输两个 64 位元的字组。以 DDR2-800 模组与双通道来说，这代表速率为 12.8GB/s。 但是，除了为此而设计的情况，DRAM 并非总是循序存取的。使用不连续的memory区域，意味著需要预充电以及新的 RAS‾ \\overline{\\text{RAS}} ​RAS​​​ 讯号。这即是工作慢下来、并且 DRAM 需要协助的时候。能越早进行预充电以及发送 RAS‾ \\overline{\\text{RAS}} ​RAS​​​，真的需要那列时的损失（penalty）就越小。 硬件与软件预取（prefetch）（见 6.3 节）能够用以创造更多时间上的重叠并减少延误。预取也有助于及时搬移memory操作，以让之后 ── 在资料真的被需要之前 ── 少点争夺。当必须储存这一轮所产生的资料、并且必须读取下一轮所需的资料时，这是个常见的问题。借由及时搬移读取操作，就不必在基本上相同的时间发出写入与读取操作。 "},"commodity-hardware-today/other-main-memory-users.html":{"url":"commodity-hardware-today/other-main-memory-users.html","title":"2.3. 其它主memory使用者","keywords":"","body":"2.3. 其它主memory使用者 除了 CPU 之外，还有其它能够存取主memory的系统元件。像是网络与大容量储存装置控制器等高效能扩充卡（card）无法负担透过 CPU 输送所有它所需要或提供的资料的成本。作为替代，它们会直接从／往主memory读取或写入资料（直接memory存取〔Direct Memory Access〕，DMA）。图 2.1 中，我们能够看到，扩充卡能够透过南北桥直接与memory沟通。其它总线，像是 USB，也需要 FSB 频宽 ── 即使它们并不使用 DMA，因为南桥也会经由北桥、透过 FSB 连接到处理器。 虽然 DMA 无疑是有益的，但它意味著有更多 FSB 频宽的竞争。在 DMA 流量很大的时候，CPU 可能会在等待从主memory来的资料时，延误得比平常更久。假如有对的硬件的话，有许多绕过这个问题的方法。借由如图 2.3 的架构，可以设法确保使用不受 DMA 影响的节点上的memory来进行运算。也可能让一个南桥依附在每一个节点上，均等地分配 FSB 在每个节点上的负载。还有无数的可能性。在第六节，我们将会介绍可望从软件上协助达成改进的技术与程序设计介面。 最后需要提一下，有些廉价系统的图形系统缺乏独立且专用的视讯 RAM（video RAM）。这些系统会使用主memory的一部分作为视讯 RAM。由于视讯 RAM 的存取很频繁（对于一个 16 bpp、60Hz 的 1024x768 的显示器而言，即为 94MB/s），而且系统memory ── 不像显示卡上的 RAM ── 并不具有两个埠，而这会严重地影响系统效能，尤其是等待时间。当以效能为优先时，最好是忽略这种系统。它们的问题比它们的价值还多。购买这些机器的人们知道它们无法获得最好的效能。 "},"cpu-caches.html":{"url":"cpu-caches.html","title":"3. CPU cache","keywords":"","body":"3. CPU cache 比起仅仅 25 年前的 CPU，现今的 CPU 复杂得多了。在那时，CPU 核的频率与memory总线在相同的等级。memory存取只比暂存器存取慢了一点点。但这点在 90 年代初期大大地改变了，那时 CPU 设计者提升了 CPU 核的频率，但memory总线的频率以及 RAM 晶片的效能并没有等比例地成长。这不是因为无法发展出更快的 RAM，而是如前一节所解释的。这是可能的，但并不经济。与当前 CPU 核一样快的 RAM，比起任何动态 RAM 都要贵上好几个数量级。 一台有著非常小、非常快的 RAM 的机器，以及一台有著许多相对快速的 RAM 的机器，如果要在两者间择一，在给定超过小小的 RAM 容量的工作集（working set）大小、以及存取硬盘这类次级储存（secondary storage）媒体的成本之后，后者永远是赢家。这里的问题在于次级储存装置 –– 通常是硬盘 –– 的速度，它必须用以保存部分被移出（swap out）的工作集。存取这些硬盘甚至比 DRAM 存取要慢上好几个数量级。 幸运的是，不必做出非全有即全无的（all-or-nothing）选择。一台电脑可以有一个小容量的高速 SRAM，再加上大容量的 DRAM。一个可能的实作会是，将处理器定址空间的某块区域划分来容纳 SRAM，剩下的则给 DRAM。操作系统的任务就会是最佳化地分配资料以善用 SRAM。基本上，在这种情境下，SRAM 是作为处理器的暂存器集的扩充来使用的。 虽然这是可能的实作，但并不可行。忽略将 SRAM memory的实体资源映射到处理器的虚拟（virtual）定址空间的问题（这本身就非常难），这个方法会需要令每个行程（process）在软件上管理memory区域的分配。memory区域的大小因处理器而异（也就是说，处理器有著不同容量的昂贵 SRAM memory）。组成一支程序的每个模组都会要求它的那份快速memory，这会由于同步的需求而引入额外的成本。简而言之，拥有快速memory的获益将会完全被管理资源的间接成本（overhead）给吃掉。 所以，并非将 SRAM 置于操作系统或者使用者的控制之下，而是让它变成由处理器透明地使用与管理的资源。在这种方式下，SRAM 是用以产生主memory中可能不久就会被处理器用到的资料的暂时副本。因为程序码与资料具有时间（temporal）与空间局部性（spatial locality）。这表示，在短时间内，很可能会重复用到同样的程序码或资料。对程序码来说，这表示非常有可能会在程序码中循环（loop），使得相同的程序码一次又一次地执行（空间局部性的完美例子）。资料存取在理想上也会被限定在一小块区域中。即使在短时间内用到的memory并非邻近，同样的资料也有很高的机会在不久后再次用到（时间局部性）。对程序码来说，代表 –– 举例来说 –– 在一轮回圈中会产生一次函式呼叫（function call），这个函式在memory中可能很远，但呼叫这个函式在时间上则会很接近。对资料来说，代表一次使用的memory总量（工作集大小）理想上是有限的，但使用的memory –– 由于 RAM 随机存取的本质 –– 并不是相邻的。理解局部性的存在是 CPU cache概念的关键，因为我们至今仍在使用它们。 一个简单的计算就能看出cache在理论上有多有效。假设存取主memory花费 200 个周期，而存取cachememory花费 15 个周期。接著，程序使用 100 个资料元素各 100 次，若是没有cache，将会在memory操作上耗费 2,000,000 个循环，而若是所有资料都被cache过，只要 168,500 个周期。提升了 91.5%。 用作cache的 SRAM 大小比起主memory小了好几倍。根据作者使用具有 CPU cache的工作站（workstation）的经验，cache的大小总是主memory大小的 1/1000 左右（现今：4MB cache与 4GB 主memory）。单是如此并不会形成问题。假如工作集（正在处理的资料集）的大小比cache大小还小，这无伤大雅。但是电脑没理由不拥有大量的主memory。工作集必定会比cache还大。尤其是执行多个行程的系统，其工作集的大小为所有个别的处理器与系统核心的大小总和。 应对cache的大小限制所需要的是，一组能在任何给定的时间点决定什么该cache的策略。由于并非所有工作集的资料都会正好在相同的时间点使用，所以我们可以使用一些技术来暂时地将一些cache中的资料替换成别的资料。而且这也许能在真的需要资料之前就搞定。这种预取会去除一些存取主memory的成本，因为对程序的执行而言，这是非同步进行的。这所有的技术都能用来让cache看起来比实际上还大。我们将会在 3.3 节讨论它们。一旦探究完这全部的技术，协助处理器就是程序开发者的责任了。这些作法将会在第六节中讨论。 "},"cpu-caches/cpu-caches-in-the-big-picture.html":{"url":"cpu-caches/cpu-caches-in-the-big-picture.html","title":"3.1. 概观 CPU cache","keywords":"","body":"3.1. 概观 CPU cache 在深入 CPU cache的技术细节之前，某些读者或许会发现，先理解cache是如何融入现代电脑系统的「大局（big picture）」是有所帮助的。 图 3.1：最简易的cache配置 图 3.1 显示了最简易的cache配置。其与能在早期找到的、采用 CPU cache的系统架构是一致的。CPU 核不再直接连结到主memory。16所有的载入与储存都必须经过cache。CPU 核与cache之间的连线是一条特殊的、快速的连线。在这个简化的示意图上，主memory与cache都被连结到系统总线，其也会用来跟其它系统元件通讯。我们已经以「FSB」介绍过系统总线，这是它现今使用的名称；见 2.2 节。在这一节中，我们会省略北桥；假定它存在，以方便 CPU 与主memory的沟通。 即便过去数十年来的大多电脑都采用冯纽曼架构（von Neumann architecture），但实验证实分离程序码与资料的cache是比较好的。Intel 自 1993 年起采用分离程序码与资料的cache，就再也没有回头过。程序码与资料所需的memory区域彼此相当独立，这也是独立的cache运作得更好的原因。近年来，另一个优点逐渐浮现：对大多数常见的处理器而言，指令解码（decoding）的步骤是很慢的；cache解码过的指令能够让执行加速，在不正确地预测或者无法预测的分支（branch）使得管线（pipeline）为空的情况下尤其如此。 在引入cache之后不久，系统变得越来越复杂。cache与主memory之间的速度差异再次增加，直到加入了另一层级的cache，比起第一层cache来得更大也更慢。仅仅提升第一层cache的大小，以经济因素来说并非一个可行的办法。今日，甚至有正常使用、具有三层cache的机器。具有这种处理器的系统看起来就像图 3.2 那样。随著单一 CPU 中的核数增加，未来cache层级也许会变得更多。 图 3.2：具有三层cache的处理器 图 3.2 显示了三层cache，并引入了我们将会在本文其余部分使用的术语。L1d 是一阶资料cache、L1i 是一阶指令cache等等。注意，这只是张示意图；实际上资料流从处理器核到主memory的路上并不需要通过任何较高层级的cache。CPU 设计者在cache介面的设计上有著很大的自由。对程序开发者来说，是看不到这些设计上的抉择的。 此外，我们有多核的处理器，每个处理器核都能拥有多条「执行绪」(thread)。一个处理器核与一条执行绪的差别在于，不同的处理器核拥有（几乎17）所有硬件资源各自的副本。除非同时用到相同的资源 –– 像是对外连线，否则处理器核是能够完全独立运作的。另一方面，执行绪则共享几乎所有处理器的资源。Intel 的执行绪实作只让其拥有个别的暂存器，甚至还是有限的 –– 某些暂存器是共享的。所以，现代 CPU 的完整架构看起来就像图 3.3。 图 3.3：多处理器、多核、多执行绪 在这张图中，我们有两颗处理器，每颗两个核，各自拥有两个执行绪。执行绪共享一阶cache。处理器核（以深灰色为底）拥有独立的一阶cache。所有 CPU 核共享更高层级的cache。两颗处理器（两个浅灰色为底的大方块）自然不会共享任何cache。这些全都很重要，在我们讨论cache对多行程与多执行绪应用程序的影响时尤为如此。 16. 在更早期的系统中，cache是如同 CPU 与主memory一样接到系统总线上的。这比起实际的解法，更像是一种临时解（hack）。 ↩ 17. 早期的多核处理器甚至有分离的第 2 阶cache (L2)，且无第 3 阶cache。 ↩ "},"cpu-caches/cache-operation-at-high-level.html":{"url":"cpu-caches/cache-operation-at-high-level.html","title":"3.2. 高阶cache操作","keywords":"","body":"3.2. 高阶cache操作 我们必须结合第二节所学到的机器架构与 RAM 技术、以及前一节所描述的cache结构，以了解使用cache的开销与节约之处。 预设情况下，由 CPU 核读取或写入的所有资料都存在cache中。有些memory区域无法被cache，但只有操作系统实作者得去挂虑这点；这对应用程序开发者而言是不可见的。也有一些指令能令程序开发者刻意地绕过某些cache。这些将会在第六节中讨论。 假如 CPU 需要一个资料字组，会先从cache开始搜寻。显而易见地，cache无法容纳整个主memory的内容（不然我们就不需要cache），但由于所有memory地址都能被cache，所以每个cache项目（entry）都会使用资料字组在主memory中的地址来标记（tag）。如此一来，读取或写入到某个地址的请求便会在cache中搜寻符合的标籤。在这个情境中，地址可以是虚拟或实体的，视cache的实作而有所不同。 除了真正的memory之外，标籤也会需要额外的空间，因此使用一个字组作为cache的粒度（granularity）是很浪费的。对于一台 x86 机器上的一个 32 位元字组而言，标籤本身可能会需要 32 位元以上。再者，由于空间局部性是作为cache基础的其中一个原理，不将此纳入考量并不太好。由于邻近的memory很可能会一起被用到，所以它也应该一起被载入到cache中。也要记得我们在 2.2.1 节所学到的：假如 RAM 模组能够在不需新的 CAS‾ \\overline{\\text{CAS}} ​CAS​​​、甚至是 RAS‾ \\overline{\\text{RAS}} ​RAS​​​ 讯号的情况下传输多个资料字组，这是更有效率的。所以储存在cache中的项目并非单一字组，而是多个连续字组的「行（line）」。在早期的cache中，这些行的长度为 32 位元组；如今一般是 64 位元组。假如memory总线的宽度是 64 位元，这表示每个cache行要传输 8 次。DDR 有效地支援这种传输方式。 当memory内容为处理器所需时，整个cache行都会被载入到 L1d 中。每个cache行的memory地址会根据cache行的大小，以遮罩（mask）地址值的方式来计算。对于一个 64 位元组的cache行来说，这表示低 6 位元为零。舍弃的位元则用作cache行内的偏移量（offset）译注。剩馀的位元在某些情况下用以定位cache中的行、以及作为标籤。在实务上，一个地址值会被切成三个部分。对于一个 32 位元的地址来说，这看来如下： 一个大小为 2O \\mathbf{O} O 的cache行，低 O \\mathbf{O} O 位元用作cache行内的偏移量。接下来的 S \\mathbf{S} S 位元选择「cache集（cache set）」。我们马上就会深入更多为何cache行会使用集合 –– 而非一个一组（single slot）–– 的细节。现在只要知道有 2S \\mathbf{S} S 个cache行的集合就够。剩下的 32−S−O=T 32 - \\mathbf{S} - \\mathbf{O} = \\mathbf{T} 32−S−O=T 位元组成标籤。这 T \\mathbf{T} T 个位元是与每个cache行相关联、以区分在同一cache集中所有别名（alias）18的值。不必储存用以定址cache集的 S \\mathbf{S} S 位元，因为它们对同个集合中的所有cache行而言都是相同的。 当一个指令修改memory时，处理器依旧得先载入一个cache行，因为没有指令能够一次修改一整个cache行（这个规则有个例外：合并写入〔write-combining〕，会在 6.1 节说明）。因此在写入操作之前，得先载入cache行的内容。cache无法持有不完全的cache行。已被写入、并且仍未写回主memory的cache行被称为「脏的（dirty）」。一旦将其写入，脏旗标（dirty flag）便会被清除。 为了能够在cache中载入新的资料，几乎总是得先在cache中腾出空间。从 L1d 的逐出操作（eviction）会将cache行往下推入 L2（使用相同的cache行大小）。这自然代表 L2 也得腾出空间。这可能转而将内容推入 L3，最终到主memory中。每次逐出操作都会越来越昂贵。这里所描述的是现代 AMD 与 VIA 处理器所优先采用的独占式cache（exclusive cache）模型。Intel 实作包含式cache（inclusive caches）19，其中每个在 L1d 中的cache行也会存在 L2 中。因此，从 L1d 进行逐出操作是更为快速的。有了足够的 L2 cache的话，将内容存在两处而造成memory浪费的缺点是很小的，而这在逐出操作时会带来回报。独占式cache的一个可能的优点是，载入一个新的cache行只需碰到 L1d 而不需 L2，这会快上一些。 只要为了处理器架构而规定的memory模型没有改变，CPU 是被允许以它们想要的方式来管理cache的。举例来说，善用少量或没有memory总线活动的时段，并主动地将脏的cache行写回到主memory中，对处理器来说是非常好的。x86 与 x86-64 –– 不同厂商、甚至是同一厂商的不同型号之间 –– 的处理器之间有著各式各样的cache架构，证明memory模型抽象化的能力。 在对称式多处理器（Symmetric Multi-Processor，SMP）系统中，CPU 的cache无法独立于彼此运作。所有处理器在任何时间都假定要看到相同的memory内容。这种memory一致观点的维持被称为「cache一致性（cache coherency）」。假如一个处理器只看它自己拥有的cache与主memory，它就不会看到其它处理器中的脏cache行的内容。提供从一个处理器到另一个处理器cache的直接存取会非常昂贵，而且是个极大的瓶颈。取而代之地，处理器会在另一个处理器要读取或写入到某个cache行时察觉到。 假如侦测到一次写入存取，并且处理器在其cache中有这个cache行的干净副本，这个cache行就会被标为无效（invalid）。未来的查询会需要重新载入这个cache行。注意到在另一颗 CPU 上的读取存取并不需要进行无效化，多个干净副本能够被保存得很好。 更加复杂的cache实作容许其它的可能性发生。假设在一个处理器cache中的一个cache行是脏的，并且第二个处理器想要读取或写入这个cache行。在这个情况下，主memory的内容太旧，而请求的处理器必须 –– 作为替代 –– 从第一个处理器取得cache行的内容。第一个处理器经由窥探注意到这个状况，并自动地将资料寄送给请求的处理器。这个动作绕过主memory，虽然在某些实作中，是假定memory控制器会注意到这个直接传输、并将更新的cache行内容储存到主memory中。假如是为了写入而进行存取，第一个处理器便会将它的区域cache行的副本无效化。 许多cache一致化的协议随著时间被逐渐发展出来。最重要的为 MESI，我们将会 3.3.4 节中介绍它。这所有的结果可以被总结为一些简单的规则： 一个脏的cache行不会出现在任何其它处理器的cache中。 相同cache行的干净副本能够存在任意数量的cache中。 假如能够维持这些规则，即便在多处理器的系统中，处理器也能够高效地使用它们的cache。所有处理器所需要做的，就是去监控其它处理器的写入存取，并将这个地址与它们区域cache中的地址做比较。在下一节，我们将会深入更多实作、尤其是成本的一些细节。 最后，我们该至少给个cache命中（hit）与错失（miss）相关成本的印象。这些是 Intel 针对 Pentium M 列出的数字： 到 周期 暂存器 L1d ~3 L2 ~14 主memory ~240 这些是以 CPU 周期测量的实际存取时间。有趣的是，对内建于晶片上的 L2 cache而言，大部分（甚至可能超过一半）的存取时间都是由线路延迟造成的。这是一个只会随著cache大小变大而变糟的实体限制。只有制程的缩小（举例来说，从 Intel 系列中 Merom 的 60nm 到 Penryn 的 45nm）能提升这些数字。 表格中的数字看起来很大，但 –– 幸运地 –– 不必在每次发生cache载入与错失时都负担全部的成本。一部分的成本可以被隐藏。现今的处理器全都会使用不同长度的内部管线，指令会在其中被解码、并且为执行而准备。部份的准备是从memory（或cache）载入值，假如它们要被传输到暂存器的话。假如memory载入操作能够足够早就在管线中开始，它也许会与其它操作平行进行，而整个载入成本就可能被隐藏。这对 L1d 经常是可能的；对某些有著长管线的处理器来说，L2 亦是如此。 提早开始memory读取有著诸多阻碍。也许简单得像是没有足够的资源来存取memory，或者可能是载入的最终地址之后才会作为另一个指令的结果取得。在这些情况中，载入成本无法被（完全地）隐藏。 对于写入操作，CPU 不必一直等到值被安然地储存进memory中为止。只要接下来指令的执行就像是与值已被存入memory有著似乎相同的效果，就没有什么能阻止 CPU 走捷径。它能够早点开始执行下个指令。有著影子暂存器（shadow register）–– 其能够持有一般暂存器无法取得的值 –– 的帮助，甚至可能改变未完成的写入操作所要储存的值。 图 3.4：随机写入的存取时间 有关cache行为影响的图表，见图 3.4。我们稍候会谈到产生资料的程序；这是个不停地以随机的方式存取可控制memory总量的程序的简易模拟。每笔资料有著固定的大小。元素的数量视选择的工作集大小而定。Y 轴表示处理一个元素所花费的 CPU 周期的平均；注意到 Y 轴为对数刻度。这同样适用于所有这类图表的 X 轴。工作集的大小总是以二的幂次表示。 这张图显示三个不同的平稳阶段。这并不让人意外：这个处理器有 L1d 与 L2 cache，但没有 L3。经由一些经验，我们可以推论这个 L1d 大小为 213 位元组，而 L2 大小为 220 位元组。假如整个工作集能塞进 L1d 中，对每个元素的每次操作的周期数会低于 10。一旦超过 L1d 的大小，处理器就必须从 L2 载入资料，而平均时间则迅速成长到 28 左右。一旦 L2 也不够大，时间便飙升到 480 个周期以上。这即是许多、或者大部分操作必须从主memory载入资料的时候。更糟的是：由于资料被修改，脏的cache行也必须被写回。 这张图应该有给予探究程序撰写上的改进、以协助提升cache使用方式的充分动机。我们在这里所谈论的并不是几个少得可怜的百分点；我们说的是有时可能的几个数量级的提升。在第六节，我们将会讨论能让我们写出更有效率的程序的技术。下一节会深入更多 CPU cache设计的细节。有这些知识很好，但对于本文其余部分并非必要。所以这一节可以跳过。 译注. 用来作为cache行内某个字组的索引。 ↩ 18. 所有地址有著相同 S \\mathbf{S} S 部分的cache行都被视为相同的别名。 ↩ 19. 这个概括并不完全正确。一些cache是独占式的，而部分包含式cache具有独占式cache的特性。 ↩ "},"cpu-caches/cpu-cache-implementation-details.html":{"url":"cpu-caches/cpu-cache-implementation-details.html","title":"3.3. CPU cache实作细节","keywords":"","body":"3.3. CPU cache实作细节 cache实作者有个麻烦是，在庞大的主memory中，每个记忆单元都可能得被cache。假如一支程序的工作集足够大，这表示有许多为了cache中的各个地方打架的主memory位置。先前曾经提过，cache与主memory大小的比率为 1 比 1000 的情况并不罕见。 "},"cpu-caches/cpu-cache-implementation-details/associativity.html":{"url":"cpu-caches/cpu-cache-implementation-details/associativity.html","title":"3.3.1. 关联度","keywords":"","body":"3.3.1. 关联度 实作一个每个cache行都能保存任意memory位置副本的cache是有可能的（见图 3.5）。这被称为一个全关联式cache（fully associative cache）。要存取一个cache行，处理器核必须要将每个cache行的标籤与请求地址的标籤进行比较。标籤会由地址中不是cache行偏移量的整个部分组成（这表示在 3.2 节图示中的 S \\mathbf{S} S 为零）。 有些cache是像这样实作的，但是看看现今使用的 L2 数量，证明这是不切实际的。给定一个有著 64B cache行的 4MB cache，这个cache将会有 65,536 个项目。为了达到足够的效能，cache逻辑必须要能够在短短几个周期内，从这所有的项目中挑出符合给定标籤的那一个。实作这点要付出庞大的精力。 图 3.5：全关联式cache示意图 对每个cache行来说，都需要一个比较器（comparator）来比对很大的标籤（注意，S \\mathbf{S} S 为零）。紧邻著每条连线的字母代表以位元为单位的宽度。假如没有给定，那么它就是一条单一位元的线路。每个比较器都必须比对两个 T \\mathbf{T} T 位元宽的值。接著，基于这个结果，选择合适的cache行内容，并令它能被取得。有多少cache行，都得合并多少组 O \\mathbf{O} O 资料线。实作一个比较器所需的晶体管数量很大，特别是它必须运作地非常快的时候。叠代比较器（iterative comparator）是不可用的。节省比较器数量的唯一方式，就是反覆地比较标籤以减少比较器的数量。这与叠代比较器并不合适的理由相同：它太花时间了。 全关联式cache对小cache（例如在某些 Intel 处理器的 TLB cache就是全关联式的）来说是有实用价值的，但那些cache都很小，非常小。我们所指的是至多只有几十个项目的情况。 对 L1i、L1d、以及更高层级的cache来说，需要采用不同的方法。我们所能做的是限缩搜寻。在最极端的限制中，每个标籤都恰好对映到一个cache项目。计算方式很简单：给定 4MB／64B、有著 65,536 个项目的cache，我们能够直接使用地址的 6 到 21 位元（16 个位元）来直接定址每个项目。低 6 位元是cache行内部的索引。 图 3.6：直接对映式cache示意图 如图 3.6 所见到的，这种直接对映式cache（direct-mapped cache）很快，而且实作起来相对简单。它需要一个比较器、一个多工器（在这张示意图中有两个，标籤与资料是分离的，但在这个设计上，这点并不是个硬性要求）、以及一些用以选择有效cache行内容的逻辑。比较器是因速度要求而复杂，但现在只有一个；因此，便能够花费更多的精力来让它变快。在这个方法中，实际的复杂之处都落在多工器上。在一个简易的多工器上，晶体管的数量以 O(logN) O(\\log N) O(logN) 成长，其中 N N N 为cache行的数量。这能够容忍，但可能会慢了点，在这种情况下，借由在多工器中增加更多的晶体管以平行化某些工作，便能够提升速度。晶体管的总数能够缓慢地随著cache大小的成长而成长，使得这种解法非常有吸引力。但它有个缺点：只有在程序用到的地址，对于用以直接映射的位元来说是均匀分布的情况下，它才能运作得很好。若非如此，而且经常这样的话，某些cache项目会因为频繁地使用而被重复地逐出，而其余的项目则几乎完全没用到、或者一直是空的。 图 3.7：集合关联式cache示意图 这个问题能借由让cache集合关联（set associative）来解决。一个集合关联式cache结合了全关联式以及直接对映式cache的良好特质，以在很大程度上避免了那些设计的弱点。图 3.7 显示了一个集合关联式cache的设计。标籤与资料的储存被分成集合，其中之一会被cache行的地址所选择。这与直接对映式cache相似。但少数的值能以相同的集合编号cache，而非令cache中的每个集合编号都只有一个元素。所有集合内成员的标籤会平行地比对，这与全关联式cache的运作方式相似。 结果是，cache不容易被不幸地 –– 或者蓄意地 –– 以相同集合编号的地址选择所击败，同时cache的大小也不会受限于能被经济地实作的比较器的数量。假如cache增长，它（在这张图中）只有行数会增加，列数则否。行数（以及比较器）只会在cache的关联度（associativity）增加的时候才会增加。现今的处理器为 L2 或者更高层级的cache所使用的关联度层级高达 24。L1 cache通常使用 8 个集合。 给定我们的 4MB／64B cache以及 8 路（8-way）集合关联度，于是这个cache便拥有 8,192 个集合，并且仅有 13 位元的标籤被用于定址cache集。要决定cache集中的哪个（如果有的话）项目包含被定址的cache行，必须要比较 8 个标籤。在非常短的时间内做到如此是可行的。借由实验我们能够看到，这是合理的。 L2 cache大小 关联度 直接 2 4 8 CL=32 CL=64 CL=32 CL=64 CL=32 CL=64 CL=32 CL=64 512k 27,794,595 20,422,527 25,222,611 18,303,581 24,096,510 17,356,121 23,666,929 17,029,334 1M 19,007,315 13,903,854 16,566,738 12,127,174 15,537,500 11,436,705 15,162,895 11,233,896 2M 12,230,962 8,801,403 9,081,881 6,491,011 7,878,601 5,675,181 7,391,389 5,382,064 4M 7,749,986 5,427,836 4,736,187 3,159,507 3,788,122 2,418,898 3,430,713 2,125,103 8M 4,731,904 3,209,693 2,690,498 1,602,957 2,207,655 1,228,190 2,111,075 1,155,847 16M 2,620,587 1,528,592 1,958,293 1,089,580 1,704,878 883,530 1,671,541 862,324 表 3.1：cache大小、关联度、以及cache行大小的影响 表 3.1 显示了对于一支程序（在这个例子中是 gcc，根据 Linux 系统核心的人们的说法，它是所有基准中最重要的一个）在改变cache大小、cache行大小、以及关联度集合大小时，L2 cache错失的次数。在 7.2 节中，我们将会介绍对于这个测试，所需要用以模拟cache的工具。 以防这些值的关联仍不明显，这所有的值的关系是，cache的大小为 cache行大小×关联度×集合的数量 \\text{cache行大小} \\times \\text{关联度} \\times \\text{集合的数量} cache行大小×关联度×集合的数量 地址是以 3.2 节的图中示意的方式，使用 O=log2cache行大小S=log2集合的数量 \\begin{aligned} \\mathbf{O} &= \\log_{2} \\text{cache行大小} \\\\ \\mathbf{S} &= \\log_{2} \\text{集合的数量} \\end{aligned} ​O​S​​​=log​2​​cache行大小​=log​2​​集合的数量​​ 来对映到cache中的。 图 3.8：cache大小 vs 关联度（CL=32） 图 3.8 让这个表格的数据更容易理解。它显示了cache行大小固定为 32 位元组的数据。看看对于给定cache大小的数字，我们可以发现关联度确实有助于显著地降低cache错失的次数。以一个 8MB cache来说，从直接对映式变成 2 路集合关联式避免了几乎 44% 的cache错失。相比于一个直接对应式cache，使用一个集合关联式cache的话，处理器能够在cache中保存更多的工作集。 在文献中，偶尔会读到引入关联度与加倍cache大小有著相同的影响。在某些极端的例子中，如同能够从 4MB 跳到 8MB cache所看到的，确实如此。但再一次加倍关联度的话，显然就不是如此了。如同我们能从数据中所看到的，接下来的提升要小得多。不过，我们不该完全低估这个影响。在范例程序中，memory使用的尖峰为 5.6M。所以使用一个 8MB cache，同样的cache集不大可能被多次（超过两次）使用。有个较大的工作集的话，能够节约的更多。如同我们能够看到的，对于较小的cache大小来说，关联度的获益较大。 一般来说，将一个cache的关联度提升到 8 以上，似乎对一个单执行绪的工作量来说只有很小的影响。随著共享第一层cache的 HT 处理器、以及使用一个共享 L2 cache的多核处理器的引入，形势转变了。现在你基本上会有两支程序命中相同的cache，这导致关联度会在实务上打对折（对四核处理器来说是四分之一）。所以能够预期，提升处理器核的数量，共享cache的关联度也应该成长。一旦这不再可能（16 路集合关联度已经很难了），处理器设计师就必须开始使用共享的 L3 或者更高层级的cache，而 L2 cache则是潜在地由处理器核的子集所共享。 我们能在图 3.8 学到的另一个影响是，增加cache大小是如何提升效能的。这个数据无法在不知道工作集大小的情况下解释。显然地，一个与主memory一样大的cache，会导致比一个较小cache更好的结果，所以一般来说不会有带著可预见优势的最大cache大小的限制。 如同上面所提到的，工作集大小的尖峰为 5.6M。这并没有给我们任何最佳cache大小的确切数字，但它能让我们估算出这个数字。问题是，并非所有被用到的memory都是连续的，因此我们会有 –– 即使是以一个 16M 的cache与一个 5.6M 的工作集 –– 冲突（conflict）译注（看看 2 路集合关联式的 16MB cache相较于直接对映版本的优势）。但有把握的是，以同样的工作量，一个 32MB cache的获益是可以忽略不计的。但谁说过工作集大小必须维持不变了？工作量是随著时间成长的，cache大小也应该如此。在购买机器、并且在你得去挑选愿意为此买单的cache大小时，是值得去衡量工作集大小的。在图 3.10 中能够看到这件事何以重要。 图 3.9：测试memory布局 执行了两种类型的测试。在第一个测试中，元素是循序处理的。测试程序沿著指标（pointer）n 前进，但阵列元素会以令它们以在memory中排列的顺序被巡访的方式链结。这能够在图 3.9 的下半部看到。有个来自最后一个元素的回溯参考。在第二个测试中（图中的上半部），阵列元素是以随机顺序巡访的。在这两种情况中，阵列元素都会形成一个循环的单向链结串列 (singly-linked list)。 译注. 这里指的是上文描述直接对映式cache时所提到的缺点。 ↩ "},"cpu-caches/cpu-cache-implementation-details/measurements-of-cache-effects.html":{"url":"cpu-caches/cpu-cache-implementation-details/measurements-of-cache-effects.html","title":"3.3.2. cache影响的量测","keywords":"","body":"3.3.2. cache影响的量测 所有的图表都是由一支能模拟任意大小的工作集、读取与写入存取、以及循序或随机存取的程序所产生的。我们已经在图 3.4 中看过一些结果。这支程序会产生与工作集大小相同、这种型别的阵列： struct l { struct l *n; long int pad[NPAD]; }; 所有的项目都使用 n 元素，以循序或是随机的顺序，链结在一个循环的串列中。即使元素是循序排列的，从一个项目前进到下一个项目总是会用到这个指标。pad 元素为资料负载（payload），并且能成长为任意大小。在某些测试中，资料会被修改，而在其余的情况中，程序只会执行读取操作。 在效能量测中，我们讨论的是工作集的大小。工作集是由一个 struct l 元素的阵列所组成的。一个 2N 位元组的工作集包含 2N / sizeof(struct l) 个元素。显而易见地，sizeof(struct l) 视 NPAD 的值而定。以 32 位元的系统来说，NPAD=7 代表每个阵列元素的大小为 32 位元组，以 64 位元的系统来说，大小为 64 位元组。 单执行绪循序存取 图 3.10：循序读取存取，NPAD=0 最简单的情况就是直接走遍串列中的所有项目。串列元素是循序排列、紧密地塞在一起的。不管处理的顺序是正向或反向都无所谓，处理器在两个方向上都能处理得一样好。我们这里 –– 以及在接下来的所有测试中 –– 所要量测的是，处理一个单向串列元素要花多久。时间单位为处理器周期。图 3.10 显示了这个结果。除非有另外说明，否则所有的量测都是在一台 Pentium 4 以 64 位元模式获得的，这表示 NPAD=0 的结构 l 大小为八位元组。 前两个量测结果受到了杂讯的污染。测量的工作量太小了，因而无法过滤掉其余系统的影响。我们能够放心地假设这些值都在 4 个周期左右。考虑到这点，我们能够看到三个不同的水平（level）： 工作集大小至多到 214 位元组。 从 215 位元组到 220 位元组。 221 位元组以上。 这些阶段能够轻易地解读：处理器拥有一个 16kB L1d 与 1MB L2。我们没有在从一个水平到另一个水平的转变之处看到尖锐的边缘，因为cache也会被系统的其它部分用到，因此cache并不是专门给这支程序的资料所使用的。特别是 L2 cache，它是一个统一式cache（unified cache），也会被用来存放指令（注：Intel 使用包含式cache）。 或许完全没有预期到的是，对于不同工作集大小的实际时间。L1d 命中的时间是预期中的：在 P4 上，L1d 命中之后的载入时间大约是 4 个周期。但 L2 存取怎么样呢？一旦 L1d 不足以保存资料，可以预期这会让每个元素花上 14 个周期以上，因为这是 L2 的存取时间。但结果显示只需要大约 9 个周期。这个差异能够以处理器中的先进逻辑来解释。预期使用连续的memory区域时，处理器会预取下一个cache行。这表示，当真的用到下个cache行时，它已经载入一半了。等待下一个cache行载入所需的延迟因而比 L2 存取时间要少得多。 一旦工作集大小成长到超过 L2 的大小，预取的效果甚至更明显。先前我们说过，一次主memory存取要花费 200+ 个周期。只有利用有效的预取，处理器才可能让存取时间维持在低至 9 个周期。如同我们能从 200 与 9 之间的差异所看到的，它的效果很好。 图 3.11：循序读取多种大小 我们能够在预取的时候 –– 至少间接地 –– 观察处理器。在图 3.11 中，我们看到的是相同工作集大小的时间，但这次我们看到的是不同 l 结构大小的曲线。这表示在串列中有比较少、但比较大的元素。不同大小有著令（仍然连续的）串列中的 n 元素之间的距离成长的影响。在图中的四种情况，距离分别为 0、56、120、248 位元组。 在底部我们可以看到图 3.10 的线，但这时它看起来差不多像是条平坦的线。其它情况的时间要糟得多了。我们也能在这张图中看到三个不同的水平，我们也看到在工作集大小很小的情况下有著很大的误差（再次忽略它们）。只要仅有 L1d 牵涉其中，这些线差不多都相互重合。 There is no prefetching necessary so all element sizes just hit the L1d for each access. 在 L2 cache命中的情况下，我们看到三条新的线相互重合得很好，但它们位在比较高的水平上（大约 28）。这是 L2 存取时间的水平。这表示从 L2 到 L1d 的预取基本上失效了。即使是 NPAD=7，我们在回圈的每一次叠代都需要一个新的cache行；以 NPAD=0 而言，在需要下一个cache行之前，回圈得叠代八次。预取逻辑无法每个周期都载入一个新的cache行。因此，我们看到的便是在每次叠代时，从 L2 载入的延误。 一旦工作集大小超过 L2 的容量，甚至变得更有趣了。现在四条线全都离得很远。不同的元素大小显然在效能差异上扮演著一个重大的角色。处理器应该要识别出步伐（stride）的大小，不为 NPAD=15 与 31 获取不必要的cache行，因为元素的大小是比预取窗（prefetch window）还小的（见 6.3.1 节）。元素大小妨碍预取效果之处，是一个硬件预取限制的结果：它无法横跨分页（page）边界。我们在每次增加大小时，都减少了 50% 硬件排程器（scheduler）的效率。假如硬件预取器（prefetcher）被允许横跨分页边界，并且下一个分页不存在或者无效时，操作系统就得被捲入分页的定位中。这表示程序要经历并非由它自己产生的分页错误（page fault）。这是完全无法接受的，因为处理器并不知道一个分页是不在memory内还是不存在。在后者的情况下，操作系统必须要中断行程。在任何情况下，假定 –– 以 NPAD=7 或以上而言 –– 每个串列元素都需要一个cache行，硬件预取器便爱莫能助了。由于处理器一直忙著读取一个字组、然后载入下一个元素，根本没有时间去从memory载入资料。 变慢的另一个主要原因是 TLB cache的错失。这是一个储存了从虚拟地址到实体地址的转译结果的cache，如同在第四节所详细解释的那样。由于 TLB cache必须非常地快，所以它非常地小。假如重复存取的分页数比 TLB cache拥有的还多，就必须不断地重算代表著虚拟到实体地址的转译结果的项目。这是一个非常昂贵的操作。对比较大的元素大小而言，一次 TLB 查询的成本是分摊在较少的元素上的。这表示对于每个串列元素，必须要计算的 TLB 项目总数较多。 为了观察 TLB 的影响，我们可以执行一个不同的测试。对于第一个量测，我们像往常一样循序地摆放元素。我们使用 NPAD=7 作为占据一整个cache行的元素。对于第二个量测，我们将每个串列元素放置在个别的分页中。每个分页的其余部分维持原样，我们不会将它算在工作集大小的总和中。20结果是，对于第一个量测，每次串列叠代都需要一个新的cache行，并且每 64 个元素一个新的分页。对第二个量测而言，每次叠代都需要载入一个在另一个分页上的cache行。 图 3.12：TLB 对循序读取的影响 结果可以在图 3.12 中看到。量测都是在与图 3.11 相同的机器上执行的。由于可用 RAM 的限制，工作集大小必须限制在 224 位元组，其需要 1GB 以将物件放置在个别的分页上。下方的红色曲线正好对应到图 3.11 中的 NPAD=7 曲线。我们看到了显示了 L1d 与 L2 cache大小的不同阶段。第二条曲线看起来完全不同。重要的特征是，当工作集大小达到 213 位元组时开始的大幅飙升。这即是 TLB cache溢出（overflow）的时候了。由于一个元素大小为 64 位元组，我们能够计算出 TLB cache有 64 个项目。由于程序锁定了memory以避免它被移出，所以成本不会受分页错误影响。 可以看出，计算实体地址、并将它储存在 TLB 中所花的周期数非常高。图 3.12 中的曲线显示了极端的例子，但现在应该能清楚的一点是，对于较大的 NPAD 值而言，一个变慢的重大因素即是 TLB cache效率的降低。由于实体地址必须要在cache行能从 L2 或主memory读取前算出来，因此地址转译的损失就被附加到了memory存取时间上。这在某种程度上解释了，为何每个串列元素在 NPAD=31 的总成本会比 RAM 在理论上的存取时间还高的原因。 图 3.13：循序读取与写入，NPAD=1 我们可以透过观察修改串列元素的测试执行的数据，来一瞥预取实作的多一些细节。图 3.13 显示了三条线。在所有情况中的元素宽度都是 16 位元组。第一条线是现在已经很熟悉的串列巡访，它会被当作一条基准线。第二条线 –– 标为「Inc」–– 仅会在前往下一个元素前，增加当前元素的 pad[0] 成员的值。第三条线 –– 标为「Addnext0」–– 会取下一个元素的 pad[0] 的值，并加到当前串列元素的 pad[0] 成员中。 天真的假设大概是「Addnext0」测试跑得比较慢，因为它有更多工作得做。在前进到下一个串列元素之前，就必须载入这个元素的值。这即是看到这个测试实际上 –– 对于某些工作集大小而言 –– 比「Inc」测试还快这点会令人吃惊的原因了。对此的解释是，载入下个串列元素基本上就是一次强制的预取。无论程序在何时前进到下个串列元素，我们都确切地知道这个元素已经在 L1d cache中了。因此我们看到，只要工作集大小能塞进 L2 cache，「Addnext0」就执行得跟单纯的「Follow」一样好。 不过「Addnext0」测试比「Inc」测试更快耗尽 L2。因为它需要从主memory载入更多的资料。这即是在工作集大小为 221 位元组时，「Addnext0」测试达到 28 个循环水平的原因了。28 循环水平是「Follow」测试所达到的 14 循环水平的两倍高。这也很容易解释。由于其它两个测试都修改了memory，L2 cache为了腾出空间给新的cache行的逐出操作便不能直接把资料丢掉。它必须被写到memory中。这表示 FSB 中的可用频宽被砍了一半，因此加倍了资料从主memory传输到 L2 所花的时间。 图 3.14：较大 L2／L3 cache的优势 循序、高效的cache管理的最后一个面向是cache的大小。虽然这应该很明显，但仍需要被提出来。图 3.14 显示了以 128 位元组元素（在 64 位元机器上，NPAD=15）进行 Increment 测试的时间。这次我们看到量测结果来自三台不同的机器。前两台机器为 P4，最后一台为 Core2 处理器。前两台由不同的cache大小来区分它们自己。第一个处理器有一个 32k L1d 与一个 1M L2。第二个处理器有 16k L1d、512k L2、与 2M L3。Core2 处理器有 32k L1d 与 4M L2。 这张图有趣的部分不必然是 Core2 处理器相对于其它两个表现得有多好（虽然这令人印象深刻）。这里主要有兴趣的地方是，工作集大小对于各自的最后一阶cache来说太大、并使得主memory得大大地涉入其中之处。 如同预期，最后一阶的cache越大，曲线在相应于 L2 存取成本的低水平停留得越久。要注意的重要部分是它所提供的效能优势。第二个处理器（它稍微旧了一点）在 220 位元组的工作集上能够以两倍于第一个处理器的速度执行。这全都归功于最后一阶cache大小的提升。有著 4M L2 的 Core2 处理器甚至表现得更好。 对于随机的工作量而言，这可能不代表什么。但若是工作量能被裁剪成最后一阶cache的大小，程序效能便能够极为大幅地提升。这也是有时候值得为拥有较大cache的处理器花费额外金钱的原因。 单执行绪随机存取 我们已经看过，处理器能够借由预取cache行到 L2 与 L1d，来隐藏大部分主memory、甚至是 L2 的存取等待时间。不过，这只有在能够预测memory的存取时才能良好运作。 图 3.15：循序 vs 随机读取，NPAD=0 若是存取模式是不可预测、或者随机的，情况便大大地不同。图 3.15 比较了循序存取每个串列元素的时间（如图 3.10）以及当串列元素是随机分布在工作集时的时间。顺序是由随机化的链结串列所决定的。没有让处理器能够确实地预取资料的方法。只有一个元素偶然在另一个在memory中也彼此邻近的元素不久之后用到，这才能起得了作用。 在图 3.15 中，有两个要注意的重点。第一点是，增长工作集大小需要大量的周期数。机器能够在 200-300 个周期内存取主memory，但这里我们达到了 450 个周期以上。我们先前已经看过这个现象了（对比图 3.11）。自动预取在这里实际上起了反效果。 图 3.16：L2d 错失率 第二个有趣的地方是，曲线并不像在循序存取的例子中那样，在多个平缓阶段变得平坦。曲线持续上升。为了解释这点，我们能够针对不同的工作集大小量测程序的 L2 存取次数。结果能够在图 3.16 与表 3.2 看到。 图表显示，当工作集大小大于 L2 的大小时，cache错失率（L2 存取数 / L2 错失数）就开始成长了。这条曲线与图 3.15 的曲线有著相似的形式：它快速地上升、略微下降、然后再度开始上升。这与每串列元素所需循环数的曲线图有著密切的关联。L2 错失率最终会一直成长到接近 100% 为止。给定一个足够大的工作集（以及 RAM），任何随机选取的cache行在 L2 或是载入过程中的机率便能够被随心所欲地降低。 集合大小 循序 随机 L2 命中数 L2 错失数 叠代次数 错失／命中比率 每叠代 L2 存取数 L2 命中数 L2 错失数 叠代次数 错失／命中比率 每叠代 L2 存取数 220 88,636 843 16,384 0.94% 5.5 30,462 4721 1,024 13.42% 34.4 221 88,105 1,584 8,192 1.77% 10.9 21,817 15,151 512 40.98% 72.2 222 88,106 1,600 4,096 1.78% 21.9 22,258 22,285 256 50.03% 174.0 223 88,104 1,614 2,048 1.80% 43.8 27,521 26,274 128 48.84% 420.3 224 88,114 1,655 1,024 1.84% 87.7 33,166 29,115 64 46.75% 973.1 225 88,112 1,730 512 1.93% 175.5 39,858 32,360 32 44.81% 2,256.8 226 88,112 1,906 256 2.12% 351.6 48,539 38,151 16 44.01% 5,418.1 227 88,114 2,244 128 2.48% 705.9 62,423 52,049 8 45.47% 14,309.0 228 88,120 2,939 64 3.23% 1,422.8 81,906 87,167 4 51.56% 42,268.3 229 88,137 4,318 32 4.67% 2,889.2 119,079 163,398 2 57.84% 141,238.5 表 3.2：循序与随机巡访时的 L2 命中与错失，NPAD=0 光是cache错失率的提高就能够解释一部分成本。但有著另一个因素。看看表 3.2，我们能够看到在 L2 / 叠代数那栏，程序每次叠代所使用的 L2 总数都在成长。每个工作集都是前一个的两倍大。所以，在没有cache的情况下，我们预期主memory的存取次数会加倍。有了cache以及（几乎）完美的可预测性，我们看到显示在循序存取的数据中，L2 使用次数增长得很保守。其增长除了工作集大小的增加以外，就没有别的原因了。 图 3.17：逐页（page-wise）随机化，NPAD=7 对于随机存取，每次工作集大小加倍的时候，每个元素的存取时间都超过两倍。这表示每个串列元素的平均存取时间增加了，因为工作集大小只有变成两倍而已。背后的原因是 TLB 错失率提高了。在图 3.17 中，我们看到在 NPAD=7 时随机存取的成本。只是这次，随机化的方式被修改了。一般的情况下，是将整个串列作为一个区块（block）随机化（以标籤〔label〕 ∞ \\infty ∞ 表示），而其它的 11 条曲线则表示在比较小的区块内进行随机化。标记为「60」的曲线，代表每组由 60 个分页（245,760 位元组）组成的集合会分别进行随机化。这表示在走到下一个区块的元素之前，会先巡访过所有区块内的串列元素。这使得在任何一个时间点使用的 TLB 项目的数量有所限制。 在 NPAD=7 时的元素大小为 64 位元组，这与cache行大小一致。由于串列元素的顺序被随机化了，因此硬件预取器不大可能有任何效果，尤其在有一堆元素的情况下。这表示 L2 cache的错失率与在一个区块内的整个串列随机化相比并不会有显著地不同。测试的效能随著区块大小增加而逐渐地逼近单一区块随机化的曲线。这表示后者的测试案例的效能显著地受到了 TLB 错失的影响。假如 TLB 错失次数能够降低，效能便会显著地提升（在我们稍候将会看到的测试中，高达 38%）。 20. 是的，这有点不一致，因为在其它的测试中，我们把结构中没用到的部分也算在元素大小里，而且我们能够定义 NPAD 以让每个元素填满一个分页。在这种情况中，工作集的大小会差得很多。不过这并不是这个测试的重点，而且无论如何预取都没什么效率，因此没有什么差别。 ↩ "},"cpu-caches/cpu-cache-implementation-details/write-behavior.html":{"url":"cpu-caches/cpu-cache-implementation-details/write-behavior.html","title":"3.3.3. 写入行为","keywords":"","body":"3.3.3. 写入行为 在我们开始研究在多执行环境（execution context）（执行绪或行程）使用相同memory的cache行为之前，我们必须先探究一个cache实作的细节。cache是假定为一致的（coherent），而且对使用者层级的程序而言，这个一致性是假定为完全透明的。系统核心程序是不同的情况；它偶尔会要求cache冲出（flush）。 这具体意味著，假如一个cache行被修改了，在这个时间点之后，对系统而言的结果与根本没有cache、并且是主memory位置本身被修改的情况是相同的。这能以两种方式或策略来实行： 直写式（write-through）cache实作； 回写式（write-back）cache实作。 直写式cache是最简单的cache一致性的实行方式。若是cache行被写入的话，处理器也会立即将cache行写到主memory中。这保证了主memory与cache永远保持一致。能够在任何cache行被取代的时候直接丢弃cache的内容。这个cache策略很简单，但并不是非常快。举例来说，一支不断地修改一个区域变数的程序会在 FSB 产生大量的流量，尽管资料很可能不会在别处用到、而且可能只会短暂存在。 回写式策略更为复杂。这时处理器不会立即将被修改的cache行写回到主memory里。取而代之地，cache行只会被标记为脏的。当cache行在未来的某个时间点从cache被丢弃时，脏位（dirty bit）将会在这时通知处理器去把资料写回去，而不是直接丢弃内容。 写回式cache有机会做得好非常多，这即是大多有著像样处理器的系统中，memory都会以这种方式cache的原因了。处理器甚至能在cache行必须被清除之前，利用 FSB 的閒置容量来储存cache行的内容。这使得脏位被清除，并且在需要cache中的空间的时候，处理器能够直接丢弃这个cache行。 但回写式实作也有个重大的问题。当有多于一个处理器（或是处理器核或 HT），并且存取到同样的memory时，它仍旧必须保证每个处理器看到的一直都是相同的memory内容。假如一个cache行在一个处理器上是脏的（也就是说，它还没被写回去），并且第二个处理器试著读取相同的memory位置，这个读取操作就不能直接送到主memory去。而是需要第一个处理器的cache行的内容。在下一节，我们将会看到这在当前是如何实作的。 在此之前，还有两种cache策略要提一下： 合并写入（write-combining）；以及 不可cache（uncacheable） 这两种策略都是用在定址空间中、并非被真正的 RAM 所支援的特殊区域。系统核心为这些地址范围设置了这些策略（在使用了memory型态范围暂存器〔Memory Type Range Register，MTRR〕的 x86 处理器上），剩下的部分自动地进行。MTRR 也能用于在直写式与回写式策略之间选择。 合并写入是一种受限的cache最佳化，更常用于显示卡一类装置上的 RAM。由于对装置来说，传输成本比区域 RAM 存取的成本还高得多，因此避免过多的传输是更为重要的。仅因为cache行中的一个字组被修改，就传输一整个cache行，在下一个操作修改了下一个字组的情况下是很浪费的。能够轻易地想像，一种常见的情况是，表示萤幕上水平相邻的像素点的memory，在多数情况下也是相邻的。如同名字所暗示的，合并写入会在cache行被写出去之前合并多个写入存取。在理想情况下，cache行会被一个字组接著一个字组地修改，并且只有在写入最后一个字组之后，cache行才会被写到装置中。这能够显著地加速装置对 RAM 的存取。 最后，不可cache的memory。这通常表示memory位置根本不被 RAM 所支援。它可能是一个被写死的特殊地址，以拥有某个在 CPU 外部实作的功能。对商用硬件来说，最常见的例子是memory对映的（memory-mapped）地址范围，转译为对附属于总线上的扩充卡以及装置（PCIe 等等）的存取。在嵌入式单板（embedded board）上，有时候会发现能够用来开关 LED 的memory地址。cache这类地址显然是个坏点子。在这种情境下的 LED 是用以除错或者状态回报的，会想要尽可能快地看到它。在 PCIe 扩充卡上的memory能够在不与 CPU 互动的情况下改变，因此这种memory不应该被cache。 "},"cpu-caches/cpu-cache-implementation-details/multi-processor-support.html":{"url":"cpu-caches/cpu-cache-implementation-details/multi-processor-support.html","title":"3.3.4. 多处理器支援","keywords":"","body":"3.3.4. 多处理器支援 在上一节，我们已经指出，当多处理器开始起作用时我们会遇到的问题。多核处理器甚至有那些并没有被共享的cache层级（至少 L1d）的问题。 提供从一个处理器到另一个处理器的cache的直接存取是完全不切实际的。首先，连线根本不够快。实际的替代方案是，将cache内容传输给另一个处理器 –– 假如需要的话。注意到这也同样适用于不在相同处理器上共享的cache。 现在的问题是，什么时候得传输这个cache行？这是个相当容易回答的问题：当一个处理器需要读取或写入一个cache行，而其在另一个处理器的cache上是脏的。但处理器要怎么样才能判断一个cache行在另一个处理器的cache上是脏的呢？仅因为一个cache行被另一个处理器载入就假定如此，（至多）也是次佳的（suboptimal）。通常，大多数的memory存取都是读取操作，产生的cache行也不是脏的。处理器对cache行的操作是很频繁的（那当然，不然我们怎么会有这篇论文？），这表示在每次写入操作之后，都去广播被改变的cache行的资讯是不切实际的。 这些年来所发展出来的就是 MESI cache一致性协议（修改〔Modified〕、独占〔Exclusive〕、共享〔Shared〕、无效〔Invalid〕）。这个协议的名称来自采用 MESI 协议时、一个cache行能够变成的四个状态： 修改 本地的处理器已经修改过cache行。这也暗指它是在任何cache中的唯一副本。 独占 cache行没有被修改过，但已知没有被载入到任何其它处理器的cache中。 共享 cache行没有被修改过，并且可能存在于另一个处理器的cache中。 无效 cache行是无效的 –– 也就是说，没有被使用。 多年来，这个协议从比较不复杂、但也比较没效率的较简易版本开始发展。有了这四个状态，便可能有效率地实作回写式cache，而又支援同时在不同的处理器上使用唯读的资料。 图 3.18：MESI 协议的状态转换 借由处理器监听 –– 或者窥探 –– 其它处理器的运作，不用太多精力便得以完成状态改变。处理器执行的某些操作会被发布在外部针脚上，因而让处理器的cache处理能被外界看到。处理中的cache行地址能在地址总线上看到。在接下来对状态与其转换（显示在图 3.18）的描述中，我们会指出总线是何时被牵扯进来的。 起初所有cache行都是空的，因此也是无效的。若是资料是为了写入而载入cache，则改为修改。若是资料是为了读取而载入，新的状态则取决于另一个处理器是否也已载入这个cache行。如果是的话，新的状态为共享，否则为独占。 若是一个修改的cache行从本地处理器被读取或写入，这个指令能够使用当前的cache内容，并且状态不变。若是第二个处理器想要读取这个cache行，第一个处理器就必须将它的cache内容寄送给第二个处理器，然后它就能将状态改为共享。寄送给第二个处理器的资料也会被memory控制器接收并处理，其会将内容储存在memory中。假如没有这么做，cache行就不能被标为共享。若是第二个处理器想要写入cache行，第一个处理器便会寄送cache行的内容，并将自己的cache行标为无效。这即是恶名昭彰的「所有权请求（Request For Ownership，RFO）」操作。在最后一个层级的cache中执行这个操作，就像是 I→M 的转换一样，相当昂贵。对直写式cache而言，我们也得加上它将新的cache行内容写入到更高阶cache或主memory所花费的时间，进而提高了成本。 若是一个cache行处于共享状态，并且本地处理器要读取它，那么就不必改变状态，读取请求能够由这个cache来达成。若是cache行要在本地写入，也能够使用这个cache行，但状态会被改成修改。这也需要令其它处理器的所有可能的cache行副本被标为无效。因此，写入操作必须要透过一个 RFO 讯息发布给其它处理器。若是cache行被第二个处理器请求读取，那么什么也不必做。主memory包含了当前的资料，本地的状态也已经是共享了。在第二个处理器想要写入到cache行的情况下（RFO），就直接将cache行标为无效。不需要总线操作。 独占状态与共享状态大致相同，只有一个重大的不同：本地的写入操作不必发布到总线上。因为已经知道本地cache是唯一一个持有这个独有的cache行的了。这会是一个巨大的优势，所以处理器会试著令尽可能多的cache行维持在独占状态，而非共享。后者是在这种时刻，无法取得这个资讯的退而求其次。独占状态也能够在完全不引发功能问题的情况下被省去。唯一会变糟的只有效能，因为 E→M 转换比 S→M 转换要快得多了。 从这些状态转换的描述中，应该很清楚多处理器操作特有的成本在哪了。是的，填入cache仍旧昂贵，但现在我们也必须留意 RFO 讯息。每当必须发送这种讯息时，工作就会变慢。 有两种必须要 RFO 讯息的情况： 一条执行绪从一个处理器迁移到另一个，并且所有cache行都必须一起移动到新的处理器上。 一个cache行真的被两个不同的处理器所需要。21 在多执行绪或多行程的程序中，总是有一些同步的需求；这种同步是使用memory实作的。所以有些有根据的 RFO 讯息。它们仍旧得尽可能地降低频率。不过，还有其他 RFO 讯息的来源。我们将会在第六节解释这些情况。cache一致性协议的讯息必须被分发给系统中的处理器。MESI 转换直到确定系统中的所有处理器都有机会回覆讯息之前都不会发生。这表示一个回覆能花上的最长可能时间决定了一致性协议的速度。22可能会有总线上的冲突、NUMA 系统的等待时间会很长、而且突发的流量当然也会让事情变慢。这全都是专注在避免不必要流量的好理由。 还有一个与拥有多于一个处理器有关的问题。这个影响是与机器高度相关的，但原理上这个问题总是存在：FSB 是一个共享的资源。在大多数机器上，所有处理器会透过单一一条总线连结到memory控制器（见图 2.1）。假如单一个处理器能够占满总线（通常是这样），那么共享相同总线的二或四个处理器甚至会更加地限制每个处理器的可用频宽。 即使每个处理器都如图 2.2 一样，有它自己的、连结到memory控制器的总线，但仍旧有连结到memory模组的总线。通常这是唯一一条总线，而 –– 即使在图 2.2 的扩充模型中 –– 同时存取相同的memory模组将会限制频宽。 每个处理器都能拥有本地memory的 AMD 模型亦是如此。所有处理器确实能快速地并行存取它们的本地memory，尤其在使用整合式memory控制器的情况。但多执行绪与多行程程序 –– 至少偶尔 –– 必须存取相同的memory区域以进行同步。 并行是受可用于必要的同步实作的有限频宽所严重地限制的。程序需要被小心地设计，以将不同处理器核对相同memory位置的存取降到最小。接下来的量测将会显示这点、以及其它与多执行绪程序有关的cache影响。 多执行绪存取 为了确保大家理解在不同处理器上同时使用相同cache行所引入的问题的严重性，我们将会在这里多看到一些针对我们先前用过的相同程序的效能图表。不过，这次会同时执行多于一条执行绪。所要量测的是最快的执行绪的执行时间。这意味著完成所有执行绪的完整执行时间还会更长。使用的机器有四个处理器；测试使用至多四条执行绪。所有处理器共享连结到memory控制器的总线，而且仅有一条连结到memory模组的总线。 图 3.19：循序读取，多条执行绪 图 3.19 显示了循序唯读存取 128 位元组项目的效能（在 64 位元机器上，NPAD=15）。对于单执行绪的曲线，我们能预期是条与图 3.11 相似的曲线。量测使用了一台不同的机器，所以实际的数字会有所不同。 这张图中重要的部分当然是执行多条执行绪时的行为。注意到在走访链结串列时，没有memory会被修改，亦无让执行绪保持同步的企图。尽管不必有 RFO 讯息、而且所有的cache行都能被共享，但我们看到当使用两条执行绪时，效能减低了高达 18%，而使用四条执行绪时则高达 34%。由于没有必须在处理器之间传输的cache行，因此变慢仅仅是由两个瓶颈中的一或二者所引起的：从处理器到memory控制的共享总线、以及从memory控制器到memory模组的总线。一旦工作集大小大于这台机器的 L3 cache，图上三种数量的执行绪都会预取新的串列元素。即便只有两条执行绪，可用频宽也不足以线性延展（scale）（即，没有执行多条执行绪带来的损失）。 图 3.20：循序 Increase，多条执行绪 当我们修改memory时，情况变得更可怕了。图 3.20 显示了循序 Increase 测试的结果。这个图表的 Y 轴使用了对数尺度。所以，别被看似很小的差异给骗了。我们在执行两条执行绪的时候仍有大约 18% 的损失，而执行四条执行绪则是惊人的 93% 损失。这表示，在使用四条执行绪时，预取流量加上回写流量就把总线占得非常满了。 我们使用对数尺度来显示 L1d 范围的结果。能够看到的是，一旦执行了多于一条执行绪，L1d 基本上就没什么效果了。只有在 L1d 不足以容纳工作集的时候，单执行绪的存取时间才会超过 20 个周期。当执行了多条执行绪时，存取时间却立即就达到了 –– 即便使用的是最小的工作集大小。 这里没有显示出问题的一个面向。这个特定的测试程序是难以量测的。即使测试修改了memory、而我们因此预期必定会有 RFO 讯息，但当使用了多于一条执行绪时，我们并没有在 L2 范围内看到更高的成本。程序必须要使用大量的memory，并且所有执行绪必须要平行地存取相同的memory。没有大量的同步 –– 其会占据大多的执行时间 –– 这是很难实现的。 图 3.21：随机 Addnextlast，多条执行绪 最后在图 3.21，我们有 Addnextlast 测试以随机的方式存取memory的数据。提供这张图主要是为了显示出这些高得吓人的数字。现在在极端的状况下，处理一个单一的串列元素要花上大约 1,500 个周期。使用更多执行绪的情况还要更加严重。我们能使用一张表格来总结多条执行绪的效率。 #执行绪 循序读取 循序递增 随机增加 2 1.69 1.69 1.54 4 2.98 2.07 1.65 表 3.3：多条执行绪的效率 表格显示了在图 3.19、3.20、与 3.21 中，多执行绪以最大工作集大小执行的效率。数据表示在使用二或四条执行绪处理最大的工作集大小时，测试程序可能达到的最佳加速。以两条执行绪而言，加速的理论极限为 2，对于四条执行绪而言为 4。两条执行绪的数据并没有那么糟。但对于四条执行绪，最后一个测试的数据显示了，几乎不值得扩展到超过两条执行绪。额外的获益是非常小的。如果我们以略为不同的方式表示图 3.21 的资料，我们便能更轻易地看出这点。 图 3.22：经由平行化的加速 图 3.22 的曲线显示了加速因子 –– 也就是相比于以单一执行绪执行的程序的相对效能。我们得忽略最小大小的情况，因为量测结果不够精确。以 L2 与 L3 cache的范围而言，我们能够看到我们确实达到了几乎是线性的加速。我们分别达到了差不多 2 与 4 倍速。但一旦 L3 cache不足以容纳工作集，数字就往下掉了。两条与四条执行绪的加速因子都掉到一样的值（见表 3.3 的第四行）。这是难以找到主机板有著超过四个全都使用同个memory控制器的 CPU 插槽的其中一个理由。有著更多处理器的机器必须要以不同的方式来做（见第五节）。 这些数字并不普遍。在某些情况下，甚至连能塞进最后一阶cache的工作集都无法做到线性加速。事实上，这才是常态，因为执行绪通常并不若这个测试程序的例子一般解耦（decoupled）。另一方面，是可能运作在大工作集上，而仍旧拥有多于两条执行绪的优势的。不过，做到这点需要一些思考。我们会在第六节讨论一些方法。 特例：Hyper-Threading Hyper-Threading (简称 HT，有时被称为对称多执行绪〔Symmetric Multi-Threading，SMT〕）由 CPU 实作，并且是个特例，因为个别执行绪无法真的同时执行。它们全都共享著暂存器集以外、几乎所有的处理资源。个别的处理器核与 CPU 仍然平行地运作，但实作在每颗处理器核上的执行绪会受到这个限制。理论上，每颗处理器核可以有许多执行绪，但是 –– 到目前为止 –– Intel CPU 的每颗处理器核至多仅有两条执行绪。CPU 有时域多工（time-multiplex）执行绪的职责。不过单是如此并没太大意义。实际的优点是，当同时执行的 HT 被延迟时，CPU 可以调度另一条 HT ，并善用像是算数逻辑一类的可用资源。在大多情况下，这是由memory存取造成的延迟。 假如两条执行绪执行在一颗 HT 核上，那么只有在两条执行绪合并的（combined）执行时间小于单执行绪程序的执行时间时，程序才会比单执行绪程序还有效率。借由重叠经常重复发生的不同memory存取的等待时间，这是可能的。一个简单的计算显示了为了达到某程度的加速，cache命中率的最小需求。 一支程序的执行时间能够以一个仅有一层cache的简易模型来估算，如下（见 [16]）： Texe=N[(1−Fmem)Tproc+Fmem(GhitTcache+(1−Ghit)Tmiss)] T_{\\text{exe}} = N [ (1 - F_{\\text{mem}}) T_{\\text{proc}} + F_{\\text{mem}} (G_{\\text{hit}} T_{\\text{cache}} + (1 - G_{\\text{hit}}) T_{\\text{miss}}) ] T​exe​​=N[(1−F​mem​​)T​proc​​+F​mem​​(G​hit​​T​cache​​+(1−G​hit​​)T​miss​​)] 变数的意义如下： N=指令数Fmem=N 次中存取memory的比率Ghit=载入次数中命中cache的比率Tproc=每个指令的周期数Tcache=cache命中的周期数Tmiss=cache错失的周期数Texe=程序执行时间 \\begin{aligned} N &= \\text{指令数} \\\\ F_{\\text{mem}} &= N \\text{ 次中存取memory的比率} \\\\ G_{\\text{hit}} &= \\text{载入次数中命中cache的比率} \\\\ T_{\\text{proc}} &= \\text{每个指令的周期数} \\\\ T_{\\text{cache}} &= \\text{cache命中的周期数} \\\\ T_{\\text{miss}} &= \\text{cache错失的周期数} \\\\ T_{\\text{exe}} &= \\text{程序执行时间} \\end{aligned} ​N​F​mem​​​G​hit​​​T​proc​​​T​cache​​​T​miss​​​T​exe​​​​​=指令数​=N 次中存取memory的比率​=载入次数中命中cache的比率​=每个指令的周期数​=cache命中的周期数​=cache错失的周期数​=程序执行时间​​ 为了要让使用两条执行绪有任何意义，两条执行绪任一的执行时间都必须至多为单执行绪程序码的一半。在任一边的唯一变数为cache命中的数量。若是我们求解方程序，以得到令执行绪的执行不减慢超过 50% 以上所需的最小cache命中率，我们会得到图 3.23 的结果。 图 3.23：加速的最小cache命中率 输入 –– 刻在 X 轴上 –– 为单执行绪程序码的cache命中率 Ghit G_{\\text{hit}} G​hit​​。Y 轴显示了多执行绪程序码的cache命中率。这个值永远不能高于单执行绪的命中率，不然单执行绪程序码也会使用这个改良的程序码。以单执行绪的命中率 –– 在这个特定的情况下 –– 低于 55% 而言，在所有情况下程序都能够因为使用执行绪而获益。由于cache错失，CPU 或多或少有足够的空閒来执行第二条 HT。 绿色的区域是目标。假如对执行绪而言的减慢小于 50%，且每条执行绪的工作量都减半，那么合并的执行时间就可能会小于单执行绪的执行时间。以用作模型的处理器（使用一个有著 HT 的 P4 的数据）而言，一支命中率为 60% 的单执行绪程序，对双执行绪程序来说需要至少 10% 的命中率。这通常是做得到的。但若是单执行绪程序的命中率为 95%，那么多执行绪程序就需要至少 80% 的命中率。这更难了。尤其 –– 这是使用 HT 的问题 –– 因为现在每条 HT 可用的有效cache大小（这里是 L1d，在实际上 L2 也是如此）被砍半了。 HT 都使用相同的cache来载入它们的资料。若是两条执行绪的工作集没有重叠，那么原始的 95% 命中率也会打对折，因而远低于所需要的 80%。 HT 因而只有在有限范围的情境中有用。单执行绪程序的cache命中率必须足够低，以在给定上面的等式、以及减小的cache大小时，新的命中率仍然满足要求。这时，也只有这时，才有任何使用 HT 的意义。实际上结果是否比较快，取决于处理器是否足以能将一条执行绪的等待时间重叠在另一条执行绪的执行时间上。平行化程序码的间接成本必须被加到新的总执行时间上，这个额外成本经常无法忽视。 在 6.3.4 节，我们将会看到一种执行绪紧密合作、而通过共有cache的紧密耦合竟然是个优点的技术。这个技术能够用于多种情境，只要程序开发者乐于将时间与精力投入到扩展他们的程序码的话。 应该清楚的是，假如两条 HT 执行完全不同的程序码（也就是说，两条硬件执行绪被操作系统如同单独的处理器一般对待，以执行个别的行程），cache大小固然会减半，这表示cache错失的显著攀升。除非cache足够大，不然这种操作系统排程的实行是有问题的。除非机器由行程组成的负载确实 –– 经由它们的设计 –– 能够获益于 HT ，否则最好在电脑的 BIOS 把 HT 关掉。23 21. 以相同处理器上的两颗处理器核而言，在较小的层级也是如此。成本只小了一点点。RFO 讯息可能会被多次送出。 ↩ 22. 这即是为何我们现今会看到 –– 举例来说 –– 有三个插槽的 AMD Opteron 系统的原因。假定处理器只拥有三条超连结（hyperlink），而且一条是北桥连接所需，每个处理器都正好相隔一跳（hop）。 ↩ 23. 另一个令 HT 维持开启的理由是除错。SMT 令人惊讶地善于在平行程序中找出好几组问题。 ↩ "},"cpu-caches/cpu-cache-implementation-details/other-details.html":{"url":"cpu-caches/cpu-cache-implementation-details/other-details.html","title":"3.3.5. 其它细节","keywords":"","body":"3.3.5. 其它细节 到目前为止，我们已经讨论过由三个部分 –– 标籤、集合索引、以及cache行偏移量 –– 组成的地址。但实际使用的地址是什么呢？所有有关的处理器现今都是将虚拟定址空间提供给行程，这代表有两种不同的地址：虚拟的以及实体的。 虚拟memory的问题是，它们不是唯一的。虚拟memory能够 –– 随著时间 –– 指涉到不同的实体memory地址。在不同行程中的相同地址也可能会指涉到不同的实体地址。所以使用实体memory地址永远比较好，对吧？ 这里的问题是，在执行期间使用的虚拟memory必须在memory管理单元（Memory Management Unit，MMU）的帮助下转译成实体地址。这是个不单纯的操作。在执行一个指令的管线中，实体地址可能只有在之后的阶段才能取得。这表示cache逻辑必须非常快速地判定这个memory位置是否被cache了。若是能够使用虚拟地址，cache查询就能够早点在管线中进行，并且在cache命中的情况下，就能够取得memory内容了。结果是，管线能够隐藏更多memory存取的成本。 处理器设计者目前是使用虚拟地址来标记第一层级的cache。这些cache非常地小，而且清除也不会太费力。若是一个行程的分页表树（page table tree）改变了，至少必须局部地清理cache。假如处理器拥有能够指定被改变的虚拟地址范围的指令，就有可能避免一次完整的冲出。考虑到 L1i 与 L1d cache的等待时间很短（~3 周期），使用虚拟地址几乎是强迫性的。 对于大一点的cache，包含 L2、L3、... cache，是需要实体地址标记的。这些cache有比较长的等待时间，而虚拟→实体地址转译能够在时间内完成。因为这些cache比较大（即，当它们被冲出时，会损失大量的资讯），并且因为主memory存取的等待时间，重新填入它们会花上很久的时间，因此冲出它们的代价经常不小。 一般来说，应该没必要知道在那些cache中的地址管理的细节。它们无法改变，而且所有会影响效能的因子通常是应该避免、或者是与高成本相关联的东西。塞满cache容量是很糟的，而且若是多数被使用的cache行都落在同一组集合中，所有cache都会很快地碰到问题。后者能够以虚拟定址的cache来避免，但对于使用者层级的行程来说，要避免使用实体地址定址的cache是不可能的。或许唯一应该记住的细节是，可能的话，别在同个行程里将同个实体memory位置映射到两个以上的虚拟地址。 另一个对程序开发者来说满无趣的cache细节是cache的替换策略。大多cache会先逐出近期最少使用的（Least Recently Used，LRU）元素。以较大的关联度（由于增加更多的处理器核，关联度可能会在接下来几年内进一步地增长）维持 LRU 清单会变得越来越昂贵，我们可能会看到被采用的不同策略。 至于cache的替换，一介程序开发者能做的不多。若是cache是使用实体地址的标籤，就没有任何办法能找出虚拟地址与cache集之间的关联。所有逻辑分页的cache行可能都映射到相同的cache集，留著大量的cache不用。如果是这样的话，不让这太常发生就是操作系统的工作了。 随著虚拟化（virtualization）的出现，事情变得更加复杂。现在甚至不是操作系统拥有对实体memory指派的控制。而是虚拟机器监视器（Virtual Machine Monitor，VMM，又称 Hypervisor）有指派实体memory的职责。 一介程序开发者能做的最多就是 a) 完全使用逻辑memory分页 b) 使用尽可能大的分页大小，以尽可能地多样化实体地址。较大的分页大小也有其它好处，但这是另一个主题（见第四节）。 "},"cpu-caches/instruction-cache.html":{"url":"cpu-caches/instruction-cache.html","title":"3.4. 指令cache","keywords":"","body":"3.4. 指令cache 并非只有处理器用到的资料有被cache；处理器执行的指令也会被cache。然而，这个cache比起资料cache，问题少了许多。有几个理由： 执行的程序码的量取决于所需的程序码大小。程序码的大小一般视问题的复杂度而定。而问题的复杂度是固定的。 程序的资料管理是由程序开发者所设计的，而程序的指令通常是由编译器产生的。编译器撰写者知道产生良好程序的规则。 程序流程比起资料存取模式更加能够预测。现今的 CPU 非常擅于发现模式。这有助于预取。 程序码总是有相当好的空间与时间局部性。 有一些程序开发者应该遵循的规则，但这些主要都是如何使用工具的规则。我们将会在第六节讨论它们。这里我们仅讨论指令cache的技术细节。 自从 CPU 核时脉急遽增加、以及cache（即使是第一层cache）与核之间的速度差距成长以来，CPU 便以管线来设计了。这表示一条指令的执行会分阶段进行。一条指令会先被解码、接著准备参数、最后再执行它。这种管线能够非常长（以 Intel 的 Netburst 架构而言，> 20 个阶段）。一条很长的管线意味著，若是管线延误了（即，通过它的指令流被中断了），它会花上一段时间才能恢复速度。管线拖延发生在 –– 举例来说 –– 下一条指令的位置无法被正确地预测、或者载入下一条指令花了太长时间（如，当它必须从memory读取的时候）的时候。 因此 CPU 设计者花费了大量的时间与晶片面积在分支预测上，以尽可能地降低管线延误发生的频率。 在 CISC 处理器上，解码阶段也会花上一些时间。x86 与 x86-64 处理器尤其受此影响。在最近几年，这些处理器因而不在 L1i 上cache指令的原始位元组序列，而是cache被解码的指令。在这种情况下的 L1i 被称作「追踪cache（trace cache）」。追踪cache令处理器能够在cache命中的情况下略过管线的前面几步，这在管线被延误时格外有效。 如同先前说过的，L2 的cache是包含程序码与资料的统一式cache。在这里，程序码显然是以位元组序列的形式被cache，而不是被解码过的。 为了达到最好的效能，只有一些与指令cache相关的规则： 产生尽可能小的程序码。有些例外，像是为了使用管线的软件管线化（software pipelining）需要建立额外的程序码的时候、以及使用小程序码的间接成本太高之处。 协助处理器做出好的预取决策。这能够通过程序布局或是显式的预取来做到。 这些规则通常由编译器的程序码产生（code generation）来强制执行。有一些程序开发者能做的事情，我们会在第六节讨论它们。 "},"cpu-caches/instruction-cache/self-modifying-code.html":{"url":"cpu-caches/instruction-cache/self-modifying-code.html","title":"3.4.1. 自我修改的程序码","keywords":"","body":"3.4.1. 自我修改的程序码 在电脑时代的早期，memory是很珍贵的。人们不遗馀力地减少程序的大小，以为程序资料腾出更多的空间。一个经常使用的技巧是，随著时间改变程序自身。偶尔仍旧会找到这种自我修改的程序码（Self Modifying Code，SMC），如今多半是为了效能因素、或者用在安全漏洞上。 一般来说应该避免 SMC。虽然它通常都被正确地执行，但有著并非如此的边界案例（boundary case），而且没有正确完成的话，它会产生效能问题。显然地，被改变的程序码无法维持在保存被解码指令的追踪cache中。但即使程序码完全（或者有时候）不会被执行，因而不会使用到追踪cache，处理器也可能会有问题。若是接下来的指令在它已经进入管线的期间被改变，处理器就得丢掉大量的成果，然后从头开始。甚至有处理器的大多状态都必须被丢弃的情况。 最后，由于处理器假定 –– 为了简化起见，而且因为这在 99.9999999% 的情况下都成立 –– 程序码分页是不可修改的（immutable），所以 L1i 的实作不会采用 MESI 协议，而是一种简化的 SI 协议。这表示，若是侦测到修改，就必须做出许多的悲观假设。 强烈地建议尽可能避免 SMC。memory不再是如此稀有的资源。最好是撰写各自的函式，而非根据特定的需求修改一个函式。或许有天 SMC 的支援能够是可选的，而我们就能够以这种方式侦测出尝试修改程序码的漏洞程序码（exploit code）。若是真的必须使用 SMC，写入操作应该要绕过cache，以免因为 L1i 所需的 L1d 的资料造成问题。关于这些指令的更多讯息，见 6.1 节。 在 Linux 上，识别出包含 SMC 的程序通常非常容易。使用正规工具链（toolchain）建构的话，所有程序的程序码都是防写的（write-protected）。程序开发者必须在连结期（link time）施展强大的魔法，以产生程序分页能够被写入的可执行档。当这种情况发生时，现代的 Intel x86 与 x86-64 处理器具有专门的、计算自我修改的程序码使用次数的效能计数器。有了这些计数器的帮助，非常轻易就能够识别有著 SMC 的程序，即使程序由于宽松的许可而成功执行。 "},"cpu-caches/cache-miss-factors.html":{"url":"cpu-caches/cache-miss-factors.html","title":"3.5. cache错失的因素","keywords":"","body":"3.5. cache错失的因素 我们已经看过，在memory存取没有命中cache时的成本一飞冲天。有时候这是无可避免的，而了解实际的成本、以及能做些什么来减轻问题是很重要的。 "},"cpu-caches/cache-miss-factors/cache-and-memory-bandwidth.html":{"url":"cpu-caches/cache-miss-factors/cache-and-memory-bandwidth.html","title":"3.5.1. cache与memory频宽","keywords":"","body":"3.5.1. cache与memory频宽 为了更好地理解处理器的能力，我们要量测在最理想情况下的可用频宽。这个量测格外有趣，因为不同处理器版本的差异很大。这即是本节充满著数个不同机器数据的原因。量测效能的程序使用 x86 与 x86-64 处理器的 SSE 指令以同时载入或储存 16 位元组。就如同我们的其它测试一样，工作集从 1kB 增加到 512MB，并量测每个周期能够载入或储存多少位元组。 图 3.24：Pentium 4 的频宽 图 3.24 显示在一台 64 位元 Intel Netburst 处理器上的效能。对于能够塞进 L1d 的工作集大小，处理器每个周期能够读取完整的 16 位元组 –– 即，每个周期执行一次载入指令（movaps 指令一次搬移 16 位元组）。测试不会对读取的资料做任何事，我们测试的仅有读取指令本身。一旦 L1d 不再足够，效能就立刻大幅地降低到每周期少于 6 位元组。在 218 的一步是因为 DTLB cache的枯竭，表示每个新分页的额外工作。由于读取是循序的，预取能够完美地预测存取，并且对于所有工作集大小，FSB 能以大约每周期 5.3 位元组传输memory内容。不过，预取的资料不会传播到 L1d。这些当然是真实程序中永远无法达到的数字。将它们想成实际上的极限吧。 比起读取的效能，令人更为吃惊的是写入与复制的效能。写入的效能 –– 即便对于很小的工作集大小 –– 始终不会上升到每周期 4 位元组以上。这暗示著，在这些 Netburst 处理器上，Intel 为 L1d 选择使用直写模式，其中的效能显然受限于 L2 的速度。这也代表复制测试 –– 其从一个memory区域复制到第二个、不重叠的memory区域 –– 的效能并没有显著地变差。所需的读取操作要快得多，并且能够与写入操作部分重叠。写入与复制量测中，最值得注意的细节是，当 L2 cache不再足够时的低效能。效能跌落到每周期 0.5 位元组！这表示写入操作比读取操作慢十倍。这意味著，对于这个程序的效能而言，最佳化那些操作是更加重要的。 图 3.25：有著 2 条 HT 的 P4 频宽 在图 3.25 中，我们看到在相同处理器上、但以两条执行绪执行的结果，每条执行绪各自归属于处理器的两条 HT 的其中一条上。这张图表与前一张使用相同的刻度，以阐明两者的差异。曲线有些微抖动，仅是因为量测两条并行执行绪的问题。结果如同预期。由于 HT 共享暂存器以外的所有资源，每条执行绪都只有一半的cache与可用频宽。这表示，即使每条执行绪都必须等待很久、并能够将执行时间拨给另一条执行绪，这也没有造成任何不同，因为另一条执行绪也必须等待memory。这忠实地显示使用 HT 的最差情况。 图 3.26：Core 2 的频宽 图 3.27：有著 2 条 HT 的 Core 2 频宽 对比图 3.24 与图 3.25，对于 Intel Core 2 处理器，图 3.26 与 3.27 的结果看起来差异甚大。这是一个有著共享 L2 的双核处理器，其 L2 是 P4 机器上的 L2 的四倍大。不过，这只解释写入与复制效能延后下降的原因。 有另一个、更大的不同。整个工作集范围内的读取效能停留在大约是最佳的每周期 16 位元组。读取效能在 220 位元组之后的下降同样是因为工作集对 DTLB 来说太大。达到这么高的数字代表处理器不仅能够预取资料、还及时传输资料。这也代表资料被预取至 L1d 中。 写入与复制的效能也大为不同。处理器没有直写策略；写入的资料被储存在 L1d 中，而且仅会在必要时逐出。这使得写入速度接近于最佳的每周期 16 位元组。一旦 L1d 不再足够，效能便显著地降低。如同使用 Netburst 处理器的情况，写入的效能显著地降低。由于读取效能很高，这里的差距甚至更大。事实上，当 L2 也不再足够时，速度差异甚至提升到 20 倍！这不代表 Core 2 处理器表现得很糟。相反的，它们的效能一直都比 Netburst 处理器核还好。 在图 3.27 中，测试执行两条执行绪，各自在 Core 2 处理器两颗处理器核的其中一颗上。两条执行绪都存取相同的memory，不过不需要完美地同步。读取效能的结果跟单执行绪的情况没什么不同。看得到稍微多一点的抖动，这在任何多执行绪的测试案例里都在预期之中。 有趣的一点是，对于能塞进 L1d 的工作集大小的写入与复制效能。如同图中能看到的，效能就像是资料必须从主memory读取一样。两条执行绪都争夺著相同的memory位置，因而必须送出cache行的 RFO 讯息。麻烦之处在于，即使两颗处理器核共享cache，这些请求也不是以 L2 cache的速度处理。一旦 L1d cache不再足够，被修改的项目会从每颗处理器核的 L1d 冲出到共享的 L2。这时，由于 L1d 的错失被 L2 cache所弥补、而且只有在资料还没被冲出时才需要 RFO 讯息，效能便显著地增加。这即是我们看到，对于这些工作集大小，速度降低 50% 的原因。这个渐近行为如同预期一般：由于两颗处理器核共享相同的 FSB，每颗处理器核会得到一半的 FSB 频宽，这表示对于大工作集而言，每条执行绪的效能大约是单执行绪时的一半。 图 3.28：AMD 10h 家族 Opteron 的频宽 即使同个供应商的处理器版本之间都有显著的差异，所以当然也值得看看其它供应商的处理器效能。图 3.28 显示一个 AMD 10h 家族 Opteron 处理器的效能。这个处理器拥有 64kB L1d、512kB L2、以及 2MB 的 L3。L3 cache被处理器的所有核所共享。效能测试的结果能在图 3.28 看到。 注意到的第一个关于数字的细节是，假如 L1d cache足够的话，处理器每个周期能够处理两条指令。读取效能超过每周期 32 位元组，甚至连写入效能都很高 –– 每周期 18.7 位元组。不过，读取的曲线立刻就掉下去，而且非常低 –– 每周期 2.3 位元组。对于这个测试，处理器没有预取任何资料，至少不怎么有效率。 另一方面，写入曲线的表现则取决于不同cache的大小。在 L1d 全满时达到效能高峰，于 L2 降到 6 位元组，于 L3 降到 2.8 位元组，最后在连 L3 也无法容纳所有资料时，降到每周期 .5 位元组。在 L1d cache时的效能超越（较旧的）Core 2 处理器，L2 存取一样快（因为 Core 2 有个比较大的cache），而 L3 与主memory存取则比较慢。 复制的效能无法比读取或写入的效能还要来得好。这即是我们看到，这条曲线起初被压在读取效能下面、而后又被压在写入效能下面的原因。 图 3.29：有著 2 条 HT 的 AMD 10h 家族的频宽 Opteron 处理器的多执行绪效能显示于图 3.29。读取效能基本上不受影响。每条执行绪的 L1d 与 L2 如先前一般运作，而在这个例子下的 L3 cache也没预取得很好。两条执行绪没有因其目的而过分地压榨 L3。在这个测试中的大问题是写入的效能。所有执行绪共享的资料都得通过 L3 cache。这种共享看起来非常没有效率，因为即使 L3 cache的大小足以容纳整个工作集，成本也远大于一次 L3 存取。将这张图与图 3.27 相比，我们看到在适当的工作集大小范围中，Core 2 处理器的两条执行绪是以共享的 L2 cache的速度来运作的。对于 Opteron 处理器，这种效能水平只有在一个非常小范围的工作集大小内才能达到，而即使在这里，它也只能接近 L3 的速度，比 Core 2 的 L2 还慢。 "},"cpu-caches/cache-miss-factors/critical-word-load.html":{"url":"cpu-caches/cache-miss-factors/critical-word-load.html","title":"3.5.2. 关键字组的载入","keywords":"","body":"3.5.2. 关键字组的载入 memory以比cache行大小还小的区块从主memory传输到cache中。现今是一次传输 64 位元，而cache行的大小为 64 或 128 位元组。这表示每个cache行需要 8 或 16 次传输。 DRAM 晶片能够以突发（burst）模式传输那些 64 位元组的区块。这能够在没有来自memory控制器的额外命令、以及可能伴随的延迟的情况下填满cache行。若是处理器预取cache行，这可能是最好的操作方式。 若是一支程序的资料或cache存取没有命中（这表示，这是个强制性cache错失〔compulsory cache miss〕–– 因为资料是第一次使用、或者是容量性cache错失〔capacity cache miss〕–– 因为受限的cache大小需要逐出cache行），情况便不同。程序继续执行所需的cache行里头的字组也许不是cache行中的第一个字组。即使在突发模式下、并以双倍资料速率来传输，个别的 64 位元区块也会在明显不同的时间点抵达。每个区块会在前一个区块抵达之后 4 个 CPU 周期以上抵达。若是程序继续执行所需的字组是cache行的第八个，程序就必须在第一个字组抵达之后，等待额外的 30 个周期以上。 事情并不必然非得如此。memory控制器能够以不同的顺序随意请求cache行的字组。处理器能够传达程序正在等待哪个字组 –– 即关键字组，而memory控制器能够先请求这个字组。一旦这个字组抵达，程序便能够在cache行其余部分抵达、并且cache还不在一致状态的期间继续执行。这个技术被称为关键字组优先与提早重新启动（Critical Word First & Early Restart）。 现今的处理器实作这项技术，但有些不可能达成的情况。若是处理器预取资料，并且关键字组是未知的。万一处理器在预取操作的途中请求这个cache行，就必须在不能够影响顺序的情况下，一直等到关键字组抵达为止。 图 3.30：在cache行末端的关键字组 即使在适当的地方有了这些最佳化，关键字组在cache行的位置也很重要。图 3.30 显示循序与随机存取的 Follow 测试结果。显示的是以用来巡访的指标位在第一个字组来执行测试，对比指标位在最后一个字组的情况下的速度减慢的结果。元素大小为 64 位元组，与cache行的大小一致。数字受到许多杂讯干扰，但能够看到，一旦 L2 不再足以持有工作集大小，关键字组在末端时的效能立刻就慢约 0.7%。循序存取似乎受到多一点影响。这与前面提及的、预取下个cache行时的问题一致。 "},"cpu-caches/cache-miss-factors/cache-placement.html":{"url":"cpu-caches/cache-miss-factors/cache-placement.html","title":"3.5.3. cache的配置","keywords":"","body":"3.5.3. cache的配置 cache在与 HT 及处理器核的关系中的位置并不在程序开发者的控制之下。但程序开发者能够决定执行绪要在何处执行，于是cache如何与使用的 CPU 共处就变得很重要。 这里我们不会深入在何时选择哪颗处理器核来执行执行绪的细节。我们只会描述在设置执行绪的亲和性（affinity）时，程序开发者必须要考虑的架构细节。 HT，根据定义，共享暂存器集以外的所有东西。这包含 L1 cache。这里没什么好说的。有趣之处从一个处理器的个别处理器核开始。每颗处理器核至少拥有它自己的 L1 cache。除此之外，现今共有的细节并不多： 早期的多核处理器完全不共享cache。 之后的 Intel 模型的双核处理器拥有共享的 L2 cache。对于四核处理器，我们必须为由两颗处理器核组成的每一对处理个别的 L2 cache。没有更高层级的cache。 AMD 的 10h 处理器家族拥有独立的 L2 cache与一个统一式 L3 cache。 在处理器供应商的宣传品中已经写许多关于它们各自的模型的优点。若是由处理器核处理的工作集并不重叠，拥有不共享的cache就有一些优势。这对于单执行绪程序而言非常有用。由于这仍经常是当下的真实情况，因此这种做法并不怎么差。但总是会有一些重叠的。cache都包含通用执行期函式库（runtime library）中最活跃使用的部分，代表有一些cache空间会被浪费。 与 Intel 的双核处理器一样完全共享 L1 以外的所有cache有个大优点。若是在两颗处理器核上的执行绪工作集有大量的重叠，可用的cachememory总量也会增加，工作集也能够更大而不致降低效能。若是工作集没有重叠，Intel 的进阶智慧型cache（Advanced Smart Cache）管理应该要防止任何一颗处理器核独占整个cache。 不过，如果两颗处理器核为了它们各自的工作集使用大约一半的cache，也会有一些冲突。cache必须不断地掂量两颗处理器核的cache使用量，而作为这个重新平衡的一部分而执行的逐出操作可能会选得很差。为了看到这个问题，让我们看看另一个测试程序的结果。 测试程序拥有一个不断 –– 使用 SSE 指令 –– 读取或写入一个 2MB memory区块的行程。选择 2MB 是因为这是这个 Core 2 处理器的 L2 cache大小的一半。行程被钉在一颗处理器核上，而第二个行程则被钉在另一颗处理器核上。第二个行程读写一块可变大小的memory区域。图表显示每周期被读取或写入的位元组数。显示四条不同的曲线，每条代表一种行程读取与写入的组合。其中 read/write 曲线代表一个总是写入 2MB 工作集的背景行程，和一个读取可变工作集、用于测量的行程。 图 3.31：两个行程的频宽 这张图有趣的部分在于 220 与 223 位元组之间。若是两颗处理器核的 L2 cache完全分离，我们能够预期四个测试的效能全都会在 221 与 222 之间 –– 这表示，L2 cache耗尽的时候 –– 往下掉。如同我们能在图 3.31 中看到的，情况并非如此。以在背景行程写入的情况而言，这是最明显的。效能在工作集大小达到 1MB 之前就开始下降。两个行程没有共享memory，因此行程也不会导致 RFO 讯息被产生。这纯粹是逐出的问题。智慧型cache管理有它的问题，导致感觉到的cache大小比起每颗处理器核可用的 2MB，更接近于 1MB。只能期望，若是在处理器核之间共享的cache依旧是未来处理器的特征的话，智慧型cache管理所使用的演算法会被修正。 有一个拥有两个 L2 cache的四核处理器仅是能够引入更高层级cache之前的权宜之计。比起独立的插槽与双核处理器，这个设计并没有什么显著的效能优势。两颗处理器核透过在外部被视为 FSB 的相同的总线沟通。没有什么特别快的资料交换。 针对多核处理器的cache设计的未来将会有更多的层级。AMD 的 10h 处理器家族起个头。我们是否会继续看到被一个处理器核的一个子集所共享的更低层级的cache仍有待观察（在 2008 年处理器的世代中，L2 cache没有被共享）。额外的cache层级是必要的，因为高速与频繁使用的cache无法被多颗处理器核所共享。效能会受到影响。也会需要非常大的高关联度cache。cache大小以及关联度两者都必须随著共享cache的处理器核数量而增长。使用一个大的 L3 cache以及合理大小的 L2 cache是个适当的权衡。L3 cache较慢，但它理想上并不如 L2 cache一样常被使用。 对程序开发者而言，所有这些不同的设计都代表进行排程决策时的复杂性。为了达到最好的效能，必须知道工作负载以及机器架构的细节。幸运的是，我们拥有确定机器架构的依据。这些介面会在之后的章节中介绍。 "},"cpu-caches/cache-miss-factors/fsb-influence.html":{"url":"cpu-caches/cache-miss-factors/fsb-influence.html","title":"3.5.4. FSB 的影响","keywords":"","body":"3.5.4. FSB 的影响 图 3.32：FSB 速度的影响 FSB 在机器的效能中扮演一个重要的角色。cache内容只能以跟memory的连线所允许的一样快地被储存与写入。我们能够借由在两台仅在memory模组速度上有差异的机器上执行一支程序，来看看到底怎么样。图 3.32 显示以一台 64 位元机器、NPAD=7 而言，Addnext0 测试（将下一个元素的 pad[0] 元素加到自己的 pad[0] 元素上）的结果。两台机器都拥有 Intel Core 2 处理器，第一台使用 667MHz DDR2 模组，第二台则是 800MHz 模组（提升 20%）。 数据显示，当 FSB 真的受很大的工作集大小所压迫时，我们的确看到巨大的优势。在这项测试中，量测到的最大效能提升为 18.2%，接近理论最大值。这表示，更快的 FSB 确实能够省下大量的时间。当工作集大小能塞入cache时（这些处理器有一个 4MB L2），这并不重要。必须记在心上的是，这里我们测量的是一支程序。一个系统的工作集包含所有同时执行的行程所需的memory。如此一来，以小得多的程序就可能轻易超过 4MB 以上的memory。 现今，一些 Intel 的处理器支援加速到 1,333MHz 的 FSB，这可能代表著额外 60% 的提升。未来将会看到更高的速度。若是速度很重要、并且工作集大小更大，肯定是值得投资金钱在快速的 RAM 与很高的 FSB 速度的。不过必须小心，因为即使处理器可能会支援更高的 FSB 速度，但主机板／北桥可能不会。检查规格是至关重要的。 "},"virtual-memory.html":{"url":"virtual-memory.html","title":"4. 虚拟memory","keywords":"","body":"4. 虚拟memory 一个处理器的虚拟memory（virtual memory，VM）子系统实作了提供给每个行程的虚拟定址空间。这令每个行程都认为它是独自在系统中的。虚拟memory的优点清单会在其它地方仔细地描述，所以这里就不重复这些了。这一节会聚焦在虚拟memory子系统的实际的实作细节、以及与此相关的成本。 虚拟定址空间是由 CPU 的memory管理单元（Memory Management Unit，MMU）实作的。操作系统必须填写分页表（page table）资料结构，但大多数 CPU 会自行做掉剩下的工作。这真的是个非常复杂的机制；理解它的最佳方式是引入使用的资料结构来描述虚拟定址空间。 由 MMU 实行的地址转译的输入为一个虚拟地址。它的值通常有极少量 –– 如果有的话 –– 的限制。在 32 位元系统上的虚拟地址为 32 位元的值，而在 64 位元系统上为 64 位元的值。在某些系统上，像是 x86 与 x86-64，使用的地址实际上牵涉到另一层级的间接性：这些架构使用了分段（segment），其只不过是将偏移量加到每个逻辑地址上。我们可以忽略地址产生过程的这个部分，它很琐碎，而且就memory管理的效能而言，不是程序开发者必须要关心的东西。24 24. x86 上的分段限制是攸关效能的，但这又是另一个故事了。 ↩ "},"virtual-memory/simplest-address-translation.html":{"url":"virtual-memory/simplest-address-translation.html","title":"4.1. 最简单的地址转译","keywords":"","body":"4.1. 最简单的地址转译 有趣的部分是虚拟地址到实体地址的转译。MMU 能够逐个分页重新映射地址。就如同定址cache行的时候一样，虚拟地址会被切成多个部分。这些部分用来索引多个用以建构最终实体地址的表格。以最简单的模型而言，我们仅有一个层级的表格。 图 4.1：一层地址转译 图 4.1 显示了到底是怎么使用虚拟地址的不同部分的。开头的部分用以选择一个分页目录（Page Directory）中的一个项目；在这个目录中的每个项目都能由操作系统个别设定。分页目录项目决定了一个实体memory分页的地址；在分页目录中，能够有多于一个指到相同实体地址的项目。记忆单元的完整实体地址是由分页目录的分页地址、结合虚拟地址的低位元所决定的。分页目录项目也包含一些像是存取权限这类关于分页的额外资讯。 分页目录的资料结构储存于主memory中。操作系统必须分配连续的实体memory、并将这个memory区域的基底地址（base address）储存在一个特殊的暂存器中。虚拟memory中适当的位元量接著会被用作一个分页目录的索引 –– 它实际上是一个目录项目的阵列。 作为一个实际的例子，以下是在 x86 机器上的 4MB 分页所使用的布局。虚拟memory的偏移量部分的大小为 22 位元，足以定址一个 4MB 分页中的每个位元组。虚拟memory剩馀的 10 位元选择了分页目录里 1024 个项目中的其中一个。每个项目包含一个 4MB 分页的一个 10 位元的基底地址，其会与偏移量结合以构成完整的 32 位元地址。 "},"virtual-memory/multi-level-page-tables.html":{"url":"virtual-memory/multi-level-page-tables.html","title":"4.2. 多层级分页表","keywords":"","body":"4.2. 多层级分页表 4MB 的分页并非常态，它们会浪费很多的memory，因为操作系统必须执行的许多操作都需要与memory分页对齐（align）。以 4kB 分页而言（32 位元机器、甚至经常是 64 位元机器上的常态），虚拟地址的偏移量部分的大小仅有 12 位元。这留了 20 位元作为分页目录的选择器。一个有著 220 个项目的表格是不切实际的。即使每个项目只会有 4 位元组，表格大小也会有 4MB。由于每个行程都可能拥有它自己独有的分页目录，这些分页目录会占据系统中大量的实体memory。 解决方法是使用多个层级的分页表。阶层于是形成一个巨大、稀疏的分页目录；没有真的用到的定址空间范围不需要被分配的memory。这种表示法因而紧密得多了，使得memory中能够拥有许多行程的分页表，而不会太过于影响效能。 图 4.2：四层地址转译 现今最复杂的分页表结构由四个层级所构成。图 4.2 显示了这种实作的示意图。虚拟memory –– 在这个例子中 –– 被切成至少五个部分。其中四个部分为不同目录的索引。第四层目录被 CPU 中一种特殊用途的暂存器所指涉。第二层到第四层目录的内容为指向更低层级目录的参考。若是一个目录项目被标记为空，它显然不需要指到任何更低层的目录。如此一来，分页表树便能够稀疏且紧密。第一层目录的项目为 –– 就像在图 4.1 一样 –– 部份的实体地址，加上像存取权限这类辅助资料。 要确定对应到一个虚拟地址的实体地址，处理器首先会确定最高层目录的地址。这个地址通常储存在一个暂存器中。CPU 接著取出对应到这个目录的虚拟memory的索引部分，并使用这个索引来挑选合适的项目。这个项目是下一个目录的地址，使用虚拟地址的下一个部分来索引。这个过程持续到抵达第一层目录，这时目录项目的值为实体地址的高位部分。加上来自虚拟memory的分页偏移位元便组成了完整的实体地址。这个过程被称为分页树走访（page tree walking）。有些处理器（像是 x86 与 x86-64）会在硬件中执行这个操作，其它的则需要来自操作系统的协助。 每个在系统中执行的行程会需要它自己的分页表树。部分地共享树是可能的，但不如说这是个例外状况。因此，如果分页表树所需的memory尽可能地小的话，对效能与延展性而言都是有益的。理想的情况是将用到的memory彼此靠近地摆在虚拟定址空间中；实际用到的实体地址则无关紧要。对一支小程序而言，仅仅使用在第二、三、四层各自的一个目录、以及少许第一层目录，可能还过得去。在有著 4kB 分页以及每目录 512 个项目的 x86-64 上，这能够以总计 4 个目录（每层一个）来定址 2MB。1GB 的连续memory能够以一个第二到第四层目录、以及 512 个第一层目录来定址。 不过，假设能够连续地分配所有memory也太过于简化了。为了弹性起见，一个行程的堆叠（stack）与堆积（heap）区域 –– 在大多情况下 –– 会被分配在定址空间中极为相对的两端。这令两个区域在需要时都能尽可能地增长。这表示，最有可能是两个需要的第二层目录，以及与此相应的、更多低层级的目录。 但即使如此也不总是符合当前的实际状况。为了安全考量，一个可执行程序的多个部分（程序码、资料、堆积、堆叠、动态共享物件〔Dynamic Shared Object，DSO〕，又称共享函式库〔shared library〕）会被映射在随机化的地址上 [9]。随机化扩大了不同部份的相对位置；这暗示著，在一个行程里使用中的不同memory区域会广泛地散布在虚拟定址空间中。借由在随机化的地址的位元数上施加一些限制也能够限制范围，但这无疑 –– 在大多情况下 –– 会让一个行程无法以仅仅一或两个第二与第三层目录来执行。 若是效能比起安全性真的重要太多了，也能够把随机化关闭。于是操作系统通常至少会在虚拟memory中连续地载入所有的 DSO。 "},"virtual-memory/optimizing-page-table-access.html":{"url":"virtual-memory/optimizing-page-table-access.html","title":"4.3. 最佳化分页表存取","keywords":"","body":"4.3. 最佳化分页表存取 分页表的所有资料结构都会被保存在主memory中；操作系统就是在这里建构并更新表格的。在一个行程的创建、或是分页表的一次修改之后，都会立即通知 CPU。分页表是用以将每个虚拟地址，使用上述的分页表走访来转成实体地址。更准确地说：每一层至少会有一个目录会被用在转换一个虚拟地址的过程中。这需要高达四次memory存取（以执行中行程的一个单一存取而言），这很慢。将这些目录表的项目视为普通的资料、并在 L1d、L2、等等cache它们是办得到的，但这可能还是太慢了。 从最早期的虚拟memory开始，CPU 设计者便已采用了一种不同的最佳化。一个简单的计算能够显示，仅将目录表的项目保存在 L1d 以及更高层级的cache中会招致可怕的效能。每个独立的地址计算会需要相符于分页表深度的若干 L1d 存取。这些存取无法平行化，因为它们都依赖于前一次查询的结果。单是这样就会 –– 在一台有著四个分页表阶层的机器上 –– 需要至少 12 个周期。再加上 L1d 错失的机率，结果是指令管道无法隐藏任何东西。额外的 L1d 存取也需要将宝贵的频宽偷到cache去。 所以，不只是将目录表的项目cache起来，而是连实体分页地址的完整计算结果也会被cache。跟程序码与资料cache行得通的理由相同，这种cache的地址计算结果是很有效的。由于虚拟地址的分页偏移量的部分不会参与到实体分页地址的计算，仅有虚拟地址的剩馀部分会用来作为cache的标籤。视分页大小而定，这代表数百或数千条的指令或资料物件会共享相同的标籤，因而共享相同的实体地址前缀（prefix）。 储存计算得来的值的cache被称为转译后备缓冲区（Translation Look-Aside Buffer，TLB）。它通常是个很小的cache，因为它必须非常快。现代的 CPU 提供了多层 TLB cache，就如同其他cache一样；更高层的cache更大也更慢。L1TLB 的小容量通常借由令cache为全关联式、加上 LRU 逐出策略来弥补。近来，这种cache的大小已经持续成长，并且 –– 在进行中 –– 被转变为集合关联式。因此，当一个新的项目必须被加入时，被逐出并取代的项目可能不是最旧的一个。 如同上面所注记的，用来存取 TLB 的标籤为虚拟地址的一部分。若是在cache中有比对到标籤，最终的实体地址就能够借由将来自虚拟地址的分页偏移量加到被cache的值上而计算出来。这是个非常快的过程；它必须如此，因为实体地址必须可用于每条使用独立地址的指令、以及 –– 在某些情况下 –– 使用实体地址作为键值（key）的 L2 查询。若是 TLB 查询没有命中，处理器必须要进行一次分页表走访；这可能是非常昂贵的。 透过软件或硬件预取程序码或资料时，若是地址在另一个分页上，能够暗自预取 TLB 的项目。这对于硬件预取而言是不可能的，因为硬件可能会引发无效的分页表走访。程序开发者因而无法仰赖硬件预取来预取 TLB 项目。必须明确地使用预取指令来达成。TLB –– 就像是资料与指令cache –– 能够出现在多个层级。就如同资料cache一样，TLB 通常有两种：指令 TLB（ITLB）以及资料 TLB（DTLB）。像是 L2TLB 这种更高层级的 TLB 通常是统一式的，与其它cache的情况相同。 "},"virtual-memory/optimizing-page-table-access/caveats-of-using-a-tlb.html":{"url":"virtual-memory/optimizing-page-table-access/caveats-of-using-a-tlb.html","title":"4.3.1. 使用 TLB 的预警","keywords":"","body":"4.3.1. 使用 TLB 的预警 TLB 是个处理器核的全域（global）资源。所有执行在处理器核的执行绪与行程都使用相同的 TLB。由于虚拟到实体地址的转译是看设置的是哪一个分页表树而定的，因此若是分页表被更改了，CPU 就不能盲目地重复使用cache的项目。每个行程有个不同的分页表树（但同个行程中的执行绪并非如此）。假如有的话，系统核心与 VMM（虚拟机器监视器）亦是如此。一个行程的定址空间布局也是可能改变的。有两种处理这个问题的方式： 每当分页表树被更改都冲出 TLB。 扩充 TLB 项目的标籤，以额外且唯一地识别它们所指涉到的分页表树。 在第一种情况中，每当情境切换（context switch）都会冲出 TLB。由于 –– 在大多操作系统中 –– 从一个执行绪／行程切换到另一个时，需要执行一些系统核心的程序码，TLB 冲出会被限制在离开（有时候是进入）系统核心定址空间时。在虚拟化的系统上，当系统核心必须呼叫 VMM、并在返回的途中时，这也会发生。若是系统核心和／或 VMM 不必使用虚拟地址、或是能够重复使用与发出系统／VMM 呼叫的行程或系统核心相同的虚拟地址（即，定址空间被重叠了），TLB 只须在 –– 离开系统核心或 VMM 后 –– 处理器恢复一个不同的行程或系统核心的执行时冲出。 冲出 TLB 有效但昂贵。举例来说，在执行一个系统呼叫时，系统核心程序可能会被限制在数千行触及 –– 或许 –– 少数的新分页（或者一个大分页，如同在某些架构上的 Linux 的情况）的指令。这个操作仅会取代与被触及的分页一样多的 TLB 项目。以 Intel 的 Core2 架构、附加它的 128 ITLB 与 256 DTLB 的项目而言，一次完整的冲出可能意味著被不必要地冲出的项目（分别）会超过 100 与 200 个。当系统呼叫返回（return）到相同的行程时，所有那些被冲出的 TLB 项目都能够被再次用到，但它们将会被丢掉。对于在系统核心或 VMM 中经常用到的程序码亦是如此。尽管系统核心以及 VMM 的分页表通常不会改变，因此 TLB 项目 –– 理论上 –– 能够被保存相当长的一段时间，但在每次进入系统核心时，TLB 也必须从零开始填入。这也解释了为何现今处理器中的 TLB cache并没有更大的原因：程序的执行时间非常可能不会长到足以填入这所有的项目。 这个事实 –– 当然 –– 不会逃出 CPU 架构师的掌心。最佳化cache冲出的一个可能性是，单独令 TLB 项目失效。举例来说，若是系统核心与资料落在一个特殊的地址范围，那么仅有落在这个地址范围的分页必须从 TLB 逐出。这只需要比对标籤，因而不怎么昂贵。这个方法在定址空间的一部分 –– 例如，透过一次 munmap 呼叫 –– 被更改的情况下也是有用的。 一个好得多的解法是扩充用来 TLB 存取的标籤。若是 –– 除了虚拟地址的部分以外 –– 为每个分页表树（即，一个行程的定址空间）加上一个唯一的识别子（identifier），TLB 根本就不必完全冲出。系统核心、VMM、以及独立的行程全都能够拥有唯一的识别子。采用这个方案的唯一议题是，可用于 TLB 标籤的位元数量会被严重地限制，而定址空间的数量则否。这表示是有必要重复使用某些识别子的。当这种情况发生时，TLB 必须被部分冲出（如果可能的话）。所有带著被重复使用的识别子的项目都必须被冲出，但这 –– 但愿如此 –– 是个非常小的集合。 当多个行程执行在系统中时，这种扩充的 TLB 标记在虚拟化的范围之外是有优势的。假如每个可执行行程的memory使用（是故 TLB 项目的使用）受限了，有个好机会是，当一个行程再次被排程时，它最近使用的 TLB 项目仍然在 TLB 中。但还有两个额外的优点： 特殊的定址空间 –– 像是那些被系统核心或 VMM 所用到的 –– 通常只会被进入一段很短的时间；后续的控制经常是返回到启动这次进入的定址空间。没有标籤的话，便会执行一或两次 TLB 冲出。有标籤的话，呼叫定址空间的cache转译会被保留，而且 –– 由于系统核心与 VMM 定址空间根本不常更改 TLB 项目 –– 来自前一次系统呼叫的转译等仍然可以被使用。 当在两条相同行程的执行绪之间切换时，根本不需要 TLB 冲出。不过，没有扩充的 TLB 标籤的话，进入系统核心就会销毁第一条执行绪的项目。 某些处理器已经 –– 一段时间了 –– 实作了这些扩充标籤。AMD 以 Pacifica 虚拟化扩充引入了一种 1 位元的标籤扩充。这个 1 位元定址空间 ID（Address Space ID，ASID）是 –– 在虚拟化的情境中 –– 用以从客户域（guest domain）的定址空间区别出 VMM 的定址空间。这使得操作系统得以避免在每次进入 VMM（举例来说，处理一个分页错误〔page fault〕）时冲出客户端的 TLB 项目、或者在返回客户端时冲出 VMM 的 TLB 项目。这个架构未来将会允许使用更多的位元。其它主流处理器可能也会遵循这套方法并支援这个功能。 "},"virtual-memory/optimizing-page-table-access/influencing-tlb-performance.html":{"url":"virtual-memory/optimizing-page-table-access/influencing-tlb-performance.html","title":"4.3.2. 影响 TLB 效能","keywords":"","body":"4.3.2. 影响 TLB 效能 有几个影响 TLB 效能的因素。第一个是分页的大小。显然地，分页越大、会被塞进去的指令或资料物件也越多。所以一个比较大的cache大小减少了所需地址转译的整体数量，代表 TLB cache中需要更少的项目。大部分架构现今允许使用多种不同的分页大小；有些大小能够并存地使用。举例来说，x86／x86-64 处理器拥有寻常的 4kB 分页大小，但它们也分别能够使用 4MB 与 2MB 的分页。IA-64 与 PowerPC 允许像是 64kB 的大小作为基础分页大小。 不过，大分页尺寸的使用也随之带来了一些问题。为了大分页而使用的memory区域在实体memory中必须是连续的。若是实体memory管理的单位大小被提高到虚拟memory分页的大小，浪费的memory总量就会增加。各种memory操作（像是载入可执行程序）需要对齐到分页边界。这表示，平均而言，在每次映射的实体memory中，每次映射浪费了一半的分页大小。这种浪费能够轻易地累加；这因此对实体memory分配的合理单位大小加了个上限。 将单位大小提升到 2MB，以容纳 x86-64 上的大分页无疑并不实际。这个大小太大了。但这又意味著每个大分页必须由多个较小的分页所构成。而且这些小分页在实体memory中必须是连续的。以 4kB 的单位分页大小分配 2MB 的连续实体memory具有挑战性。这需要寻找一个有著 512 个连续分页的空閒区域。在系统执行一段时间、并且实体memory变得片段之后，这可能极端困难（或者不可能）。 因此在 Linux 上，有必要在系统启动的时候使用特殊的 hugetlbfs 档案系统来分配这些大分页。一个固定数量的实体分页会被保留来专门作为大虚拟分页来使用。这绑住了可能不会一直用到的资源。这也是个有限的池（pool）；增加它通常代表著重新启动系统。尽管如此，在效能贵重、资源充足、且麻烦的设置不是个大阻碍的情况下，庞大的分页便为大势所趋。资料库服务器就是个例子。 $ eu-readelf -l /bin/ls Program Headers: Type Offset VirtAddr PhysAddr FileSiz MemSiz Flg Align ... LOAD 0x000000 0x0000000000400000 0x0000000000400000 0x0132ac 0x0132ac R E 0x200000 LOAD 0x0132b0 0x00000000006132b0 0x00000000006132b0 0x001a71 0x001a71 RW 0x200000 ... 图 4.3：ELF 程序标头指示了对齐需求 提高最小的虚拟分页大小（对比于可选的大分页）也有它的问题。memory映射操作（例如，载入应用程序）必须遵循这些分页大小。不可能有更小的映射。一个可执行程序不同部分的位置 –– 对大多架构而言 –– 有个固定的关系。若是分页大小增加到超过在可执行程序或者 DSO 创建时所考虑的大小时，就无法执行载入操作。将这个限制记在心上是很重要的。图 4.3 显示了能够如何决定一个 ELF 二进位资料（binary）的对齐需求的。它被编码在 ELF 的程序标头（header）。在这个例子中，一个 x86-64 的二进位资料，值为 20000016=2,097,152=2MB 200000_{16} = 2,097,152 = \\text{2MB} 200000​16​​=2,097,152=2MB，与处理器所支援的最大分页大小相符。 使用较大的分页大小有个次要的影响：分页表树的层级数量会被减低。由于对应到分页偏移量的虚拟地址部分增加了，就没有剩下那么多需要透过分页目录处理的位元了。这表示，在一次 TLB 错失的情况下，必须完成的工作总量减少了。 除了使用大分页尺寸外，也可能借由将同时用到的资料搬移到较少的分页上，以减少所需的 TLB 项目数量。这类似于我们先前讨论的针对cache使用的一些最佳化。 Only now the alignment required is large. 考虑到 TLB 项目的数量非常少，这会是个重要的最佳化。 "},"virtual-memory/impact-of-virtualization.html":{"url":"virtual-memory/impact-of-virtualization.html","title":"4.4. 虚拟化的影响","keywords":"","body":"4.4. 虚拟化的影响 操作系统映像（image）的虚拟化会变得越来越流行；这表示memory管理的另一层会被加到整体中。行程（基本上为监狱〔jail〕）或操作系统容器（container）的虚拟化并不属于这个范畴，因为只有一个操作系统会牵涉其中。像 Xen 或 KVM 这类技术能够 –– 无论有没有来自处理器的协助 –– 执行独立操作系统映像。在这些情况下，只有一个直接控制实体memory存取的软件。 图 4.4：Xen 虚拟化模型 在 Xen 的情况下（见图 4.4），Xen VMM 即是这个软件。不过 VMM 本身并不实作太多其它的硬件控制。不若在其它较早期的系统（以及首次释出的 Xen VMM）上的 VMM，除了memory与处理器之外的硬件是由具有特权的 Dom0 域所控制的。目前，这基本上是与没有特权的 DomU 系统核心相同的系统核心，而且 –– 就所关心的memory管理而言 –– 它们没什么区别。重要的是，VMM 将实体memory分发给了 Dom0 与 DomU 系统核心，其因而实作了普通的memory管理，就好像它们是直接执行在一个处理器上一样。 为了实现完成虚拟化所需的域的分离，Dom0 与 DomU 系统核心中的memory处理并不具有无限制的实体memory存取。VMM 不是借由分发独立的实体分页、并让客户端操作系统处理定址的方式来分发memory；这不会提供任何针对有缺陷或者流氓客户域的防范。取而代之地，VMM 会为每个客户域建立它自己拥有的分页表树，并使用这些资料结构来分发memory。好处是能够控制对分页表树的管理资讯的存取。若是程序没有合适的权限，它就什么也无法做。 这种存取控制被利用在 Xen 提供的虚拟化之中，无论使用的是半虚拟化（paravirtualization）或是硬件虚拟化（亦称全虚拟化）。客户域采用了有意与半虚拟化以及硬件虚拟化十分相似的方式，为每个行程建立了它们的分页表树。无论客户端操作系统在何时修改了它的分页表，都会呼叫 VMM。VMM 于是使用在客户域中更新的资讯来更新它自己拥有的影子分页表。这些是实际被硬件用到的分页表。显然地，这个过程相当昂贵：分页表树每次修改都需要一次 VMM 的呼叫。在没有虚拟化的情况下对memory映射的更动并不便宜，而它们现在甚至变得更昂贵了。 考虑到从客户端操作系统到 VMM 的更改并返回，它们本身已经非常昂贵，额外的成本可能非常大。这即是为何处理器开始拥有额外的功能，以避免影子分页表的建立。这很好，不仅因为速度的关系，它也减少了 VMM 的memory消耗。Intel 有扩充分页表（Extended Page Table，EPT），而 AMD 称它为巢状分页表（Nested Page Table，NPT）。基本上这两个技术都拥有客户端操作系统从「客户虚拟地址（guest virtual address）」产生「宿主虚拟地址（host virtual address）」的分页表。宿主虚拟地址接著必须被进一步 –– 使用每个域的 EPT／NPT 树 –– 转译成真正的实体地址。这会令memory处理以几乎是非虚拟化情况的速度来进行，因为大多数memory管理的 VMM 项目都被移除了。它也减少了 VMM 的memory使用，因为现在每个域（对比于行程）都仅有一个必须要维护的分页。 这个额外的地址转译步骤的结果也会储存在 TLB 中。这表示 TLB 不会储存虚拟的实体地址，而是查询的完整结果。已经解释过 AMD 的 Pacifica 扩充引入了 ASID 以避免在每个项目上的 TLB 冲出。ASID 的位元数量在最初释出的处理器扩充中只有一位；这足以区隔 VMM 与客户端操作系统了。Intel 拥有用于相同目的的虚拟处理器 ID（virtual processor ID，VPID），只不过有更多的位元数。但是对于每个客户域而言，VPID 都是固定的，因此它无法被用来标记个别的行程，也不能在这个层级避免 TLB 冲出。 每次定址空间修改所需的工作量是有著虚拟化操作系统的一个问题。不过，基于 VMM 的虚拟化还有另一个固有的问题：没有办法拥有两层memory处理。但是memory处理很难（尤其在将像 NUMA 这类难题纳入考虑的时候，见第五节）。Xen 使用一个分离 VMM 的方式使得最佳化的（甚至是好的）处理变得困难，因为所有的memory管理实作的难题 –– 包含像memory区域的探寻这类「琐碎」事 –– 都必须在 VMM 中重复。操作系统拥有成熟且最佳化的实作；真的应该避免重复这些事。 图 4.5：KVM 虚拟化模型 这即是为何废除 VMM／Dom0 模型是个如此有吸引力的替代方案。图 4.5 显示了 KVM Linux 系统核心扩充是如何试著解决这个问题的。没有直接执行在硬件上、并控制所有客户的分离 VMM；而是一个普通的 Linux 系统核心接管了这个功能。这表示在 Linux 系统核心上完整且精密的memory处理功能被用来管理系统中的memory。客户域与被创造者称为「客户模式（guest mode）」的普通的使用者层级行程一同执行。虚拟化功能 –– 半虚拟化或全虚拟化 –– 是由 KVM VMM 所控制。这只不过是另一个使用者层级的行程，使用系统核心实作的特殊 KVM 装置来控制一个客户域。 这个模型相较于 Xen 模型的分离 VMM 的优点是，即使在使用客户端操作系统时仍然有两个运作的memory处理者，但只需要唯一一种在 Linux 系统核心中的实作。没有必要像 Xen VMM 一样在另一段程序码中重复相同的功能。这导致更少的工作、更少的臭虫、以及 –– 也许 –– 更少两个memory管理者接触的摩擦，因为在一个 Linux 客户端中的memory管理者会与外部在裸机上执行的 Linux 系统核心的memory管理者做出相同的假设。 总而言之，程序开发者必须意识到，采用虚拟化的时候，cache错失（指令、资料、或 TLB）的成本甚至比起没有虚拟化还要高。任何减少这些工作的最佳化，在虚拟化的环境中甚至会获得更多的回报。处理器设计者将会 –– 随著时间的推移 –– 透过像是 EPT 与 NPT 这类技术来逐渐减少这个差距，但它永远也不会完全消失。 "},"numa-support.html":{"url":"numa-support.html","title":"5. NUMA 支援","keywords":"","body":"5. NUMA 支援 在第二节我们看过，在某些机器上，存取实体memory特殊区域的成本差异是视存取的源头而定的。这种类型的硬件需要来自操作系统以及应用程序的特殊照顾。我们会以 NUMA 硬件的一些细节开头，接著我们会涵盖 Linux 系统核心为 NUMA 提供的一些支援。 "},"numa-support/numa-hardware.html":{"url":"numa-support/numa-hardware.html","title":"5.1. NUMA 硬件","keywords":"","body":"5.1. NUMA 硬件 非均匀memory架构正变得越来越普遍。在最简单的 NUMA 类型中，一个处理器能够拥有本地memory（见图 2.3），存取它比存取其它处理器的本地memory还便宜。对这种类型的 NUMA 系统而言，成本差异并不大 –– 即，NUMA 因子很小。 NUMA 也会 –– 而且尤其会 –– 用在大机器中。我们已经描述过拥有多个存取相同memory的处理器的问题。对商用硬件而言，所有处理器都会共享相同的北桥（此刻忽略 AMD Opteron NUMA 节点，它们有它们自己的问题）。这使得北桥成为一个严重的瓶颈，因为所有memory流量都会经过它。当然，大机器能够使用客制的硬件来代替北桥，但除非使用的memory晶片拥有多个埠 –– 即，它们能够从多条总线使用 –– 不然依旧有个瓶颈在。多埠 RAM 很复杂、而且建立与支援起来很昂贵，因此几乎不会被使用。 下一个复杂度上的改进为 AMD 使用的模型，其中的互连机制（在 AMD 情况下为超传输〔Hyper Transport〕，是它们由 Digital 授权而来的技术）为没有直接连接到 RAM 的处理器提供了存取。能够以这种方式组成的结构大小是有限的，除非你想要任意地增加直径（diameter）（即，任意两节点之间的最大距离）。 图 5.1：超立方体 一种连接节点的高效拓扑（topology）为超立方体（hypercube），其将节点的数量限制在 2C 2^{C} 2​C​​，其中 C C C 为每个节点拥有的交互连接介面的数量。以所有有著 2n 2^{n} 2​n​​ 个 CPU 与 n n n 条交互连接的系统而言，超立方体拥有最小的直径。图 5.1 显示了前三种超立方体。每个超立方体拥有绝对最小（the absolute minimum）的直径 C C C。AMD 第一世代的 Opteron 处理器，每个处理器拥有三条超传输连结。至少有一个处理器必须有个附属在一条连结上的南桥，代表 –– 目前而言 –– 一个 C=2 C = 2 C=2 的超立方体能够直接且有效率地实作。下个世代将在某个时间点拥有四条连结，届时将可能有 C=3 C = 3 C=3 的超立方体。 不过，这不代表无法支援更大的处理器集合体（accumulation）。有些公司已经开发出能够使用更大的处理器集合的 crossbar（例如，Newisys 的 Horus）。但这些 crossbar 提高了 NUMA 因子，而且在一定数量的处理器上便不再有效。 下一个改进为连接 CPU 的群组，并为它们全体实作一个共享的memory。所有这类系统都需要特制化的硬件，绝不是商用系统。这样的设计存在多方面的复杂度。一个仍然十分接近于商用机器的系统为 IBM x445 以及类似的机器。它们能够当作有著 x86 与 x86-64 的普通 4U、8 路机器购买。两台（某些时候高达四台）这种机器就能够被连接起来运作，如同一台有著共享memory的机器。使用的交互连接引入了一个操作系统 –– 以及应用程序 –– 必须纳入考量的重要的 NUMA 因子。 在光谱的另一端，像 SGI 的 Altix 这样的机器是专门被设计来互连的。SGI 的 NUMAlink 互连结构非常地快，同时拥有很短的等待时间；两个特性对于高效能计算（high-performance computing，HPC）都是必要条件，尤其是在使用讯息传递介面（Message Passing Interface，MPI）的时候。当然，缺点是，这种精密与特制化是非常昂贵的。它们令合理地低的 NUMA 因子成为可能，但以这些机器能拥有的 CPU 数量（几千个）以及有限的互连能力，NUMA 因子实际上是动态的，而且可能因工作负载而达到不可接受的程度。 更常使用的解决方法是，使用高速网络将许多商用机器连接起来，组成一个集群（cluster）。不过，这些并非 NUMA 机器；它们没有实作共享的定址空间，因此不会归于这里讨论的任何一个范畴中。 "},"numa-support/os-support-for-numa.html":{"url":"numa-support/os-support-for-numa.html","title":"5.2.操作系统对 NUMA 的支援","keywords":"","body":"5.2.操作系统对 NUMA 的支援 为了支援 NUMA 机器，操作系统必须将memory分散式的性质纳入考量。举例来说，若是一个行程执行在一个给定的处理器上，被指派给行程定址空间的实体 RAM 理想上应该要来自本地memory。否则每条指令都必须为了程序码与资料去存取远端的memory。有些仅存于 NUMA 机器的特殊情况要被考虑进去。DSO 的文字区段（text segment）在一台机器的实体 RAM 中通常正好出现一次。但若是 DSO 被所有 CPU 上的行程与执行绪用到（例如，像 libc 这类基本的执行期函式库），这表示并非一些、而是全部的处理器都必须拥有远端的地址。操作系统理想上会将这种 DSO「映像（mirror）」到每个处理器的实体 RAM 中，并使用本地的副本。这并非一种最佳化，而是个必要条件，而且通常难以实作。它可能没有被支援、或者仅以有限的形式支援。 为了避免情况变得更糟，操作系统不该将一个行程或执行绪从一个节点迁移到另一个。操作系统应该已经试著避免在一般的多处理器机器上迁移行程，因为从一个处理器迁移到另一个处理器意味著cache内容遗失。若是负载分配需要将一个行程或执行绪迁出一个处理器，操作系统通常能够挑选任一个拥有足够剩馀容量的新处理器。在 NUMA 环境中，新处理器的选择受到稍微多一些的限制。对于行程正在使用的memory，新选择的处理器不该有比旧的处理器还高的存取成本；这限制目标的清单。若是没有符合可用标准的空閒处理器，操作系统除了迁移到memory存取更为昂贵的处理器以外别无他选。 在这种情况下，有两种可能的方法。首先，可以期盼这种情况是暂时的，而且行程能够被迁回到一个更合适的处理器上。或者，操作系统也能够将行程的memory迁移到更靠近新使用的处理器的实体分页上。这是个相当昂贵的操作。可能得要复制大量的memory，尽管不必在一个步骤中完成。当发生这种情况的时候，行程必须 –– 至少短暂地 –– 中止，以正确地迁移对旧分页的修改。有了让分页迁移高效又快速，有整整一串其它的必要条件。简而言之，操作系统应该避免它，除非它是真的有必要的。 一般来说，不能够假设在一台 NUMA 机器上的所有行程都使用等量的memory，使得 –– 由于遍及各个处理器的行程的分布 –– memory的使用也会被均等地分配。事实上，除非执行在机器上的应用程序非常特殊（在 HPC 世界中很常见，但除此之外则否），不然memory的使用是非常不均等的。某些应用程序会使用巨量的memory，其余的几乎不用。若总是分配产生请求的处理器本地的memory，这将会 –– 或早或晚 –– 造成问题。系统最终将会耗尽执行大行程的节点本地的memory。 为了应对这些严重的问题，memory –– 预设情况下 –– 不会只分配给本地的节点。为了利用所有系统的memory，预设策略是条带化（stripe）memory。这保证所有系统memory的同等使用。作为一个副作用，有可能变得能在处理器之间自由迁移行程，因为 –– 平均而言 –– 对于所有用到的memory的存取成本没有改变。由于很小的 NUMA 因子，条带化是可以接受的，但仍不是最好的（见 5.4 节的数据）。 这是个帮助系统避免严重问题、并在普通操作下更为能够预测的劣化（pessimization）。但它降低整体的系统效能，在某些情况下尤为显著。这即是 Linux 允许每个行程选择memory分配规则的原因。一个行程能够为它自己以及它的子行程选择不同的策略。我们将会在第六节介绍能用于此的介面。 "},"numa-support/published-information.html":{"url":"numa-support/published-information.html","title":"5.3. 被发布的资讯","keywords":"","body":"5.3. 被发布的资讯 系统核心透过 sys 虚拟档案系统（sysfs）将处理器cache的资讯发布在 /sys/devices/system/cpu/cpu*/cache 在 6.2.1 节，我们会看到能用来查询不同cache大小的介面。这里重要的是cache的拓朴。上面的目录包含了列出 CPU 拥有的不同cache资讯的子目录（叫做 index*）。档案 type、level、与 shared_cpu_map 是在这些目录中与拓朴有关的重要档案。一个 Intel Core 2 QX6700 的资讯看起来就如表 5.1。 type level shared_cpu_map cpu0 index0 Data 1 00000001 index1 Instruction 1 00000001 index2 Unified 2 00000003 cpu1 index0 Data 1 00000002 index1 Instruction 1 00000002 index2 Unified 2 00000003 cpu2 index0 Data 1 00000004 index1 Instruction 1 00000004 index2 Unified 2 0000000c cpu3 index0 Data 1 00000008 index1 Instruction 1 00000008 index2 Unified 2 0000000c 表 5.1：Core 2 CPU cache的 sysfs 资讯 这份资料的意义如下： 每颗处理器核25拥有三个cache：L1i、L1d、L2。 L1d 与 L1i cache没有被任何其它的处理器核所共享 –– 每颗处理器核有它自己的一组cache。这是由 shared_cpu_map 中的位元图（bitmap）只有一个被设置的位元所暗示的。 cpu0 与 cpu1 的 L2 cache是共享的，正如 cpu2 与 cpu3 上的 L2 一样。 若是 CPU 有更多cache阶层，也会有更多的 index* 目录。 type level shared_cpu_map cpu0 index0 Data 1 00000001 index1 Instruction 1 00000001 index2 Unified 2 00000001 cpu1 index0 Data 1 00000002 index1 Instruction 1 00000002 index2 Unified 2 00000002 cpu2 index0 Data 1 00000004 index1 Instruction 1 00000004 index2 Unified 2 00000004 cpu3 index0 Data 1 00000008 index1 Instruction 1 00000008 index2 Unified 2 00000008 cpu4 index0 Data 1 00000010 index1 Instruction 1 00000010 index2 Unified 2 00000010 cpu5 index0 Data 1 00000020 index1 Instruction 1 00000020 index2 Unified 2 00000020 cpu6 index0 Data 1 00000040 index1 Instruction 1 00000040 index2 Unified 2 00000040 cpu7 index0 Data 1 00000080 index1 Instruction 1 00000080 index2 Unified 2 00000080 表 5.2：Opteron CPU cache的 sysfs 资讯 对于一个四槽、双核的 Opteron 机器，cache资讯看起来如表 5.2。可以看出这些处理器也有三种cache：L1i、L1d、L2。没有处理器核共享任何阶层的cache。这个系统有趣的部分在于处理器拓朴。少了这个额外资讯，就无法理解cache资料。sys 档案系统将这个资讯摆在下面这个档案 /sys/devices/system/cpu/cpu*/topology 表 5.3 显示了在 SMP Opteron 机器的这个阶层里头的令人感兴趣的档案。 physical_package_id core_id core_siblings thread_siblings cpu0 0 0 00000003 00000001 cpu1 1 00000003 00000002 cpu2 1 0 0000000c 00000004 cpu3 1 0000000c 00000008 cpu4 2 0 00000030 00000010 cpu5 1 00000030 00000020 cpu6 3 0 000000c0 00000040 cpu7 1 000000c0 00000080 表 5.3：Opteron CPU 拓朴的 sysfs 资讯 将表 5.2 与 5.3 摆在一起，我们能够发现 没有 CPU 拥有 HT （thethread_siblings 位元图有一个位元被设置）、 这个系统实际上共有四个处理器（physical_package_id 0 到 3）、 每个处理器有两颗核、以及 没有处理器核共享任何cache。 这正好与较早期的 Opteron 一致。 目前为止提供的资料中完全缺少的是，有关这台机器上的 NUMA 性质的资讯。任何 SMP Opteron 机器都是一台 NUMA 机器。为了这份资料，我们必须看看在 NUMA 机器上存在的 sys 档案系统的另一个部分，即下面的阶层中 /sys/devices/system/node 这个目录包含在系统上的每个 NUMA 节点的子目录。在特定节点的目录中有许多档案。在前两张表中描述的 Opteron 机器的重要档案与它们的内容显示在表 5.4。 cpumap distance node0 00000003 10 20 20 20 node0 0000000c 20 10 20 20 node2 00000030 20 20 10 20 node3 000000c0 20 20 20 10 表 5.4：Opteron 节点的 sysfs 资讯 这个资讯将所有的一切连系在一起；现在我们有个机器架构的完整轮廓了。我们已经知道机器拥有四个处理器。每个处理器构成它自己的节点，能够从 node* 目录的 cpumap 档案中的值里头设置的位元看出来。在那些目录的 distance 档案包含一组值，一个值代表一个节点，表示在各个节点上存取memory的成本。在这个例子中，所有本地memory存取的成本为 10，所有对任何其它节点的远端存取的成本为 20。26这表示，即使处理器被组织成一个二维超立方体（见图 5.1），在没有直接连接的处理器之间存取也没有比较贵。成本的相对值应该能用来作为存取时间的实际差距的估计。所有这些资讯的准确性是另一个问题了。 25. cpu0 到 cpu3 为处理器核的相关资讯来自于另一个将会简短介绍的地方。 ↩ 26. 顺带一提，这是不正确的。这个 ACPI 资讯明显是错的，因为 –– 虽然用到的处理器拥有三条连贯的超传输连结 –– 至少一个处理器必须被连接到一个南桥上。至少一对节点必须因此有比较大的距离。 ↩ "},"numa-support/remote-access-costs.html":{"url":"numa-support/remote-access-costs.html","title":"5.4. 远端存取成本","keywords":"","body":"5.4. 远端存取成本 图 5.2：多节点的读／写效能 不过，距离是有关系的。AMD 在 [1] 提供了一台四槽机器的 NUMA 成本的文件。写入操作的数据显示在图 5.2。写入比读取还慢，这并不让人意外。有趣的部分在于 1 与 2 跳（1- and 2-hop）情况下的成本。两个 1 跳的成本实际上有略微不同。细节见 [1]。2 跳读取与写入（分别）比 0 跳读取慢了 30% 与 49%。2 跳写入比 0 跳写入慢了 32%、比 1 跳写入慢了 17%。处理器与memory节点的相对位置能够造成很大的差距。来自 AMD 下个世代的处理器将会以每个处理器四条连贯的超传输连结为特色。在这个例子中，一台四槽机器的直径会是一。但有八个插槽的话，同样的问题又 –– 来势汹汹地 –– 回来了，因为一个有著八个节点的超立方体的直径为三。 所有这些资讯都能够取得，但用起来很麻烦。在 6.5 节，我们会看到较容易存取与使用这个资讯的介面。 00400000 default file=/bin/cat mapped=3 N3=3 00504000 default file=/bin/cat anon=1 dirty=1 mapped=2 N3=2 00506000 default heap anon=3 dirty=3 active=0 N3=3 38a9000000 default file=/lib64/ld-2.4.so mapped=22 mapmax=47 N1=22 38a9119000 default file=/lib64/ld-2.4.so anon=1 dirty=1 N3=1 38a911a000 default file=/lib64/ld-2.4.so anon=1 dirty=1 N3=1 38a9200000 default file=/lib64/libc-2.4.so mapped=53 mapmax=52 N1=51 N2=2 38a933f000 default file=/lib64/libc-2.4.so 38a943f000 default file=/lib64/libc-2.4.so anon=1 dirty=1 mapped=3 mapmax=32 N1=2 N3=1 38a9443000 default file=/lib64/libc-2.4.so anon=1 dirty=1 N3=1 38a9444000 default anon=4 dirty=4 active=0 N3=4 2b2bbcdce000 default anon=1 dirty=1 N3=1 2b2bbcde4000 default anon=2 dirty=2 N3=2 2b2bbcde6000 default file=/usr/lib/locale/locale-archive mapped=11 mapmax=8 N0=11 7fffedcc7000 default stack anon=2 dirty=2 N3=2 图 5.3：/proc/PID/numa_maps 的内容 系统提供的最后一块资讯就在行程自身的状态中。能够确定memory映射档、写时复制（Copy-On-Write，COW）27分页与匿名memory（anonymous memory）是如何散布在系统中的节点上的。系统核心为每个处理器提供一个虚拟档（pseudo-file） /proc/PID/numa_maps，其中 PID 为行程的 ID ，如图 5.3 所示。档案中的重要资讯为 N0 到 N3 的值，它们表示为节点 0 到 3 上的memory区域分配的分页数量。一个可靠的猜测是，程序是执行在节点 3 的核上。程序本身与被弄脏的分页被分配在这个节点上。唯读映射，像是 ld-2.4.so 与 libc-2.4.so 的第一次映射、以及共享档案 locale-archive 是被分配在其它节点上的。 如同我们已经在图 5.2 看到的，当横跨节点操作时，1 与 2 跳读取的效能分别掉了 9% 与 30%。对执行来说，这种读取是必须的，而且若是没有命中 L2 cache的话，每个cache行都会招致这些额外成本。若是memory离处理器很远，对超过cache大小的大工作负载而言，所有量测的成本都必须提高 9%／30%。 图 5.4：在远端memory操作 为了看到在现实世界的影响，我们能够像 3.5.1 节一样测量频宽，但这次使用的是在远端、相距一跳的节点上的memory。这个测试相比于使用本地memory的数据的结果能在图 5.4 中看到。数字在两个方向都有一些大起伏，这是一个测量多执行绪程序的问题所致，能够忽略。在这张图上的重要资讯是，读取操作总是慢了 20%。这明显慢于图 5.2 中的 9%，这极有可能不是连续读／写操作的数字，而且可能与较旧的处理器修订版本有关。只有 AMD 知道了。 以塞得进cache的工作集大小而言，写入与复制操作的效能也慢了 20%。当工作集大小超过cache大小时，写入效能不再显著地慢于本地节点上的操作。互连的速度足以跟上memory的速度。主要因素是花费在等待主memory的时间。 27. 当一个memory分页起初有个使用者，然后必须被复制以允许独立的使用者时，写时复制是一种经常在操作系统实作用到的方法。在许多情境中，复制 –– 起初或完全 –– 是不必要的。在这种情况下，只在任何一个使用者修改memory的时候复制是合理的。操作系统拦截写入操作、复制memory分页、然后允许写入指令继续执行。 ↩ "},"what-programmers-can-do.html":{"url":"what-programmers-can-do.html","title":"6. 程序开发者能做些什么？","keywords":"","body":"6. 程序开发者能做些什么？ 在前面几节的描述之后，无疑地，程序开发者有非常非常多 –– 正向或者负向地 –– 影响程序效能的机会。而这里仅讨论与memory有关的操作。我们将会全面地解释这些部分，由最底层的物理 RAM 存取以及 L1 cache开始，一路涵盖到影响memory管理的操作系统功能。 "},"what-programmers-can-do/bypassing-the-cache.html":{"url":"what-programmers-can-do/bypassing-the-cache.html","title":"6.1. 绕过cache","keywords":"","body":"6.1. 绕过cache 当资料被产生、并且没有（立即）被再次使用时，memory储存操作会先读取完整的cache行然后修改cache资料，这点对效能是有害的。这个操作会将可能再次用到的资料踢出cache，以让给那些短期内不会再次被用到的资料。尤其是像矩阵 –– 它会先被填值、接著才被使用 –– 这类大资料结构。在填入矩阵的最后一个元素前，第一个元素就会因为矩阵太大被踢出cache，导致写入cache丧失效率。 对于这类情况，处理器提供对非暂存（non-temporal）写入操作的支援。这个情境下的非暂存指的是资料在短期内不会被使用，所以没有任何cache它的理由。这些非暂存的写入操作不会先读取cache行然后才修改它；反之，新的内容会被直接写进memory。 这听来代价高昂，但并不是非得如此。处理器会试著使用合并写入（见 3.3.3 节）来填入整个cache行。若是成功，那么memory读取操作是完全不必要的。如 x86 以及 x86-64 架构，gcc 提供若干 intrinsic 函式 译注： #include void _mm_stream_si32(int *p, int a); void _mm_stream_si128(int *p, __m128i a); void _mm_stream_pd(double *p, __m128d a); #include void _mm_stream_pi(__m64 *p, __m64 a); void _mm_stream_ps(float *p, __m128 a); #include void _mm_stream_sd(double *p, __m128d a); void _mm_stream_ss(float *p, __m128 a); 最有效率地使用这些指令的情况是一次处理大量资料。资料从memory载入、经过一或多步处理、而后写回memory。资料「流（stream）」经处理器，这些指令便得名于此。 memory位置必须各自对齐至 8 或 16 位元组。在使用多媒体扩充（multimedia extension）的程序码中，也可以用这些非暂存的版本替换一般的 _mm_store_* 指令。我们并没有在 A.1 节的矩阵相乘程序中这么做，因为写入的值会在短时间内被再次使用。这是串流指令无所助益的一个例子。6.2.1 节会更加深入这段程序码。 处理器的合并写入缓冲区可以将部分写入cache行的请求延迟一小段时间。一个接著一个执行所有修改单一cache行的指令，以令合并写入能真的发挥功用通常是必要的。以下是一个如何实践的例子： #include void setbytes(char *p, int c) { __m128i i = _mm_set_epi8(c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c); _mm_stream_si128((__m128i *)&p[0], i); _mm_stream_si128((__m128i *)&p[16], i); _mm_stream_si128((__m128i *)&p[32], i); _mm_stream_si128((__m128i *)&p[48], i); } 假设指标 p 被适当地对齐，呼叫这个函式会将指向的cache行中的所有位元组设为 c。合并写入逻辑会看到四个生成的 movntdq 指令，并仅在最后一个指令被执行之后，才对memory发出写入命令。总而言之，这段程序不仅避免在写入前读取cache行，也避免cache被并非立即需要的资料污染。这在某些情况下有著巨大的好处。一个经常使用这项技术的例子即是 C 函式库中的 memset 函式，它在处理大块memory时应该要使用类似于上述程序的作法。 某些架构提供专门的解法。PowerPC 架构定义 dcbz 指令，它能用以清除整个cache行。这个指令不会真的绕过cache，因为cache行仍会被分配来存放结果，但没有任何资料会从memory被读出来。这相比于非暂存储存指令更加受限，因为cache行只能全部被清空而污染cache（在资料为非暂存的情况），但其不需合并写入逻辑来达到这个结果。 为了一探非暂存指令的运作，我们将观察一个用以测量矩阵 –– 由一个二维阵列所组成 –– 写入效能的新测试。编译器将矩阵置放于memory中，以令最左边的（第一个）索引指向一列在memory中连续置放的所有元素。右边的（第二个）索引指向一列中的元素。测试程序以两种方式迭代矩阵：第一种是在内部回圈增加行号，第二种是在内部回圈增加列号。这代表其行为如图 6.1 所示。 图 6.1：矩阵存取模式 我们测量初始化一个 3000 × 3000 矩阵所花的时间。为了观察memory的表现，我们采用不会使用cache的储存指令。在 IA-32 处理器上，「非暂存提示（non-temporal hint）」即被用在于此。作为比较，我们也测量一般的储存操作。结果见于表 6.1。 内部回圈增加 列 行 一般 0.048s 0.127s 非暂存 0.048s 0.160s 表 6.1：矩阵初始化计时 对于使用cache的一般写入操作，我们观察到预期中的结果：若是memory被循序地使用，我们会得到比较好的结果，整个操作费 0.048s，相当于 750MB/s，几近于随机存取的情况却花 0.127s（大约 280MB/s）。这个矩阵已经大到令cache没那么有效。 我们感兴趣的部分主要是绕过cache的写入操作。可能令人吃惊的是，在这里循序存取跟使用cache的情况一样快。这个结果的原因是处理器执行上述的合并写入操作。此外，对于非暂存写入的memory排序（memory ordering）规则亦被放宽：程序需要明确地插入memory屏障（memory barriers）（如 x86 与 x86-64 处理器的 sfence 指令）。意味著处理器在写回资料时有著更多的自由，因此能尽可能地善用可用的频宽。 内部回圈以行向（column-wise）存取的情况就不同。无cache存取的结果明显地慢于cache存取（0.16s，约 225MB/s）。这里我们可以理解到，合并写入是不可能的，每个记忆单元都必须被独立处理。这需要不断地从 RAM 晶片上选取新的几列，附带著与此相应的延迟。结果是比有cache的情况还慢 25%。 在读取操作上，处理器 –– 直到最近 –– 除了非暂存存取（Non-Temporal Access，NTA）预取指令的弱提示之外，仍欠缺相应的支援。没有与合并写入对等的读取操作，这对诸如memory对映 I/O（memory-mapped I/O）这类无法被cache的memory尤其糟糕。Intel 附带 SSE4.1 扩充引入 NTA 载入。它们以一些串流载入缓冲区（streaming load buffer）实作；每个缓冲区包含一个cache行。针对一个cache行的第一个 movntdqa 指令会将cache行载入一个缓冲区 –– 可能会替换掉另一个cache行。随后，对同一个cache行、以 16 位元组对齐的存取操作将会由载入缓冲区以少量的成本来提供服务。除非有其它理由，cache行将不会被载入到cache中，于是便能够在不污染cache的情况下载入大量的memory。编译器为这个指令提供一个 intrinsic 函式： #include __m128i _mm_stream_load_si128 (__m128i *p); 这个 intrinsic 函式应该要以 16 位元组区块的地址做为参数执行多次，直到每个cache行都被读取为止。在这时才应该开始处理下一个cache行。由于只有少数几个串流读取缓冲区，可能要一次从两个memory位置进行读取。 我们应该从这个实验得到的是，现代的 CPU 非常巧妙地最佳化无cache写入 –– 近来甚至包括读取操作，只要它们是循序操作的。在处理只会被用到一次的大资料结构时，这个知识是非常有用的。再者，cache能够降低一些 –– 但不是全部 –– 随机memory存取的成本。在这个例子中，由于 RAM 存取的实作，导致随机存取慢 70%。在实作改变以前，无论何时都应该避免随机存取。 我们将会在谈论预取的章节再次一探非暂存旗标。 译注. intrinsic 函式可简称 intrinsics，由编译器提供，类似 inline 函式，但跟微处理器架构紧密相关，因为编译器知道如何运用最佳的方式来输出对应的微处理器指令。有些状况下，intrinsics 可能会呼叫标准函式库或执行环境的函式，甚至可能会有跨越处理器之间 intrinsics 的转换，例如译者维护的 SSE2NEON 专案。 ↩ "},"what-programmers-can-do/cache-access.html":{"url":"what-programmers-can-do/cache-access.html","title":"6.2. cache存取","keywords":"","body":"6.2. cache存取 希望改进他们程序效能的程序开发者会发现，最好聚焦在影响一阶cache的改变上，因为这很可能会产生最好的结果。我们将会在讨论延伸到其它层级之前先讨论它。显然地，所有针对一阶cache的最佳化也会影响其它cache。所有memory存取的主题都是相同的：改进局部性（空间与时间）并对齐程序码与资料。 "},"what-programmers-can-do/cache-access/optimizing-level-1-data-cache-access.html":{"url":"what-programmers-can-do/cache-access/optimizing-level-1-data-cache-access.html","title":"6.2.1. 最佳化一阶资料cache存取","keywords":"","body":"6.2.1. 最佳化一阶资料cache存取 在 3.3 节，我们已经看过 L1d cache的有效使用能够提升效能。在这一节，我们会展示什么样的程序码改变能够协助改进这个效能。延续前一节，我们首先聚焦在循序存取memory的最佳化。如同在 3.3 节中看到的数字，处理器在memory被循序存取的时候会自动预取资料。 使用的范例程序码为矩阵乘法。我们使用两个 1000×1000 1000 \\times 1000 1000×1000 double 元素的方阵（square matrices）。对于那些忘记数学的人，给定元素为 aij a_{ij} a​ij​​ 与 bij b_{ij} b​ij​​ 的矩阵 A A A 与 B B B，0≤i,jN 0 \\leq i,j 0≤i,jN，乘积为 (AB)ij=∑k=0N−1aikbkj=ai1b1j+ai2b2j+⋯+ai(N−1)b(N−1)j (AB)_{ij} = \\sum^{N - 1}_{k = 0} a_{ik} b_{kj} = a_{i1} b_{1j} + a_{i2} b_{2j} + \\cdots + a_{i(N - 1)} b_{(N - 1)j} (AB)​ij​​=​k=0​∑​N−1​​a​ik​​b​kj​​=a​i1​​b​1j​​+a​i2​​b​2j​​+⋯+a​i(N−1)​​b​(N−1)j​​ 一个直观的 C 实作看起来可能像这样 for (i = 0; i 两个输入矩阵为 mul1 与 mul2。假定结果矩阵 res 全被初始化为零。这是个既好又简单的实作。但应该很明显的是，我们有个正好是在图 6.1 解释过的问题。在 mul1 被循序存取的时候，内部的回圈增加 mul2 的列号。这表示 mul1 是像图 6.1 中左边的矩阵那样处理，而 mul2 是像右边的矩阵那样处理。这可能不太好。 有一个能够轻易尝试的可能补救方法。由于矩阵中的每个元素会被多次存取，是值得在使用第二个矩阵 mul2 之前将它重新排列（数学术语的话，「转置〔transpose〕」）的。 (AB)ij=∑k=0N−1aikbjkT=ai1bj1T+ai2bj2T+⋯+ai(N−1)bj(N−1)T (AB)_{ij} = \\sum^{N - 1}_{k = 0} a_{ik} b^{\\text{T}}_{jk} = a_{i1} b^{\\text{T}}_{j1} + a_{i2} b^{\\text{T}}_{j2} + \\cdots + a_{i(N - 1)} b^{\\text{T}}_{j(N - 1)} (AB)​ij​​=​k=0​∑​N−1​​a​ik​​b​jk​T​​=a​i1​​b​j1​T​​+a​i2​​b​j2​T​​+⋯+a​i(N−1)​​b​j(N−1)​T​​ 在转置之后（通常以上标「T」表示），我们现在循序地迭代两个矩阵。就 C 程序而言，现在看起来像这样： double tmp[N][N]; for (i = 0; i 我们建立一个容纳被转置的矩阵的暂时变数（temporary variable）。这需要动到额外的memory，但这个成本会被 –– 希望如此 –– 弥补回来，因为每行 1000 次非循序存取是更为昂贵的（至少在现代的硬件上）。是进行一些效能测试的时候。在有著 2666MHz 时脉的 Intel Core 2 上的结果为（以时钟周期为单位）： 原始 转置 周期数 16,765,297,870 3,922,373,010 相对值 100% 23.4% 虽然只是个简单的矩阵转置，但我们能达到 76.6% 的加速！复制操作的损失完全被弥补。1000 次非循序存取真的很伤。 下个问题是，我们是否能做得更好。无论如何，我们确实需要一个不需额外复制的替代方法。我们并不是总有馀裕能进行复制：矩阵可能太大、或者可用的memory太小。 替代实作的探寻应该从彻底地检验涉及到的数学与原始实作所执行的操作开始。简单的数学知识让我们能够发现，只要每个加数（addend）正好出现一次，对结果矩阵的每个元素执行的加法顺序是无关紧要的。28这个理解让我们能够寻找将执行在原始程序码内部回圈的加法重新排列的解法。 现在，让我们来检验在原始程序码执行中的实际问题。被存取的 mul2 元素的顺序为：(0,0) (0, 0) (0,0)、(1,0) (1, 0) (1,0)、 ... 、(N−1,0) (N - 1, 0) (N−1,0)、(0,1) (0,1) (0,1)、(1,1) (1, 1) (1,1)、 ...。元素 (0,0) (0, 0) (0,0) 与 (0,1) (0, 1) (0,1) 位于同一个cache行中，但在内部回圈完成一轮的时候，这个cache行早已被逐出。以这个例子而言，每一轮内部回圈都需要 –– 对三个矩阵的每一个而言 –– 1000 个cache行（Core 2 处理器为 64 位元组）。这加起来远比 L1d 可用的 32k 还多。 但若是我们在执行内部回圈的期间，一起处理中间回圈的两次迭代呢？在这个情况下，我们使用两个来自必定在 L1d 中的cache行的 double 值。我们将 L1d 错失率减半。译注这当然是个改进，但 –– 视cache行的大小而定 –– 也许仍不是我们能够得到的最好结果。Core 2 处理器有个cache行大小为 64 位元组的 L1d。实际的大小能够使用 sysconf (_SC_LEVEL1_DCACHE_LINESIZE) 在执行期查询、或是使用命令列（command line）的 getconf 工具程序（utility），以让程序能够针对特定的cache行大小编译。以 sizeof(double) 为 8 来说，这表示 –– 为了完全利用cache行 –– 我们应该展开内部回圈 8 次。继续这个想法，为了有效地使用 res 矩阵 –– 即，为了同时写入 8 个结果 –– 我们也该展开外部回圈 8 次。我们假设这里的cache行大小为 64，但这个程序码也能在 32 位元组cache行的系统上运作，因为cache行也会被 100% 利用。一般来说，最好在编译期像这样使用 getconf 工具程序来写死（hardcode）cache行大小： gcc -DCLS=$(getconf LEVEL1_DCACHE_LINESIZE) ... 若是二元档是假定为一般化（generic）的话，应该使用最大的cache行大小。使用非常小的 L1d 表示并非所有资料都能塞进cache，但这种处理器无论如何都不适合高效能程序。我们写出的程序码看起来像这样： #define SM (CLS / sizeof (double)) for (i = 0; i 这看起来超可怕的。在某种程度上它是如此，但只是因为它包含一些技巧。最显而易见的改变是，我们现在有六层巢状回圈。外部回圈以 SM（cache行大小除掉 sizeof(double)）为间隔迭代。这将乘法切成多个能够以更多cache局部性处理的较小的问题。内部回圈迭代外部回圈漏掉的索引。再一次，这里有三层回圈。这里唯一巧妙的部分是 k2 与 j2 回圈的顺序不同。这是因为在实际运算中，仅有一个表示式取决于 k2、但有两个取决于 j2。 这里其余的复杂之处来自 gcc 在最佳化阵列索引的时候并不是非常聪明的结果。额外变数 rres、rmul1、与 rmul2 的引入，借由将内部回圈的常用表示式（expression）尽可能地拉出来，以最佳化程序码。C 与 C++ 语言预设的别名规则（aliasing rule）并不能帮助编译器做出这些决定（除非使用 restrict，所有指标存取都是别名的潜在来源）。这即是为何对于数值程序设计而言，Fortran 仍是一个偏好语言的原因：它令快速程序的撰写更简单。29 原始 转置 子矩阵 向量化 周期数 16,765,297,870 3,922,373,010 2,895,041,480 1,588,711,750 相对值 100% 23.4% 17.3% 9.47% 表 6.2：矩阵乘法计时 所有努力所带来的成果能够在表 6.2 看到。借由避免复制，我们增加额外的 6.1% 效能。此外，我们不需要任何额外的memory。只要结果矩阵也能塞进memory，输入矩阵可以是任意大小的。这是我们现在已经达成的一个通用解法的一个必要条件。 在表 6.2 中还有一栏没有被解释过。大多现代处理器现今包含针对向量化（vectorization）的特殊支援。经常被标为多媒体扩充，这些特殊指令能够同时处理 2、4、8、或者更多值。这些经常是 SIMD（单指令多资料，Single Instruction, Multiple Data）操作，借由其它操作的协助，以便以正确的形式获取资料。由 Intel 处理器提供的 SSE2 指令能够在一个操作中处理两个 double 值。指令参考手册列出提供对这些 SSE2 指令存取的 intrinsic 函式。若是使用这些 intrinsic 函式，程序执行会变快 7.3%（相对于原始实作）。结果是，一支以原始程序码 10% 的时间执行的程序。翻译成人们认识的数字，我们从 318 MFLOPS 变为 3.35 GFLOPS。由于我们在这里仅对memory的影响有兴趣，程序的原始码被摆到 A.1 节。 应该注意的是，在最后一版的程序码中，我们仍然有一些 mul2 的cache问题；预取仍然无法运作。但这无法在不转置矩阵的情况下解决。或许cache预取单元将会变得聪明地足以识别这些模式，那时就不需要额外的更动。不过，以一个 2.66 GHz 处理器上的单执行绪程序而言，3.19 GFLOPS 并不差。 我们在矩阵乘法的例子中最佳化的是被载入的cache行的使用。一个cache行的所有位元组总是会被用到。我们只是确保在cache行被逐出前会用到它们。这当然是个特例。 更常见的是，拥有塞满一或多个cache行的资料结构，而程序在任何时间点都只会使用几个成员。我们已经在图 3.11 看过，大结构尺寸在只有一些成员被用到时的影响。 图 6.2：散布在多个cache行中 图 6.2 显示使用现在已熟知的程序执行另一组基准测试的结果。这次会加上同个串列元素的两个值。在一个案例中，两个元素都在同一个cache行内；在另一个案例中，一个元素位在串列元素的第一个cache行，而第二个位在最后一个cache行。这张图显示我们正遭受的效能衰减。 不出所料，在所有情况下，若是工作集塞得进 L1d 就不会有任何负面影响。一旦 L1d 不再充足，则是使用一个行程的两个cache行来偿付损失，而非一个。红线显示串列被循序地排列时的数据。我们看到寻常的两步模式：当 L2 cache充足时的大约 17% 的损失、以及当必须用到主memory时的大约 27% 的损失。 在随机memory存取的情况下，相对的数据看起来有点不同。对于塞得进 L2 的工作集而言的效能衰减介于 25% 到 35% 之间。再往后它下降到大约 10%。这不是因为损失变小，而是因为实际的memory存取不成比例地变得更昂贵。这份数据也显示，在某些情况下，元素之间的距离是很重要的。Random 4 CLs 的曲线显示较高的损失，因为用到第一个与第四个cache行。 要查看一个资料结构对比于cache行的布局，一个简单的方法是使用 pahole 程序（见 [4]）。这个程序检验定义在二进位档案中的资料结构。取一个包含这个定义的程序： struct foo { int a; long fill[7]; int b; }; 当在一台 64 位元机器上编译时，pahole 程序的输出（在其它东西之中）包含显示于图 6.3 的输出。这个输出结果告知我们很多东西。首先，它显示这个资料结构使用超过一个cache行。这个工具假设目前使用的处理器的cache行大小，但这个值能够使用一个命令列参数来覆写。尤其在结构大小几乎没有超过一个cache行、以及许多这种型别的物件会被分配的情况下，寻求一个压缩这种结构的方式是合理的。或许几个元素能有比较小的型别、又或者某些栏位实际上是能使用独立位元来表示的旗标。 struct foo { int a; /* 0 4 */ /* XXX 4 bytes hole, try to pack */ long int fill[7]; /* 8 56 */ /* --- cacheline 1 boundary (64 bytes) --- */ int b; /* 64 4 */ }; /* size: 72, cachelines: 2 */ /* sum members: 64, holes: 1, sum holes: 4 */ /* padding: 4 */ /* last cacheline: 8 bytes */ 图 6.3：pahole 执行的输出 在这个范例的情况中，压缩是很容易的，而且它也被这支程序所暗示。输出显示在第一个元素后面有个四位元的洞（hole）。这个洞是由结构的对齐需求以及 fill 元素所造成的。很容易发现元素 b –– 其大小为四位元组（由那行结尾的 4 所指出的）–– 完美地与这个间隔（gap）相符。在这个情况下的结果是，间隔不再存在，而这个资料结构塞得进一个cache行中。pahole 工具能自己完成这个最佳化。若是使用 --reorganize 参数，并将结构的名称加到命令列的结尾，这个工具的输出即是最佳化的结构、以及使用的cache行。除了移动栏位以填补间隔之外，这个工具也能够最佳化位元栏位以及合并填充（padding）与洞。更多细节见 [4]。 有个正好大得足以容纳尾端元素的洞当然是个理想的情况。为了让这个最佳化有用，物件本身也必须对齐cache行。我们马上就会开始处理这点。 pahole 输出也能够轻易看出元素是否必须被重新排列，以令那些一起用到的元素也会被储存在一起。使用 pahole 工具，很容易就能够确定哪些元素要在同个cache行，而不是必须在重新排列元素时才能达成。这并不是一个自动的过程，但这个工具能帮助很多。 各个结构元素的位置、以及它们被使用的方式也很重要。如同我们已经在 3.5.2 节看到的，晚到cache行的关键字组的程序效能是很糟的。这表示一位程序开发者应该总是遵循下列两条原则： 总是将最可能为关键字组的结构元素移到结构的开头。 存取资料结构、以及存取顺序不受情况所约束时，以它们定义在结构中的顺序来存取。 以小结构而言，这表示元素应该以它们可能被存取的顺序排列。这必须以灵活的方式处理，以允许其它像是补洞之类的最佳化也能被使用。对于较大的资料结构，每个cache行大小的区块应该遵循这些原则来排列。 不过，若是物件自身不若预期地对齐，就不值得花时间来重新排列它。一个物件的对齐，是由资料型别的对齐需求所决定的。每个基础型别有它自己的对齐需求。对于结构型别，它的任意元素中最大的对齐需求决定这个结构的对齐。这几乎总是小于cache行大小。这表示即使一个结构的成员被排列成塞得进同一个cache行，一个被分配的物件也可能不具有相符于cache行大小的对齐。有两种方法能确保物件拥有在设计结构布局时使用的对齐： 物件能够以明确的对齐需求分配。对于动态分配（dynamic allocation），呼叫 malloc 仅会以相符于最严格的标准型别（通常是 long double）的对齐来分配物件。不过，使用 posix_memalign 请求较高的对齐也是可能的。 #include int posix_memalign(void **memptr, size_t align, size_t size); 这个函式将一个指到新分配的memory的指标储存到由 memptr 指到的指标变数中。memory区块大小为 size 位元组，并在 align 位元组边界上对齐。 对于由编译器分配的物件（在 .data、.bss 等，以及在堆叠中），能够使用一个变数属性（attribute）： struct strtype variable __attribute((aligned(64))); 在这个情况下，不管 strtype 结构的对齐需求为何，variable 都会在 64 位元组边界上对齐。这对全域变数与自动变数也行得通。 对于阵列，这个方法并不如你可能预期的那般运作。只有阵列的第一个元素会被对齐，除非每个元素的大小是对齐值的倍数。这也代表每个单一变数都必须被适当地标注。posix_memalign 的使用也不是完全不受控制的，因为对齐需求通常会导致碎片与／或更高的memory消耗。 一个使用者定义型别的对齐需求能够使用一个型别属性来改变： struct strtype { ...members... } __attribute((aligned(64))); 这会使编译器以合适的对齐来分配所有的物件，包含阵列。不过，程序开发者必须留意针对动态分配物件的合适对齐的请求。这里必须再一次使用 posix_memalign。使用 gcc 提供的 alignof 运算子（operator）、并将这个值作为第二个参数传递给 posix_memalign 是很简单的。 之前在这一节提及的多媒体扩充几乎总是需要对齐memory存取。即，对于 16 位元组的memory存取而言，地址是被假定以 16 位元组对齐的。x86 与 x86-64 处理器拥有能够处理非对齐存取的memory操作的特殊变体，但这些操作比较慢。对于所有memory存取都需要完全对齐的大多 RISC 架构而言，这种严格的对齐需求并不新奇。即使一个架构支援非对齐的存取，这有时也比使用合适的对齐还慢，尤其是在不对齐导致一次载入或储存使用两个cache行、而非一个的情况下。 图 6.4：非对齐存取的间接成本 图 6.4 显示非对齐memory存取的影响。现已熟悉的测试会在（循序或随机）走访memory被量测的期间递增一个资料元素，一次使用对齐的串列元素、一次使用刻意不对齐的元素。图表显示程序因非对齐存取而招致的效能衰减。循序存取情况下的影响比起随机的情况更为显著，因为在后者的情况下，非对齐存取会部分地被一般来说较高的memory存取成本所隐藏。在循序的情况下，对于塞得进 L2 cache的工作集大小来说，效能衰减大约是 300%。这能够由 L1 cache的有效性降低来解释。某些递增操作现在会碰到两个cache行，而且现在在一个串列元素上操作经常需要两次cache行的读取。L1 与 L2 之间的连接简直太壅塞。 对于非常大的工作集大小，非对齐存取的影响仍然是 20% 至 30% –– 考虑到对于这种大小的对齐存取时间很长，这是非常多的。这张图表应该显示对齐是必须被严加对待的。即使架构支援非对齐存取，也绝对不要认为「它们跟对齐存取一样好」。 不过，有一些来自这些对齐需求的附带结果。若是一个自动变数拥有一个对齐需求，编译器必须确保它在所有情况下都能够被满足。这并不容易，因为编译器无法控制呼叫点（call site）与它们处理堆叠的方式。这个问题能够以两种方式处理： 产生的程序主动地对齐堆叠，必要时插入间隔。这需要程序检查对齐、建立对齐、并在之后还原对齐。 要求所有的呼叫端都将堆叠对齐。 所有常用的应用程序二进位介面（application binary interface，ABI）都遵循第二条路。如果一个呼叫端违反规则、并且对齐为被呼叫端所需，程序很可能会失去作用。不过，对齐的完美保持并不会平白得来。 在一个函式中使用的一个堆叠框（frame）的大小不必是对齐的倍数。这表示，若是从这个堆叠框呼叫其它函式，填充就是必要的。很大的不同是，在大部分情况下，堆叠框的大小对编译器而言是已知的，因此它知道如何调整堆叠指标，以确保任何从这个堆叠框呼叫的函式的对齐。事实上，大多编译器会直接将堆叠框的大小调高，并以它来完成操作。 如果使用可变长度阵列（variable length array，VLA）或 alloca，这种简单的对齐处理方式就不合适。在这种情况下，堆叠框的总大小只会在执行期得知。在这种情况下可能会需要主动的对齐控制，使得产生的程序码（略微地）变慢。 在某些架构上，只有多媒体扩充需要严格的对齐；在那些架构上的堆叠总是当作普通的资料型别进行最低限度的对齐，对于 32 与 64 位元架构通常分别是 4 或 8 位元组。在这些系统上，强制对齐会招致不必要的成本。这表示，在这种情况下，我们可能会想要摆脱严格的对齐需求，如果我们知道不会依赖它的话。不进行多媒体操作的尾端函式（tail function）（那些不呼叫其它函式的函式）不必对齐。只呼叫不需对齐的函式的函式也不用。若是能够识别出够大一组函式，一支程序可能会想要放宽对齐需求。对于 x86 的二元档，gcc 拥有宽松堆叠对齐需求的支援： -mpreferred-stack-boundary=2 若是这个选项（option）的值为 N N N，堆叠对齐需求将会被设为 2N 2^{N} 2​N​​ 位元组。所以，若是使用 2 为值，堆叠对齐需求就被从预设值（为 16 位元组）降低成只有 4 位元组。在大多情况下，这表示不需额外的对齐操作，因为普通的堆叠推入（push）与弹出（pop）操作无论如何都是在四位元组边界上操作的。这个机器特定的选项能够帮忙减少程序大小，也能够提升执行速度。但它无法被套用到许多其它的架构上。即使对于 x86-64，一般来说也不适用，因为 x86-64 ABI 要求在 SSE 暂存器中传递浮点数参数，而 SSE 指令需要完整的 16 位元组对齐。然而，只要能够使用这个选项，就能造成明显的差别。 结构元素的高效摆放与对齐并非资料结构影响cache效率的唯一面向。若是使用一个结构的阵列，整个结构的定义都会影响效能。回想一下图 3.11 的结果：在这个情况中，我们增加阵列元素中未使用的资料总量。结果是预取越来越没效果，而程序 –– 对于大资料集 –– 变得越来越没效率。 对于大工作集，尽可能地使用可用的cache是很重要的。为了达到如此，可能有必要重新排列资料结构。虽然对程序开发者而言，将所有概念上属于一块儿的资料摆在同个资料结构是比较简单的，但这可能不是最大化效能的最好方法。假设我们有个如下的资料结构： struct order { double price; bool paid; const char *buyer[5]; long buyer_id; }; 进一步假设这些纪录会被存在一个大阵列中，并且有个经常执行的工作（job）会加总所有帐单的预期付款。在这种情境中，buyer 与 buyer_id 使用的memory是不必被载入到cache中的。根据图 3.11 的资料来判断，程序将会表现得比它能达到的还糟了高达五倍。 将 order 切成两块，前两个栏位储存在一个结构中，而另一个栏位储存在别处要好得多。这个改变无疑提高程序的复杂度，但效能提升证明这个成本的正当性。 最后，让我们考虑一下另一个 –– 虽然也会被应用在其它cache上 –– 主要是影响 L1d 存取的cache使用的最佳化。如同在图 3.8 看到的，增加的cache关联度有利于一般的操作。cache越大，关联度通常也越高。L1d cache太大，以致于无法为全关联式，但又没有足够大到要拥有跟 L2 cache一样的关联度。若是工作集中的许多物件属于相同的cache集，这可能会是个问题。如果这导致由于过于使用一组集合而造成逐出，即使大多cache都没被用到，程序还是可能会受到延迟。这些cache错失有时被称为冲突性错失（conflict miss）。由于 L1d 定址使用虚拟地址，这实际上是能够受程序开发者控制的。如果一起被用到的变数也储存在一块儿，它们属于相同集合的可能性是被最小化的。图 6.5 显示多快就会碰上这个问题。 图 6.5：cache关联度影响 在这张图中，现在熟悉的、使用 NPAD=15 的 Follow30 测试是以特殊的配置来量测的。X 轴是两个串列元素之间的距离，以空串列元素为单位量测。换句话说，距离为 2 代表下一个元素的地址是在前一个元素的 128 位元组之后。所有元素都以相同的距离在虚拟memory空间中摆放。Y 轴显示串列的总长度。仅会使用 1 至 16 个元素，代表工作集总大小为 64 至 1024 位元组。Z 轴显示寻访每个串列元素所需的平均周期数。 图中显示的结果应该不让人吃惊。若是被用到的元素很少，所有的资料都塞得进 L1d，而每个串列元素的存取时间仅有 3 个周期。对于几乎所有串列元素的安排都是如此：虚拟地址以几乎没有冲突的方式，被良好地映射到 L1d 的槽（slot）中。（在这张图中）有两个情况不同的特殊距离值。若是距离为 4096 位元组（即，64 个元素的距离）的倍数、并且串列的长度大于八，每个串列元素的平均周期数便大幅地增加。在这些情况下，所有项目都在相同的集合中，并且 –– 一旦串列长度大于关联度 –– 项目会从 L1d 被冲出，而下一轮必须从 L2 重新读取。这造成每个串列元素大约 10 个周期的成本。 使用这张图，我们能够确定使用的处理器拥有一个关联度 8、且总大小为 32kB 的 L1d cache。这表示，这个测试能够 –– 必要的话 –– 用以确定这些值。可以为 L2 cache量测相同的影响，但在这里更为复杂，因为 L2 cache是使用实体地址来索引的，而且它要大得多。 但愿程序开发者将这个数据视为值得关注集合关联度的一种暗示。将资料摆放在二的幂次的边界上足够常见于现实世界中，但这正好是容易导致上述影响与效能下降的情况。非对齐存取可能会提高冲突性错失的可能性，因为每次存取都可能需要额外的cache行。 图 6.6：AMD 上 L1d 的 Bank 地址 如果执行这种最佳化，另一个相关的最佳化也是可能的。AMD 的处理器 –– 至少 –– 将 L1d 实作为多个独立的 bank。只有当两个资料字组储存在不同的 bank 中或储存在同一索引（index）下相同的 bank 中，L1d cache才能在每一个周期里拿到两个字组。bank 地址是以虚拟地址的低位元编码的，如图 6.6 所示。假若会共同使用的变数也储存在一起，则它们也会有高可能性在不同的 bank 中或在同一索引下相同的 bank 中。 28. 我们这里忽略可能会改变上溢位（overflow）、下溢位（underflow）、或是四舍五入（rounding）的发生的算术影响。 ↩ 译注. 原文说法较简略，作者的意思是：在一开始三层回圈的实作中，最内部的每一次 k 回圈迭代同时处理 res[i][j] += mul1[i][k] * mul2[k][j] 与 res[i][j + 1] += mul1[i][k] * mul2[k][j + 1]。由于才刚存取过 mul2[k][j] 与 res[i][j]，所以 mul2[k][j + 1] 与 res[i][j + 1] 还在 L1d cache中，因而降低错失率。后述的方法是这个方法的一般化（generalization）。 ↩ 29. 理论上在 1999 年修订版引入 C 语言的 restrict 关键字应该解决这个问题。不过编译器还是不理解。原因主要是存在著太多不正确的程序码，其会误导编译器、并导致它产生不正确的目的码（object code）。 ↩ 30. 测试是在一台 32 位元机器上执行的，因此 NPAD=15 代表每个串列元素一个 64 位元组cache行。 ↩ "},"what-programmers-can-do/cache-access/optimizing-level-1-instruction-cache-access.html":{"url":"what-programmers-can-do/cache-access/optimizing-level-1-instruction-cache-access.html","title":"6.2.2. 最佳化一阶指令cache存取","keywords":"","body":"6.2.2. 最佳化一阶指令cache存取 准备有效使用 L1i 的程序码需要与有效使用 L1d 类似的技术。不过，问题是，程序开发者通常不会直接影响 L1i 的使用方式，除非他以组合语言来撰写程序。若是使用编译器，程序开发者能够透过引导编译器建立更好的程序布局，来间接地决定 L1i 的使用。 程序有跳跃（jump）之间为线性的优点。在这些期间，处理器能够有效地预取memory。跳跃打破这个美好的想像，因为 跳跃目标（target）可能不是静态决定的； 而且即使它是静态的，若是它错失所有cache，memory获取可能会花上很长一段时间。 这些问题造成执行中的停顿，可能严重地影响效能。这即是为何现今的处理器在分支预测（branch prediction，BP）上费尽心思的原因。高度特制化的 BP 单元试著尽可能远在跳跃之前确定跳跃的目标，使得处理器能够开始将新的位置的指令载入到cache中。它们使用静态与动态规则、而且越来越擅于判定执行中的模式。 对指令cache而言，尽早将资料拿到cache甚至是更为重要的。如同在 3.1 节提过的，指令必须在它们被执行之前解码，而且 –– 为了加速（在 x86 与 x86-64 上很重要）–– 指令实际上是以被解码的形式、而非从memory读取的位元组／字组的形式被cache的。 为了达到最好的 L1i 使用，程序开发者至少应该留意下述的程序码产生的面向： 尽可能地减少程序码量（code footprint）。这必须与像是回圈展开（loop unrolling）与行内展开（inlining）等最佳化取得平衡。 程序执行应该是没有气泡（bubble）的线性的。31 合理的情况下，对齐程序码。 我们现在要看一些根据这些面向、可用于协助最佳化程序的编译器技术。 编译器有启动不同最佳化层级的选项，特定的最佳化也能够个别地启用。在高最佳化层级（gcc 的 -O2 与 -O3）启用的许多最佳化处理回圈最佳化与函式行内展开。一般来说，这些是不错的最佳化。如果以这些方式最佳化的程序码占了程序总执行时间的很重要的一部分，便能够提升整体的效能。尤其是，函式的行内展开允许编译器一次最佳化更大的程序码块（chunk），从而能够产生更好地利用处理器的管线架构的机器码。当程序较大的一部分能被视为一个单一单元时，程序码与资料的处理（透过死码消除〔dead code elimination〕或值域传播〔value range propagation〕、等等）的效果更好。 较大的程序大小意味著 L1i（以及 L2 与更高阶层）cache上的压力更大。这可能导致较差的效能。较小的程序可能比较快。幸运的是，gcc 有一个针对于此的最佳化选项。如果使用 -Os，编译器将会为程序大小最佳化。已知会增加程序大小的最佳化会被关掉。使用这个选项经常产生惊人的结果。尤其在编译器无法真的获益于回圈展开与行内展开的情况下，这个选项就是首选。 行内展开也能被个别处理。编译器拥有引导行内展开的启发法（heuristic）与限制；这些限制能够由程序开发者控制。-finlinelimit 选项指定对行内展开而言，必须被视为过大的函式有多大。若是一个函式在多处被呼叫，在所有函式中行内展开它便会导致程序大小的剧增。但还有更多细节。假设一个函式 inlcand 在两个函式 f1 与 f2 中被呼叫。函式 f1 与 f2 本身是先后被呼叫的。 start f1 code f1 inlined inlcand more code f1 end f1 start f2 code f2 inlined inlcand more code f2 end f2 start inlcand code inlcand end inlcand start f1 code f1 end f1 start f2 code f2 end f2 表 6.3：行内展开 Vs 没有行内展开 表 6.3 显示在两个函式中没有行内展开与行内展开的情况下，产生的程序码看起来会怎么样。若是函式 inlcand 在 f1 与 f2 中被行内展开，产生的程序码的大小为 size f1 + size f2 + 2× 2 \\times 2× size inlcand。如果没有进行行内展开的话，总大小减少 size inlcand。这即是在 f1 与 f2 相互在不久后呼叫的话，L1i 与 L2 cache额外所需的量。再加上：若是 inlcand 没被行内展开，程序码可能仍在 L1i 中，而它就不必被再次解码。再加上：分支预测单元或许能更好地预测跳跃，因为它已经看过这段程序。如果对程序而言，被行内展开的函式大小上限的编译器预设值并不是最好的，它应该要被降低。 不过，有些行内展开总是合理的情况。假如一个函式只会被呼叫一次，它也应该被行内展开。这给编译器执行更多最佳化的机会（像是值域传播，其会显著地改进程序码）。行内展开也许会受选择限制所阻碍。对于像这样的情况，gcc 有个选项来明确指定一个函式总是要被行内展开。加上 always_inline 函式属性会命令编译器执行恰如这个名称所指示的操作。 在相同的情境下，若是即便一个函式足够小也不该被行内展开，能够使用 noinline 函式属性。假如它们经常从多处被呼叫，即使对于小函式，使用这个属性也是合理的。若是 L1i 内容能被重复使用、并且整体的程序码量减少，这往往弥补额外函式呼叫的附加成本。如今分支预测单元是非常可靠的。若是行内展开能够促成更进一步的最佳化，情况就不同。这是必须视情况来决定的。 如果行内展开的程序码总是会被用到的话，always_inline 属性表现得很好。但假如不是这样呢？如果偶尔才会呼叫被行内展开的函式会怎么样： void fct(void) { ... code block A ... if (condition) inlfct() ... code block C ... 为这种程序序列产生的程序码一般来说与原始码的结构相符。这表示首先会是程序区块 A、接著是一个条件式跳跃 –– 假如条件式被求值为否（false），就往前跳跃。接下来是为行内展开的 inlfct 产生的程序码，最后是程序区块 C。这看起来全都很合理，但它有个问题。 若是 condition 经常为否，执行就不是线性的。中间有一大块没用到的程序码，不仅因为预取污染 L1i，它也会造成分支预测的问题。若是分支预测错，条件表示式可能非常没有效率。 这是个普遍的问题，而且并不专属于函式的行内展开。无论在何时用到条件执行、而且它是不对称的（即，表示式比起某一种结果还要更常产生另一种结果），就有不正确的静态分支预测、从而有管线中的气泡的可能性。这能够借由告知编译器，以将较不常执行的程序码移出主要的程序路径来避免。在这种情况下，为一个 if 叙述产生的条件分支将会跳跃到一个跳脱顺序的地方，如下图所示。 上半部表示单纯的程序布局。假如区域 B –– 即，由上面被行内展开的函式 inlfct 所产生的 –– 因为条件 I 跳过它而经常不被执行，处理器的预取会拉进包含鲜少用到的区块 B 的cache行。这能够借由区块的重新排列来改变，其结果能够在图的下半部看到。经常执行的程序码在memory中是线性的，而鲜少执行的程序码被移到不伤及预取与 L1i 效率的某处。 gcc 提供两个实现这点的方法。首先，编译器能够在重新编译程序码的期间将效能分析（profiling）的输出纳入考量，并根据效能分析摆放程序区块。我们将会在第七节看到这是如何运作的。第二个方法则是借由明确的分支预测。gcc 认得 __builtin_expect： long __builtin_expect(long EXP, long C); 这个结构告诉编译器，表示式 EXP 的值非常有可能会是 C。回传值为 EXP。__builtin_expect 必须被用在条件表示式中。在几乎所有的情况中，它会被用在布林表示式的情境中，在这种情况下定义两个辅助巨集（macro）要更方便一些： #define unlikely(expr) __builtin_expect(!!(expr), 0) #define likely(expr) __builtin_expect(!!(expr), 1) 然后可以像这样用这些巨集 if (likely(a > 1)) 若是程序开发者使用这些巨集、然后使用 -freorder-blocks 最佳化选项，gcc 会如上图那样重新排列区块。这个选项会随著 -O2 启用，但对于 -Os 会被停用。有另一个重新排列区块的 gcc 选项（-freorder-blocks-and-partition），但它的用途有限，因为它不适用于例外处理。 还有另一个小回圈的大优点，至少在某些处理器上。Intel Core 2 前端有一个特殊的功能，称作回圈指令流检测器（Loop Stream Detector，LSD）。若是一个回圈拥有不多于 18 条指令（没有一个是对子程序〔routine〕的呼叫）、仅要求至多 4 次 16 位元组的解码器撷取、拥有至多 4 条分支指令、并且被执行超过 64 次，那么这个回圈有时会被锁在指令伫列中，因而在回圈被再次用到的时候能够更为快速地使用。举例来说，这适用于会通过一个外部回圈进入很多次的很小的内部回圈。即使没有这种特化的硬件，小巧的回圈也有优点。 就 L1i 而言，行内展开并非最佳化的唯一面向。另一个面向是对齐，就如资料一般。但有些明显的差异：程序大部分是线性的一团，其无法任意地摆在定址空间中，而且它无法直接受程序开发者影响，因为是编译器产生这些程序的。不过，有些程序开发者能够控制的面向。 对齐每条单一指令没有任何意义。目标是令指令流为连续的。所以对齐仅在战略要地上才有意义。为了决定要在何处加上对齐，理解能有什么好处是必要的。有条在一个cache行开头的指令32代表cache行的预取是最大化的。对指令而言，这也代表著解码器是更有效的。很容易看出，若是执行一条在cache行结尾的指令，处理器就必须准备读取一个新的cache行、并对指令解码。有些事情可能会出错（像是cache行错失），代表平均而言，一条在cache行结尾的指令执行起来并不跟在开头的指令一样有效。 如果控制权刚转移到在cache行结尾的指令（因此预取无效），则情况最为严重。将以上推论结合后，我们得出对齐程序码最有用的地方： 在函式的开头； 在仅会通过跳跃到达的基础区块的开头； 对某些扩充而言，在回圈的开头。 在前两种情况下，对齐的成本很小。在一个新的位置继续执行，假如决定让它在cache行的开头，我们便最佳化预取与解码。33编译器借由无操作（no-op）指令的插入，填满因对齐程序产生的间隔，而实现这种对齐。这种「死码（dead code）」占用一些空间，但通常不伤及效能。 第三种情况略有不同：对齐每个回圈的开头可能会造成效能问题。问题在于，一个回圈的开头往往是连续地接在其它的程序码之后。若是情况不是非常凑巧，便会有个在前一条指令与被对齐的回圈开头之间的间隔。不像前两种情况，这个间隔无法完全不造成影响。在前一条指令执行之后，必须执行回圈中的第一条指令。这表示，在前一条指令之后，要不是非得有若干条无操作指令以填补间隔、要不就是非得有个到回圈开头的无条件跳跃。两种可能性都不是免费的。特别是回圈本身并不常被执行的话，无操作指令或是跳跃的开销可能会比对齐回圈所省下的还多。 有三种程序开发者能够影响程序对齐的方法。显然地，若是程序是以组合语言撰写，其中的函式与所有的指令都能够被明确地对齐。组合语言为所有架构提供 .align 假指令（pseudo-op）以做到这点。对高阶语言而言，必须将对齐需求告知编译器。不像资料型别与变数那样，这在原始码中是不可能的。而是要使用一个编译器选项： -falign-functions=N 这个选项命令编译器将所有函式对齐到下一个大于 N 的二的幂次的边界。这表示会产生一个至多 N 位元组的间隔。对小函式而言，使用一个很大的 N 值是个浪费。对只有难得才会执行的程序也相同。在可能同时包含常用与没那么常用的介面的函式库中，后者可能经常发生。选项值的明智选择可以借由避免对齐来让工作加速或是节省memory。能够借由使用 1 作为 N 的值、或是使用 -fno-align-functions 选项来关掉所有的对齐。 有关前述的第二种情况的对齐 –– 无法循序达到的基础区块的开头 –– 能够使用一个不同的选项来控制： -falign-jumps=N 所有其它的细节都相同，关于浪费memory的警告也同样适用。 第三种情况也有它自己的选项： -falign-loops=N 再一次，同样的细节与警告都适用。除了在这里，如同先前解释过的，对齐会造成执行期的成本，因为在对齐的地址会被循序地抵达的情况下，要不是必须执行无操作指令就是必须执行跳跃指令。 gcc 还知道一个用来控制对齐的选项，在这里提起它仅是为了完整起见。-falign-labels 对齐了程序中的每个单一标籤（label）（基本上是每个基础区块的开头）。除了一些例外状况之外，这都会让程序变慢，因而不该被使用。 31. 气泡生动地描述在一个处理器的管线中执行的空洞，其会在执行必须等待资源的时候发生。关于更多细节，请读者参阅处理器设计的文献。 ↩ 32. 对某些处理器而言，cache行并非指令的最小区块（atomic block）。Intel Core 2 前端会将 16 位元组区块发给解码器。它们会被适当的对齐，因此没有任何被发出的区块能横跨cache行边界。对齐到cache行的开头仍有优点，因为它最佳化预取的正面影响。 ↩ 33. 对于指令解码，处理器往往会使用比cache行还小的单元，在 x86 与 x86-64 的情况中为 16 位元组。 ↩ "},"what-programmers-can-do/cache-access/optimizing-level-2-and-higher-cache-access.html":{"url":"what-programmers-can-do/cache-access/optimizing-level-2-and-higher-cache-access.html","title":"6.2.3. 最佳化二阶与更高阶cache存取","keywords":"","body":"6.2.3. 最佳化二阶与更高阶cache存取 关于一阶cache的最佳化所说的一切也适用于二阶与更高阶cache存取。有两个最后一阶cache的额外面向： cache错失一直都非常昂贵。L1 错失（希望）频繁地命中 L2 与更高阶cache，于是限制其损失，但最后一阶cache显然没有后盾。 L2 cache与更高阶cache经常由多颗处理器核与／或 HT 所共享。每个执行单元可用的有效cache大小因而经常小于总cache大小。 为了避免cache错失的高成本，工作集大小应该配合cache大小。若是资料只需要一次，这显然不是必要的，因为cache无论如何都没有效果。我们要讨论的是被需要不只一次的资料集的工作负载。在这种情况下，使用一个太大而不能塞得进cache的工作集将会产生大量的cache错失，即使预取成功地执行，也会拖慢程序。 即使资料集太大，一支程序也必须完成它的职责。以最小化cache错失的方式完成工作是程序开发者的职责。对于最后一阶cache，是可能 –– 如同 L1 cache –– 以较小的部分来执行工作的。这与表 6.2 最佳化的矩阵乘法非常雷同。不过，有一点不同在于，对于最后一阶cache，要处理的资料区块可能比较大。如果也需要 L1 最佳化，程序会变得更加复杂。想像一个矩阵乘法，其资料集 –– 两个输入矩阵与输出矩阵 –– 无法同时塞进最后一阶cache。在这种情况下，或许适合同时最佳化 L1 与最后一阶cache存取。 众多处理器世代中的 L1 cache行大小经常是固定的；即使不同，差异也很小。假设为较大的大小是没什么大问题的。在有著较小cache大小的处理器中，会用到两个或更多cache行、而非一个。在任何情况下，写死cache行大小、并为此最佳化程序都是合理的。 对于较高层级的cache，若程序是假定为一般化的话，就不是这样。那些cache的大小可能有很大的差异。八倍或更多倍并不罕见。将较大的cache大小假定为预设大小是不可能的，因为这可能表示，除了那些有著最大cache的机器之外，程序在所有机器上都会表现得很差。相反的选择也很糟：假定为最小的cache，代表浪费掉 87% 或者更多的cache。这很糟；如同我们能从图 3.14 看到的，使用大cache对程序的速度有著巨大的影响。 这表示程序必须动态地将自身调整为cache行大小。这是一种程序特有的最佳化。我们这里能说的是，程序开发者应该正确地计算程序的需求。不仅资料集本身需要，更高层级的cache也会被用于其它目的；举例来说，所有执行的指令都是从cache载入的。若是使用函式库里头的函式，这种cache的使用可能会加总为一个可观的量。那些函式库函式也可能需要它们自己的资料，进一步减少可用的memory。 一旦我们有一个memory需求的公式，我们就能够将它与cache大小作比较。如同先前所述，cache可能会被许多其它处理器核所共享。目前34，在没有写死知识的情况下，取得正确资讯的唯一方法是透过 /sys 档案系统。在表 5.2，我们已经看过系统核心发布的有关于硬件的资讯。程序必须在目录： /sys/devices/system/cpu/cpu*/cache 找到最后一阶cache。这能够由在这个目录里的层级档案中的最高数值来辨别出来。当目录被识别出来时，程序应该读取在这个目录中的 size 档案的内容，并将数值除以 shared_cpu_map 档案中的位元遮罩中设置的数字。 以这种方式计算的值是个安全的下限。有时一支程序会知道多一些有关其它执行绪或行程的行为。若是那些执行绪被排程在共享这个cache的处理器核或 HT 上、并且已知cache的使用不会耗尽它在总cache大小中所占的那份，那么计算出的限制可能会太小，而不是最佳的。是否要比公平共享应该使用的还多，真的要视情况而定。程序开发者必须做出抉择，或者必须让使用者做个决定。 34. 当然很快就会有更好的方法！ ↩ "},"what-programmers-can-do/cache-access/optimizing-tlb-usage.html":{"url":"what-programmers-can-do/cache-access/optimizing-tlb-usage.html","title":"6.2.4. 最佳化 TLB 使用","keywords":"","body":"6.2.4. 最佳化 TLB 使用 有两种 TLB 使用的最佳化。第一种最佳化是减少一支程序必须使用的分页数。这会自动导致较少的 TLB 错失。第二种最佳化是借由减少必须被分配的较高层目录表的数量，以令 TLB 查询便宜一些。较少的目录表代表使用的memory较少，这可能使得目录查询有较高的cache命中率。 第一种最佳化与分页错误的最小化密切相关。我们将会在 7.5 节仔细地涵盖这个主题。分页错误经常是个一次性的成本，但由于 TLB cache通常很小而且会被频繁地冲出，因此 TLB 错失是个长期的损失。分页错误比起 TLB 错失还贵了数个数量级，但若是一支程序跑得足够久、而且程序的某些部分会被足够频繁地执行，TLB 错失甚至可能超过分页错误的成本。因此重要的是，不仅要从分页错误的角度、也要从 TLB 错失的角度来考虑分页最佳化。差异在于，分页错误的最佳化只要求分页范围内的程序码与资料分组，而 TLB 最佳化则要求 –– 在任何时间点 –– 尽可能少的 TLB 项目。 第二种 TLB 最佳化甚至更难控制。必须使用的分页目录数量是视行程的虚拟定址空间中使用的地址范围分布而定的。定址空间中广泛多样的位置代表著更多的目录。 一个难题是，定址空间布局随机化（Address Space Layout Randomization，ASLR）恰好造成这种状况。堆叠、DSO、堆积、与可能的可执行档的载入地址会在执行期随机化，以防止机器的攻击者猜出函式或变数的地址。 只有在最大效能至关重要的情况下，才应该关掉 ASLR。额外目录的成本低到足以令这步是不必要的，除了一些极端的状况之外。系统核心能随时执行的一个可能的最佳化是，确保一个单一的映射不会横跨两个目录之间的memory空间边界。这会以最小的方式限制 ASLR，但不足以大幅地削弱它。 程序开发者直接受此影响的唯一方式是在明确请求一个定址空间区域的时候。这会在以 MAP_FIXED 使用 mmap 的时候发生。以这种方式分配定址空间区域非常危险，人们几乎不会这么做。如果程序开发者使用上述方法且允许自由选取地址，则他们应该要知道最后一阶分页目录的边界，及适当挑选所请求的地址。 "},"what-programmers-can-do/prefetching.html":{"url":"what-programmers-can-do/prefetching.html","title":"6.3. 预取","keywords":"","body":"6.3. 预取 预取的目的是隐藏memory存取的等待时间。现今处理器的命令管道与乱序（out-of-order，简称 OoO）执行的功能能够隐藏一些等待时间，但最多也只是对命中cache的存取而言。要掩盖主memory存取的等待时间，命令伫列可能得要非常地长。某些没有 OoO 的处理器试著借由提高核的数量来补偿，但除非所有使用的程序码都被平行化，否则这是个不太好的交易。 预取能进一步帮助隐藏等待时间。处理器靠它自己执行预取，由某些事件触发（硬件预取）或是由程序明确地请求（软件预取）。 "},"what-programmers-can-do/prefetching/hardware-prefetching.html":{"url":"what-programmers-can-do/prefetching/hardware-prefetching.html","title":"6.3.1. 硬件预取","keywords":"","body":"6.3.1. 硬件预取 CPU 启动硬件预取的触发，通常是二或多个cache错失的某种模式的序列。这些cache错失可能在cache行之前或之后。在旧的实作中，只有邻近cache行的cache错失会被识别出来。使用当代硬件，步伐也会被识别出来，代表跳过固定数量的cache行会被识别为一种模式并被适当地处理。 若每次单一的cache错失都会触发一次硬件预取，对于效能来说大概很糟。随机memory存取模式 –– 例如存取全域变数 –– 是非常常见的，而产生的预取会大大地浪费 FSB 频宽。这即是为何启动预取需要至少两次cache错失。处理器现今全都预期有多于一条memory存取的串流。处理器试著自动将每个cache错失指派给这样的一条串流，并且在达到门槛时启动硬件预取。CPU 现今能追踪更高阶cache的八到十六条单独的串流。 负责模式识别的单元与各自的cache相关联。可以有一个 L1d 与 L1i cache的预取单元。很可能有一个 L2 与更高阶cache的预取单元。L2 与更高阶cache的预取单元是被所有使用相同cache的其它处理器核与 HT 所共享。八到十六条单独串流预取单元的数量便因而迅速减少。 预取有个大弱点：它无法跨越分页边界。理解到 CPU 支援需求分页（demand paging）时，原因应该很明显。若是预取被允许横跨分页边界，存取可能会触发一个事件，以令分页能够被取得。这本身可能很糟，尤其是对效能而言。更糟的是预取器并不知道程序或操作系统本身的语义（semantic）。它可能因此预取实际上永远不会被请求的分页。如此意味著预取器会运行超过处理器曾以可识别模式存取过的memory区域尽头。这不只可能，而且非常有可能。若是处理器 –– 作为一次预取的一个副作用 –– 触发对这样的分页的请求，操作系统甚至可能会在这种请求永远也不会发生时完全扔掉它的追踪纪录。 因此重要的是认识到，无论预取器在预测模式上有多厉害，程序也会在分页边界上历经cache错失，除非它明确地从新的分页预取或是读取。这是如 6.2 节描述的最佳化资料布局、以借由将不相关的资料排除在外来最小化cache污染的另一个理由。 由于这个分页限制，处理器现今并没有非常复杂的逻辑来识别预取模式。以仍占主导地位的 4k 分页大小而言，有意义的也就这么多。这些年来已经提高识别步伐的地址范围，但超过现今经常使用的 512 位元组窗格（window）可能没太大意义。目前的预取单元并不认得非线性的存取模式。这种模式较有可能是真的随机、或者至少足够不重复到令试著识别它们不具意义。 若是硬件预取被意外地触发，能做的只有这么多。一个可能是试著找出这个问题，并稍微改变资料与／或程序布局。这大概满困难的。可能有特殊的在地化（localized）解法，像是在 x86 与 x86-64 处理器上使用 ud2 指令35。这个无法自己执行的指令是在一条间接的跳跃指令后被使用；它被作为指令获取器（fetcher）的一个信号使用，表示处理器不应浪费精力解码接下来的memory，因为执行将会在一个不同的位置继续。不过，这是个非常特殊的情况。在大部分情况下，必须要忍受这个问题。 能够完全或部分地停用整个处理器的硬件预取。在 Intel 处理器上，一个特定模型暂存器（Model Specific Register，MSR）便用于此（IA32_MISC_ENABLE，在许多处理器上为位元 9；位元 19 只停用邻近cache行预取）。这在大多情况下必须发生在系统核心中，因为它是个特权操作。若是数据分析显示，执行于系统上的一个重要的应用程序因硬件cache而遭受频宽耗竭与过早的cache逐出，使用这个 MSR 是一种可能性。 35. 或是 non-instruction。这是推荐的未定义操作码。 ↩ "},"what-programmers-can-do/prefetching/software-prefetching.html":{"url":"what-programmers-can-do/prefetching/software-prefetching.html","title":"6.3.2. 软件预取","keywords":"","body":"6.3.2. 软件预取 硬件预取的优势在于不必调整程序。缺点如同方才描述的，存取模式必须很直观，而且预取无法横跨分页边界进行。因为这些原因，我们现在有更多可能性，软件预取它们之中最重要的。软件预取不需借由插入特殊的指令来修改原始码。某些编译器支援编译指示（pragma）以或多或少地自动插入预取指令。 在 x86 和 x86-64，intrinsic 函式会由编译器产生特殊的指令: #include enum _mm_hint { _MM_HINT_T0 = 3, _MM_HINT_T1 = 2, _MM_HINT_T2 = 1, _MM_HINT_NTA = 0 }; void _mm_prefetch(void *p, enum _mm_hint h); 程序能够在程序中的任何指标上使用 _mm_prefetch intrinsic 函式。许多处理器（当然包含所有 x86 与 x86-64 处理器）都会忽略无效指标产生的错误，这令程序开发者的生活好过非常多。若是被传递的指标指向合法的memory，会命令预取单元将资料载入到cache中，并且 –– 必要的话 –– 逐出其它资料。不必要的预取应该被确实地避免，因为这会降低cache的有效性，而且它会耗费memory频宽（在被逐出的cache行是脏的情况下，可能需要两个cache行的频宽）。 要与 _mm_prefetch 一起使用的不同提示（hint）是由实作定义的。这表示每个处理器版本能够（稍微）不同地实作它们。一般能说的是，_MM_HINT_T0 会为包含式cache将资料获取到所有cache层级，并为独占式cache获取到最低层级的cache。若是资料项目在较高层级的cache中，它会被载入到 L1d 中。_MM_HINT_T1 提示将资料拉进 L2 而非 L1d。若是有个 L3 cache，_MM_HINT_T2 能做到类似于此的事情。不过，这些是没怎么被明确指定的细节，需要对所使用的实际处理器进行验证。一般来说，若是资料在使用 _MM_HINT_T0 之后立刻被用到就没错。当然这要求 L1d cache大小要大得足以容纳所有被预取的资料。若是立即被使用的工作集大小太大，将所有东西预取到 L1d 就是个坏点子，而应该使用其它两种提示。 第四种提示，_MM_HINT_NTA 能够吩咐处理器特殊地对待预取的cache行。NTA 代表非暂存对齐（non-temporal aligned），我们已经在 6.1 节解释过。程序告诉处理器应该尽可能地避免以这个资料污染cache，因为资料只在一段很短的期间内会被使用。对于包含式cache实作，处理器因而能够在载入时避免将资料读取进较低层级的cache。当资料从 L1d 逐出时，资料不必被推进 L2 或更高层级的cache中，但能够直接写到memory中。可能有其它处理器设计师在给定这个提示时能够布署的其它手法。程序开发者必须谨慎地使用这个提示：若是目前的工作集大小太大，并强制逐出以 NTA 提示载入的cache行，就要重新从memory载入。 图 6.7：使用预取的平均，NPAD=31 图 6.7 显示使用现已熟悉的指标追逐框架（pointer chasing framework）的测试结果。串列是随机地被摆放在memory中的。与先前测试的不同之处在于，程序真的会在每个串列节点上花一些时间（大约 160 周期）。如同我们从图 3.15 的数据中学到的，一旦工作集大小大于最后一阶cache，程序的效能就会受到严重的影响。 我们现在能够试著在计算之前发出预取请求来改善这种状况。即，我们在回圈的每一轮预取一个新元素。串列中被预取的节点与正在处理的节点之间的距离必须被谨慎地选择。假定每个节点在 160 周期内被处理、并且我们必须预取两个cache行（NPAD=31），五个串列元素的距离便足够。 图 6.7 的结果显示预取确实有帮助。只要工作集大小不超过最后一阶cache的大小（这台机器拥有 512kB = 219B 的 L2），数字就是相同的。预取指令并不会增加能量测出来的额外负担。一旦超过 L2 大小，预取省下 50 到 60 周期之间，高达 8%。预取的使用无法隐藏任何损失，但它稍微有点帮助。 AMD 在它们 Opteron 产品线的 10h 家族实作另一个指令：prefetchw。在 Intel 这边迄今仍没有这个指令的等价物，也不能透过 intrinsic 使用。prefetchw 指令要求 CPU 将cache行预取到 L1 中，就如同其它预取指令一样。差异在于cache行会立即变成「M」状态。若是之后没有接著对cache行的写入，这将会是个不利之处。但若是有一或多次写入，它们将会被加速，因为写入操作不必改变cache状态 –– 其在cache行被预取时就被设好。这对于竞争的cache行尤为重要，其中在另一个处理器的cache中的cache行的一次普通的读取操作会先在两个cache中将状态改成「S」。 预取可能有比我们这里达到的微薄的 8% 还要更大的优势。但它是众所皆知地难以做得正确，尤其是在预期相同的二元档在各种各样的机器上都表现良好的情况。由 CPU 提供的效能计数器能够帮助程序开发者分析预取。能够被计数并取样的事件包含硬件预取、软件预取、有用的／使用的软件预取、在不同层级的cache错失、等等。在 7.1 节，我们将会介绍这些事件。这所有的计数器都是机器特有的。 在分析程序时，应该要先看看cache错失。找出大量cache错失来源的所在时，应该试著针对碰上问题的memory存取加上预取指令。这应该一次处理一个地方。每次修改的结果应该借由观察量测有用预取指令的效能计数器来检验。若是那些计数器没有提升，那么预取可能是错的，它并没有给予足够的时间来从memory载入，或者预取从cache逐出仍然需要的memory。 gcc 现今能够在唯一一种情况下自动发出预取指令。若是一个回圈叠代在一个阵列上，能够使用下面的选项： -fprefetch-loop-arrays 编译器会计算出预取是否合理，以及 –– 如果是的话 –– 它应该往前看多远。对小阵列而言，这可能是个不利之处，而且若是在编译期不知道阵列的大小的话，结果可能更糟。gcc 手册提醒道，这个好处极为仰赖于程序码的形式，而在某些情况下，程序可能真的会跑得比较慢。程序开发者必须谨慎地使用这个选项。 "},"what-programmers-can-do/prefetching/special-kind-of-prefetch-speculation.html":{"url":"what-programmers-can-do/prefetching/special-kind-of-prefetch-speculation.html","title":"6.3.3. 特殊的预取类型：猜测","keywords":"","body":"6.3.3. 特殊的预取类型：猜测 一个现代处理器的 OoO 执行能力允许在不与彼此冲突的情况下搬移指令。举例来说（这次使用 IA-64 为例）： st8 [r4] = 12 add r5 = r6, r7;; st8 [r18] = r5 这段程序序列将 12 储存至由暂存器 r4 指定的地址、将 r6 与 r7 暂存器的内容相加、并将它储存在暂存器 r5 中。最后，它将总和储存至由暂存器 r18 指定的地址。这里的重点在于，加法指令能够在第一个 st8 指令之前 –– 或者同时 –– 执行，因为并没有资料的依赖关系。但假如必须载入其中一个加数会怎么样呢？ st8 [r4] = 12 ld8 r6 = [r8];; add r5 = r6, r7;; st8 [r18] = r5 额外的 ld8 指令将值载入到由 r8 指令的地址。在这个载入指令与接下来的 add 指令之间有个明确的资料依赖关系（这便是指令后面的 ;; 的理由，感谢提问）。这里的关键在于，新的 ld8 指令 –– 不若 add 指令 –– 无法被移到第一个 st8 前面。处理器无法在指令解码的期间足够快速地决定储存与载入是否冲突 –– 即，r4 与 r8 是否可能有相同的值。假如它们有相同的值，st8 指令会决定载入到 r6 的值。更糟的是，在载入错失cache的情况下，ld8 可能也会随之带来漫长的等待时间。IA 64 架构针对这种情况支援猜测式载入（speculative load）： ld8.a r6 = [r8];; [... other instructions ...] st8 [r4] = 12 ld8.c.clr r6 = [r8];; add r5 = r6, r7;; st8 [r18] = r5 新的 ld8.a 与 ld8.c.clr 指令是一对的，并取代前一段程序序列的 ld8 指令。ld8.a 为猜测式载入。这个值无法被直接使用，但处理器能开始运作。这时，当到达 ld8.c.clr 指令的时候，这个内容可能已经被载入（假定这个间隔中有足够数量的指令）。这个指令的引数（argument）必须与 ld8.a 指令相符。若是前面的 st8 指令没有覆写这个值（即 r4 与 r8 相同译注），就什么也不必做。猜测式载入做它的工作，而载入的等待时间被隐藏。若是载入与储存冲突，ld8.c.clr 会重新从memory载入值，而我们最终会得到一个正常的 ld8 指令的语义。 猜测式载入（仍？）没有被广泛使用。但如同这个例子所显示的，它是个非常简单而有效的隐藏等待时间的方法。预取基本上是等同的东西，并且对有著少量暂存器的处理器而言，猜测式载入可能没多大意义。猜测式载入有直接将值载入到暂存器中，而不载入到可能会被再次逐出的cache行（举例来说，当执行绪被移出排程〔deschedule〕的时候）这个（有时很大的）优点。如果能够使用猜测的话，应该要使用它。 译注. r4 与 r8 相同指的是「值会被覆写的情况」。 ↩ "},"what-programmers-can-do/prefetching/helper-threads.html":{"url":"what-programmers-can-do/prefetching/helper-threads.html","title":"6.3.4. 辅助执行绪","keywords":"","body":"6.3.4. 辅助执行绪 在尝试使用软件预取时，往往会碰到程序复杂度的问题。若是程序必须迭代于一个资料结构上（在我们的情况中是个串列），必须在同个回圈中实作两个独立的迭代：执行作业的普通迭代、与往前看以使用预取的第二个迭代。这轻易地变得足够复杂到容易产生失误。 此外，决定要往前看多远是必要的。太短的话，memory将无法及时被载入。太远的话，刚载入的资料可能会被再一次逐出。另一个问题是，虽然它不会阻挡或等待memory载入，但预取指令很花时间。指令必须被解码，假如解码器太忙碌的话 –– 举例来说，由于良好撰写／产生的程序码 –– 这可能很明显。最后，回圈的程序大小会增加。这降低 L1i 的效率。若借由一次发出多个预取指令来试著避免部分成本，则会碰到显著的预取请求数的问题。 一个替代方法是完全独立地执行一般的操作与预取。这能使用两条普通的执行绪来进行。执行绪显然必须被排程，以令预取执行绪填充一个被两条执行绪存取的cache。有两个值得一提的特殊解法： 在相同的处理器核上使用 HT （见 3.3.4 节，Hyper-Threading）。在这种情况下，预取能够进入 L2（或者甚至是 L1d）。 使用比 SMT 执行绪「更愚笨的（dumber）」执行绪，其除预取与其它简单的操作之外什么也不做。这是个处理器厂商可能会探究的选项。 HT 的使用是尤其令人感兴趣的。如同我们已经在 3.3.4 节看到的，假如 HT 执行独立的程序码的话，cache的共享是个问题。反而，在一条执行绪被用作一条预取辅助执行绪（helper thread）时，这并不是个问题。与此相反，这是个令人渴望的结果，因为最低层级的cache被预载。此外，由于预取执行绪大多是空閒或者在等待memory，所以假如不必自己存取主memory的话，其余 HT 的一般操作并不会太受干扰。后者正好是预取辅助执行绪所预防的。 唯一棘手的部分是确保辅助执行绪不会往前跑得太远。它不能完全污染cache，以致最早被预取的值被再次逐出。在 Linux 上，使用 futex 系统呼叫 [7] 或是 –– 以稍微高一些的成本 –– 使用 POSIX 执行绪同步基本指令（primitive），是很容易做到同步的。 图 6.8：使用辅助执行绪的平均，NPAD=31 这个方法的好处能够在图 6.8 中看到。这是与图 6.7 中相同的测试，只不过加上额外的结果。新的测试建立一条额外的辅助执行绪，往前执行大约 100 个串列项目，并读取（不只预取）每个串列元素的所有cache行。在这种情况下，我们每个串列元素有两个cache行（在一台有著 64 位元组cache行大小的 32 位元机器上，NPAD=31）。 两条执行绪被排程在相同处理器核的两条 HT 上。测试机仅有一颗处理器核，但结果应该与多于一颗处理器核的结果大致相同。亲和性函式 –– 我们将会在 6.4.3 节介绍 –– 被用来将执行绪绑到合适的 HT 上。 要确定操作系统知道哪两个 (或更多) 处理器为 HT ，可以使用来自 libNUMA 的 NUMA_cpu_level_mask 介面（见附录 D）。 #include ssize_t NUMA_cpu_level_mask(size_t destsize, cpu_set_t *dest, size_t srcsize, const cpu_set_t*src, unsigned int level); 这个介面能用来决定透过cache与memory连结的 CPU 阶层架构。这里感兴趣的是对应于 HT 的一阶cache。为在两条 HT 上排程两条执行绪，能够使用 libNUMA 函式（为简洁起见，省略错误处理）： cpu_set_t self; NUMA_cpu_self_current_mask(sizeof(self), &self); cpu_set_t hts; NUMA_cpu_level_mask(sizeof(hts), &hts, sizeof(self), &self, 1); CPU_XOR(&hts, &hts, &self); 在执行这段程序之后，我们有两个 CPU 位元集。self 能用来设定目前执行绪的亲和性，而 hts 中的遮罩能被用来设定辅助执行绪的亲和性。这在理想上应该在执行绪被建立前发生。在 6.4.3 节，我们会介绍设定亲和性的介面。若是没有可用的 HT ，NUMA_cpu_level_mask 函式会回传 1。这能够用以作为避免这个最佳化的征兆。 这个基准测试的结果可能出乎意料（也可能不会）。若是工作集塞得进 L2，辅助执行绪的间接成本将效能降低 10% 到 60% 之间（主要在比较低的那端，再次忽略最小的工作集大小，杂讯太多）。这应该在预料之中，因为若是所有资料都已经在 L2 cache中，预取辅助执行绪仅仅使用系统资源，却没有对执行有所贡献。 不过，一旦不再足够的 L2 大小耗尽，情况就改变。预取辅助执行绪协助将执行时间降低大约 25%。我们仍旧看到一条上升的曲线，只不过是因为无法足够快速地处理预取。不过，主执行绪执行的算术操作与辅助执行绪的memory载入操作彼此互补。资源冲突是最小的，其导致这种相辅相成的结果。 这个测试的结果应该能够被转移到更多其它的情境。由于cache污染而经常无用的 HT ，在这些情境中表现出众，并且应该被善用。附录 D 介绍的 NUMA 函式库令执行绪兄弟的找寻非常容易（见这个附录中的范例）。若是函式库不可用，sys 档案系统令一支程序能够找出执行绪的兄弟（见表 5.3 的 thread_siblings 栏位）。一旦能够取得这个资讯，程序就必须定义执行绪的亲和性，然后以两种模式执行回圈：普通的操作与预取。被预取的memory总量应该视共享的cache大小而定。在这个例子中，L2 大小是有关的，程序能够使用 sysconf(_SC_LEVEL2_CACHE_SIZE) 来查询大小。辅助执行绪的进度是否必须被限制取决于程序。一般来说，最好确定有一些同步，因为排程细节可能会导致显著的效能降低。 "},"what-programmers-can-do/prefetching/direct-cache-access.html":{"url":"what-programmers-can-do/prefetching/direct-cache-access.html","title":"6.3.5. 直接cache存取","keywords":"","body":"6.3.5. 直接cache存取 在现代操作系统中，cache错失的一个来源是到来的资料流量的处理。像网卡卡（Network Interface Card，NIC）与硬盘控制器等现代硬件，能够在不涉及 CPU 的情况下，直接将接收或读取的资料写入到memory中。这对于我们现今拥有的装置的效能而言至关重要，但它也造成问题。假使有个从网络传入的封包：操作系统必须检查封包的标头（header）以决定要如何处理它。NIC 将封包摆进memory，然后通知处理器它的到来。处理器没有机会去预取资料，因为它并不知道资料将何时抵达，甚至可能不会确切知道它将会被存在哪。结果是在读取标头时的一次cache错失。 Intel 已经在它们的晶片组与 CPU 中加上技术以缓解这个问题 [14]。构想是将封包的资料填入将会被通知到来的封包的 CPU 的cache。封包的承载内容在这里并不重要，这个资料一般将会由更高阶的函数 –– 要不是在系统核心中、就是在使用者层级 –– 处理。封包标头被用来决定封包必须以什么方式处理，因此这个资料是立即所需的。 网络 I/O 硬件已有 DMA 以写入封包。这表示它直接地与潜在整合在北桥中的memory控制器进行沟通。memory控制器的另一边是通过 FSB 到处理器的介面（假设memory控制器没有被整合到 CPU 自身）。 (a) 启动 DMA (b) 执行 DMA 与 DCA 图 6.9：直接cache存取 直接cache存取（Direct Cache Access，DCA）背后的想法是，扩充 NIC 与memory控制器之间的通讯协议。在图 6.9 中，第一张图显示在一台有著南北桥的正规机器上的 DMA 传输的起始。NIC 被连接到南桥上（或作为其一部分）。它启动 DMA 存取，但提供关于封包标头的新资讯，其应该被推进处理器的cache中。 在第二步中，传统的行为仅会是以连结到memory的连线完成 DMA 传输。对于被设置 DCA 旗标的 DMA 传输，北桥会以特殊的、新的 DCA 旗标在 FSB 上同时送出资料。处理器一直窥探著 FSB，并且若是它认出 DCA 旗标，它会试著将寄给处理器的资料载入到最低阶cache中。事实上，DCA 旗标是个提示；处理器能够自由地忽略它。在 DMA 传输完成之后，会以信号通知处理器。 在处理封包时，操作系统必须先确定是哪种封包。若是 DCA 提示没有被忽略的话，操作系统必须执行、以识别封包的载入操作很有可能会命中cache。将每个封包数以百计个循环的节约，乘上每秒能处理的成千上万个封包，节省的加总量是个非常可观的数字，尤其在谈到等待时间的时候。 少了 I/O 硬件（在这个例子中为 NIC）、晶片组与 CPU 的整合，这种最佳化是不可能的。因此，假如需要这个技术的话，确保明智地挑选平台是必要的。 "},"what-programmers-can-do/multi-thread-optimizations.html":{"url":"what-programmers-can-do/multi-thread-optimizations.html","title":"6.4. 多执行绪最佳化","keywords":"","body":"6.4. 多执行绪最佳化 关于多执行绪，有三个cache使用的面向是很重要的： 并行（Concurrency） 最小处理（Atomicity） 频宽 这些面向也适用于多行程的情况，但因为多行程（大多数）是独立的，因此为它们最佳化并没有那么容易。可能的多行程最佳化是那些可用于多执行绪情况的子集。所以这里我们会专门讨论后者。 在这种前后文下，并行指的是在一次执行多于一条执行绪时，一个行程所历经的memory影响。执行绪的一个特性是它们全都共享相同的定址空间，因此全都能够存取相同的memory。在理想的情况下，执行绪所使用的memory区域在多数时候都是不同的。在这种情况下，那些执行绪仅稍许耦合（couple）（举例来说，共有的输入与／或输出）。若是多于一条执行绪使用相同的资料，就需要协调：这即是最小处理发挥作用的时候。最后，视机器架构而定，可用的memory与可用于处理器的处理器之间的总线频宽是有限的。我们将会在接下来的章节分别论及这三个面向 –– 虽然它们是紧密相连的。 "},"what-programmers-can-do/multi-thread-optimizations/concurrency-optimizations.html":{"url":"what-programmers-can-do/multi-thread-optimizations/concurrency-optimizations.html","title":"6.4.1. 并行最佳化","keywords":"","body":"6.4.1. 并行最佳化 一开始，我们将会在本节讨论两个个别的议题，其实际上需要对立的最佳化。一个多执行绪应用程序在一些它的执行绪中使用共有的资料。一般的cache最佳化要求将资料保存在一起，使得应用程序的memory使用量很小，从而最大化在任意时间塞得进cache的memory总量。译注1 不过，使用这个方法有个问题：若是多条执行绪写入到一个memory位置，每个相对应处理器核的 L1d 中的cache行必须处于「E」（独占）状态。这表示会送出许多的 RFO 讯息。在最糟的情况下，每次写入存取都会送出一个讯息。所以一个普通的写入将会突然变得非常昂贵。若是使用相同的memory位置，同步就是必须的（可能透过 atomic 操作译注3的使用，其会在下个章节讨论到）。不过，当所有执行绪都使用不同的memory位置、并且可能是独立的时候，问题也显而易见。 图 6.10：并行cache行存取的间接成本 图 6.10 显示这种「假共享（false sharing）」的结果。测试程序（显示于 A.3 节）建立若干执行绪，其除递增一个memory位置（5 亿次）外什么也不做。量测的时间是从程序启动、直到程序等待最后一条执行绪结束之后。执行绪被钉在独立的处理器上。机器拥有四个 P4 处理器。蓝色值表示被指派到每条执行绪的memory分配位在个别cache行上的执行时间。红色部分为执行绪的位置被移到仅一个cache行时出现的损失。 蓝色的量测（使用独立的cache行时所需的时间）与预期的相符。程序在无损失的情况下延展至多条执行绪。每个处理器都将它的cache行保存在它拥有的 L1d 中，而且没有频宽问题，因为不必读取太多程序码或资料（事实上，它们全都被cache）。量测的些微提升其实是系统的杂讯、和可能的一些预取影响（执行绪使用连续的cache行）。 使用唯一一个cache行所需的时间、以及每条执行绪一个个别的cache行所需的时间相除所计算出的量测的间接成本分别是 390%、734%、以及 1,147%。乍看之下，这些很大的数字可能很令人吃惊，但考虑到需要的cache交互影响，这应该很显而易见。已经完成写入到cache行之后，就从一个处理器的cache拉出cache行。译注2在任何给定的时刻，除了拥有cache行的处理器以外，所有处理器都会被延迟，无法做任何事。每个额外的处理器都会导致更多的延迟。 图 6.11：四个处理器核的间接成本 由于这些量测，清楚的是这种情况必须在程序中避免。考虑到巨大的损失，在许多情况下，这个问题是很显而易见的（至少，效能分析会显示程序位置），但有个使用现代硬件的陷阱。图 6.11 显示当程序执行在一台单一处理器节点具备四核的机器上（Intel Core 2 QX 6700）的等价量测。即使使用这个处理器的两个个别的 L2，测试案例也没有显示出任何可延展性的问题。当相同的cache行被使用超过一次时有些许的间接成本，但它并没有随著处理器核的数量增加。36若是用多于一个这种处理器，我们自然会看到类似于那些在图 6.10 中的结果。尽管越来越多多核处理器的使用，许多机器还是会继续使用多处理器。因此，正确的处理这种状况是很重要的，这可能意味著要在真实的 SMP 机器上测试程序。 有个针对这个问题的非常简单的「修正」：将每个变数摆在它们自己的cache行。这是与先前提到的发挥作用的最佳化的冲突之处，具体来说就是应用程序的memory使用量会增加许多。这是不能忍受的；因此有必要想出一个更聪明的解法。 需要确定哪些变数一次只会被唯一一条执行绪使用到，始终只有一条执行绪使用的那些变数、也可能是那些不时会被争夺的变数。针对这些情况的每一个的不同解法是可能而且有用的。以变数的区分来说，最基本的标准是：它们是否曾被写入过、以及这有多常发生。 不曾被写入、以及那些仅会被初始化一次的变数基本上是常数（constant）。由于仅有写入操作需要 RFO 讯息，因此能够被在cache中共享常数（「S」状态）。所以，不必特别处理这些变数；将它们归在一起很好。若是程序开发者正确地以 const 标记这些变数，工具链将会把这些变数从普通的变数移出到 .rodata（唯读资料）或 .data.rel.ro（重定位〔relocation〕后唯读） 资料段（section）。37不需其他特别的行为。若是出于某些理由，变数无法正确地以 const 标记，程序开发者能够借由将它们指派到一个特殊的资料段来影响它们的摆放。 当连结器构造出最后的二元档时，它首先会附加来自所有输入档、具有相同名称的资料段；那些资料段接著会以连结器脚本所决定的顺序排列。这表示，借由将所有基本上为常数、但没被这样标记的变数移到一个特殊的资料段，程序开发者便能够将那些变数全部群组在一起。它们之中不会有个经常被写入的变数。借由适当地对齐在这个资料段中的第一个变数，就可能保证不会发生假共享。假定这个小例子： int foo = 1; int bar __attribute__((section(\".data.ro\"))) = 2; int baz = 3; int xyzzy __attribute__((section(\".data.ro\"))) = 4; 假如被编译的话，这个输入档定义四个变数。有趣的部分是，变数 foo 与 baz、以及 bar 与 xyzzy 被各自群组在一起。少了那个属性定义，编译器就会以原始码中定义的顺序将四个变数全都分配在一个叫做 .data 的资料段中。38使用现有这段程序，变数 bar 与 xyzzy 会被放置在一个叫做 .data.ro 的资料段中。将这个资料段叫做 .data.ro 或多或少有些随意。一个 .data. 的前缀保证 GNU 连结器会将这个资料段与其它资料段摆在一起。 相同的技术能被用于分离出主要是读取、但偶尔也会被写入的变数。只要选择一个不同的资料段名称就可以。在某些像是 Linux 系统核心的情况中，这种分离看起来很合理。 若是一个变数永远仅会被一条执行绪用到的话，有另一个指定变数的方式。在这种情况下，使用执行绪区域变数（thread-local variable）是可能而且有用的（见 [8]）。gcc 中的 C 与 C++ 语言允许使用 __thread 关键字将变数定义为各条执行绪的。 int foo = 1; __thread int bar = 2; int baz = 3; __thread int xyzzy = 4; 变数 bar 与 xyzzy 并非被分配在普通的资料段中；而是每条执行绪拥有它自己的、储存这种变数的分离区域。这些变数能够拥有静态初始子（static initializer）。所有执行绪区域变数都能够被所有其它的执行绪定址，但除非一条执行绪将执行绪区域变数的指标传递给那些其它的执行绪，其它执行绪也没法找到这个变数。由于变数为执行绪区域的，假共享就不是个问题 –– 除非程序人为地造成问题。这个解法很容易设置（编译器与连结器做了所有的事），但它有它的成本。当建立执行绪时，它必须花上一些时间来设置执行绪区域变数，这需要时间与memory。此外，定址执行绪区域变数通常比使用全域或自动变数更昂贵（如何自动地将成本最小化 –– 如果可能的话 –– 的解释见 [8]）。 另一个使用执行绪区域储存区（thread-local storage，TLS）的缺点是，假如变数的使用转移给另一条执行绪，在旧执行绪的目前值是无法被新执行绪取得的。每条执行绪的变数副本都是不同的。通常这根本不是问题，但假如是的话，转移到新的执行绪就需要协调，能够在这个时刻复制目前值。 一个更大的问题是可能浪费资源。假如在任何时候都仅有一条执行绪会使用这个变数，所有执行绪都必须付出memory的代价。若是一条执行绪不使用任何 TLS 变数的话，TLS memory区域的惰性分配（lazy allocation）会防止它成为问题（除了在应用程序本身的 TLS）。若是一条执行绪仅在 DSO 中使用一个 TLS 变数，所有在这个物件中的其它 TLS 变数也都会被分配memory。假如大规模地使用 TLS，这可能会潜在地累加。 一般来说，可以给出的最好的建议是 至少分离唯读（初始化之后）与读写变数。可能将这种分离扩展到，以主要是读取的变数作为第三种类别。 将一起用到的读写变数一起群组在一个结构中。使用结构，是确保在某种程度上，被所有 gcc 版本一致翻译成，所有那些变数的memory区域都紧靠在一起的唯一方法。 将经常被不同执行绪写入的读写变数移到它们自己的cache行。这可能代表要在末端加上填充，以填满cache行的剩馀部分。若是结合步骤 2，这经常不是真的浪费。扩展上面的例子，我们可能会产生下列程序（假定 bar 与 xyzzy 要一起使用）： int foo = 1; int baz = 3; struct { struct al1 { int bar; int xyzzy; }; char pad[CLSIZE sizeof(struct al1)]; } rwstruct __attribute__((aligned(CLSIZE))) = { { .bar = 2, .xyzzy = 4 } }; 某些程序的改变是必要的（bar 的参考必须被取代为 rwstruct.bar，xyzzy 亦同），但就这样。编译器与连结器会做完剩下的事情。39 若是一个变数被多条执行绪用到，但每次使用都是独立的，则将变数移入 TLS。 译注1. 因为cache的最小单位为cache行。因此若是资料摆在一起，代表它们所占用的cache行数量较少，因此一次能cache的资料量就变多。 ↩ 译注2. 因为所有执行绪写入的资料都在同个cache行内。因此刚写入的cache行立刻就会因为其它执行绪也要对相同的cache行进行写入，而变为「I（无效）」状态。 ↩ 译注3. 原子的英文 \"atom\" 源于希腊文 ἄτομος (拉丁转写为 atomos)，意思是「不可分割的单位」，十五世纪晚期 atomos 这词去除后缀转写为现代英语，成为 atom。电脑科学进一步借用 atomic 一词来表示「不可再拆分的」，于是 \"atomic operation\" 寓意为「不可再拆分的执行步骤」，也就是「最小操作」，即某个动作执行时，中间没有办法分割。倘若我们将 atomic operation 翻译为「原子操作」，可能会让人联想到高科技或者核能 (nuclear)，但事实根本不是这个意思，于是这里保留原文。 ↩ 36. 我无法解释在四颗处理器核全都用上时的较低的数字，但它是能够重现的。 ↩ 37. 资料段，由它们的名字所识别，为一个 ELF 档案中包含程序与资料的 atomic 单元。 ↩ 38. 这并不受 ISO C 标准保证，但 gcc 是这么做的。 ↩ 39. 到目前为止，这段程序都必须在命令列以 -fms-extensions 编译。 ↩ "},"what-programmers-can-do/multi-thread-optimizations/atomicity-optimizations.html":{"url":"what-programmers-can-do/multi-thread-optimizations/atomicity-optimizations.html","title":"6.4.2. 最小操作的最佳化","keywords":"","body":"6.4.2. 最小操作的最佳化 假如多条执行绪同时修改相同的memory位置，处理器并不保证任何具体的结果。这是个为了避免在所有情况的 99.999% 中的不必要成本而做出的慎重决定。举例来说，若有个在「S」状态的memory位置、并且有两条执行绪同时必须增加它的值的时候，在从cache读出旧值以执行加法之前，执行管线不必等待cache行变为「E」状态。而是会读取目前cache中的值，并且一旦cache行变为「E」状态，新的值便会被写回去。若是在两条执行绪中的两次cache读取同时发生，结果并不如预期；其中一个加法会没有效果。 对于可能发生并行操作的情况，处理器提供 atomic 操作。举例来说，这些 atomic 操作可能在直到能以像是 atomic 地对memory位置进行加法的方式执行加法之前，不会读取旧值。除了等待其它处理器核之外，某些处理器甚至会将特定地址的 atomic 操作发给在主机板上的其它装置。这全都会令 atomic 操作变慢。 处理器厂商决定提供不同的一组 atomic 操作。早期的 RISC 处理器，与代表简化（reduced）的「R」相符，提供非常少的 atomic 操作，有时仅有一个 atomic 的位元设置与测试。40在光谱的另一端，我们有提供大量 atomic 操作的 x86 与 x86-64。普遍来说可用的 atomic 操作能够归纳成四类： 位元测试 这些操作 atomic 地设置或者清除一个位元，并回传一个代表位元先前是否被设置的状态。 载入锁定／条件储存（Load Lock/Store Conditional，LL/SC）41 LL/SC 操作成对使用，其中特殊的载入指令用以开始一个事务（transaction），而最后的储存仅会在这个位置没有在这段期间内被修改的情况才会成功。储存操作指出成功或失败，所以程序能够在必要时重复它的工作。 比较并交换（Compare-and-Swap，CAS 这是个三元（ternary）操作，仅在目前值与第三个参数值相同的时候，将一个以参数提供的值写入到一个地址中（第二个参数）； atomic 算术 这些操作仅在 x86 与 x86-64 可用，其能够在memory位置上执行算术与逻辑操作。这些处理器拥有对这些操作的非 atomic 版本的支援，但 RISC 架构则否。所以，怪不得它们的可用性是有限的。 一个处理器架构可能会支援 LL/SC 指令或 CAS 指令其一，不会两者都支援。两种方法基本上相同；它们能提供一样好的 atomic 算术操作实作，但看起来 CAS 是近来偏好的方法。其它所有的操作都能够间接地以它来实作。例如，一个 atomic 加法： int curval; int newval; do { curval = var; newval = curval + addend; } while (CAS(&var, curval, newval)); 呼叫 CAS 的结果指出操作是否成功。若是它回传失败（非零的值），回圈会再次执行、执行加法、并且再次尝试呼叫 CAS。这会重复到成功为止。这段程序值得注意的是，memory位置的地址必须以两个独立的指令来计算。42对于 LL/SC，程序看起来大致相同： int curval; int newval; do { curval = LL(var); newval = curval + addend; } while (SC(var, newval)); 这里我们必须使用一个特殊的载入指令（LL），而且我们不必将memory位置的目前值传递给 SC，因为处理器知道memory位置是否曾在这期间被修改过。 for (i = 0; i for (i = 0; i for (i = 0; i 1. 做加法并读取结果 2. 做加法并回传旧值 3. atomic 地以新值替换 图 6.12：在一个回圈中 atomic 递增 The big differentiators are x86 and x86-64, where we have the atomic operations and, here, it is important to select the proper atomic operation to achieve the best result. 图 6.12 显示实作一个 atomic 递增操作的三种方法。在 x86 与 x86-64 上，三种方法全都会产生不同的程序，而在其它的架构上，程序则可能完全相同。效能的差异很大。下面的表格显示由四条并行的执行绪进行 1 百万次递增的执行时间。程序使用 gcc 的内建函式（__sync_*） 1. Exchange Add 2. Add Fetch 3. CAS 0.23s 0.21s 0.73s 前两个数字很相近；我们看到回传旧值稍微快一点。重要的资讯在被强调的那一栏，使用 CAS 时的成本。毫不意外，它要昂贵许多。对此有诸多理由 有两个memory操作; CAS 操作本身比较复杂，甚至需要条件操作; 整个操作必须在一个回圈中完成，以防两个同时的存取造成一次 CAS 呼叫失败。 现在读者可能会问个问题：为什么有人会使用这种利用 CAS 的复杂、而且较长的程序？对此的回答是：复杂性通常会被隐藏。如同先前提过的，CAS 是横跨所有有趣架构的统一 atomic 操作。所以有些人认为，以 CAS 定义所有的 atomic 操作就足够。这令程序更为简单。但就如数字所显示的，这绝对不是最好的结果。CAS 解法的memory管理的间接成本很大。下面示意仅有两条执行绪的执行，每条在它们自己的处理器核上。 执行绪 #1 执行绪 #2 var cache状态 v = var 在 Proc 1 上为「E」 n = v + 1 v = var 在 Proc 1+2 上为「S」 CAS(var) n = v + 1 在 Proc 1 上为「E」 CAS(var) 在 Proc 2 上为「E」 我们看到，在这段很短的执行期间内，cache行状态至少改变三次；两次改变为 RFO。再加上，因为第二个 CAS 会失败，所以这条执行绪必须重复整个操作。在这个操作的期间，相同的情况可能会再度发生。 相比之下，在使用 atomic 算术操作时，处理器能够将执行加法（或者其它什么的）所需的载入与储存操作保持在一起。能够确保同时发出的cache行请求直到 atomic 操作完成前都会被阻挡。 因此，在范例中的每次回圈迭代至多会产生一次 RFO cache请求，就没有别的。 这所有的一切都意味著，在一个能够使用 atomic 算术与逻辑操作的层级定义机器抽象是很重要的。CAS 不该被普遍地用作统一的机制。 对于大多数处理器而言， atomic 操作本身一直是 atomic。对于不需要 atomic 的情况，只有借由提供完全独立的程序路径时，才能够避免这点。 This means more code, a conditional, and further jumps to direct execution appropriately. 对于 x86 与 x86-64，情况就不同：相同的指令能够以 atomic 与非 atomic 的方式使用。为了令它们 atomic 化，便对指令用上一个特殊的前缀：lock 前缀。假如在一个给定的情况下， atomic 需求是不必要的，这为 atomic 操作提供避免高成本的机会。例如，在函式库中，在需要时必须一直是执行绪安全（thread-safe）的程序就能够受益于此。没有撰写程序时所需的资讯，决策能够在执行期进行。技巧是跳过 lock 前缀。这个技巧适用于 x86 与 x86-64 允许以 lock 前缀的所有指令。 cmpl $0, multiple_threads je 1f lock 1: add $1, some_var 如果这段组合语言程序看起来很神秘，别担心，它很简单的。第一个指令检查一个变数是否为零。非零在这个情况中表示有多于一条执行中的执行绪。若是这个值为零，第二个指令就会跳到标籤 1。否则，就执行下一个指令。这就是狡猾的部分。若是 je 没有跳跃，add 指令便会以 lock 前缀执行。否则，它会在没有 lock 前缀的情况下执行。 增加像是条件跳跃这样一个潜在昂贵的操作（在分支预测错误的情况下是很昂贵的）看似事与愿违。确实可能如此：若是大多时候都有多条执行绪在执行中，效能会进一步降低，尤其在分支预测不正确的情况。但若是有许多仅有一条执行绪在使用中的情况，程序是明显比较快的。使用一个 if-then-else 构造的替代方法在两种情况下都会引入额外的非条件跳跃，这可能更慢。假定一次 atomic 操作花费大约 200 个周期，使用这个技巧（或是 if-then-else 区块）的交叉点是相当低的。这肯定是个要记在心上的技术。不幸的是，这表示无法使用 gcc 的 __sync_* 内建函式。 40. HP Parisc 仍然没有提供更多的操作... ↩ 41. 有些人会使用「链结（linked）」而非「锁定」，这是一样的。 ↩ 42. x86 与 x86-64 上的 CAS 操作码（opcode）能够避免第二次与后续迭代中的值的载入，但在这个平台上，我们能够用一个单一的加法操作码、以一个较简单的方式来撰写 atomic 加法。 ↩ "},"what-programmers-can-do/multi-thread-optimizations/bandwidth-considerations.html":{"url":"what-programmers-can-do/multi-thread-optimizations/bandwidth-considerations.html","title":"6.4.3. 频宽考量","keywords":"","body":"6.4.3. 频宽考量 当使用多条执行绪、并且它们不会因为在不同的处理器核上使用相同的cache行而造成cache争夺时，仍然会有潜在的问题。每个处理器拥有连接到与这个处理器上所有处理器核与 HT 共享的memory的最大频宽。取决于机器架构（如，图 2.1 中的那个），多核可能会共享连结到memory或北桥的相同的总线。 处理器核本身即便在完美的情况下全速运转，到memory的连线也无法在不等待的前提下满足所有载入与储存的请求。现在，将可用的频宽进一步以处理器核、HT、以及共享一条到北桥的连线的处理器的数量划分，平行突然变成一个大问题。有效率程序的效能可能会受限于可用的memory频宽。 图 3.32 显示增加处理器的 FSB 速度能帮上大忙。这就是为什么随著处理器核数量的成长，我们也会看到 FSB 速度上的提升。尽管如此，若是程序使用很大的工作集，并且被充分最佳化过，这也永远不会足够。程序开发者必须准备好识别由有限频宽所致的问题。 现代处理器的效能量测计数器能够观察到 FSB 的争夺。在 Core 2 处理器上，NUS_BNR_DRV 事件计算一颗处理器核因为总线尚未准备就绪而必须等待的周期数。这指出总线被重度使用，而且载入或储存至主memory要花费比平常还要更长的时间。Core 2 处理器支援更多事件，能够计算像 RFO 或一般的 FSB 使用率等特定的总线行为。在开发期间研究一个应用程序的可延展性的可能性的时候，后者可能会派上用场。若是总线使用率已接近 1.0，可延展性的机会是最小的。 若是识别出一个频宽问题，有几件能够做到的事情。它们有时候是矛盾的，所以某些实验可能是必要的。一个解法是去买更快的电脑，假如有什么可买的话。取得更多的 FSB 速度、更快的 RAM 模组、或许还有处理器本地的memory，大概 –– 而且很可能会 –– 有帮助。不过，这可能成本高昂。若是程序仅在一台机器（或少数几台机器）上需要，硬件的一次性开销可能会比重写程序的成本还低。不过一般来说，最好是对程序下手。 在最佳化程序码本身以避免cache错失之后，达到更好频宽使用率的唯一剩馀选项是将执行绪更妥善地放在可用的处理器核上。预设情况下，系统核心中的排程器会根据它自己的策略，将一条执行绪指派给一个处理器。将一条执行绪从一颗处理器核移到另一颗是被尽可能避免的。不过，排程器并不真的知道关于工作负载的任何事情。它能够从cache错失等收集资讯，但这在许多情况下并不是非常有帮助。 图 6.13：没效率的排程 一个可能导致很大的memory总线使用率的情况，是在两条执行绪被排程在不同的处理器（或是在不同cache区域的核）上、而且它们使用相同的资料集的时候。图 6.13 显示这种状况。处理器核 1 与 3 存取相同的资料（以相同颜色的存取指示与memory区域表示）。同样地，处理器核 2 与 4 存取相同的资料。但执行绪被排程在不同的处理器上。这表示每次资料集都必须要从memory读取两次。这种状况能够被更好地处理。 图 6.14：有效率的排程 在图 6.14 中，我们看到理想上来看应该要是怎么样。现在被使用的总cache大小减少，因为现在处理器核 1 与 2 以及 3 与 4 都在相同的资料上运作。资料集只需从memory读取一次。 这是个简单的例子，但透过扩充，它适用于许多情况。如同先前提过的，操作系统核心中的排程器对资料的使用并没有深刻的理解，所以程序开发者必须确保排程是被有效率地完成的。没有很多操作系统核心的介面可用于传达这个需求。事实上，只有一个：定义执行绪亲和性。 执行绪亲和性表示，将一条执行绪指派给一颗或多颗处理器核。排程器接著将会在决定在哪执行这条执行绪的时候，（只）在那些处理器核中选择。即使有其它閒置的处理器核，它们也不会被考虑。这听来可能像是个缺陷，但这是必须偿付的代价。假如太多执行绪排外地执行在一组处理器核上，剩馀的处理器核可能大多数都是閒置的，而除了改变亲和性之外就没什么能做的。预设情况下，执行绪能够执行在任一处理器核上。 有一些查询与改变一条执行绪的亲和性的介面： #define _GNU_SOURCE #include int sched_setaffinity(pid_t pid, size_t size, const cpu_set_t *cpuset); int sched_getaffinity(pid_t pid, size_t size, cpu_set_t *cpuset); 这两个介面必须要被用在单执行绪的程序上。pid 引数指定哪个行程的亲和性应该要被改变或测定。呼叫者显然需要适当的权限来做这件事。第二与第三个参数指定处理器核的位元遮罩。第一个函数需要填入位元遮罩，使得它能够设定亲和性。第二个函数以选择的执行绪的排程资讯来填充位元遮罩。这些介面都被宣告在 中。 cpu_set_t 型别也和一些操作与使用这个型别物件的巨集一同被定义在这个标头档中。 #define _GNU_SOURCE #include #define CPU_SETSIZE #define CPU_SET(cpu, cpusetp) #define CPU_CLR(cpu, cpusetp) #define CPU_ZERO(cpusetp) #define CPU_ISSET(cpu, cpusetp) #define CPU_COUNT(cpusetp) CPU_SETSIZE 指定有多少 CPU 能够以这个资料结构表示。其它三个巨集运用 cpu_set_t 物件。要初始化一个物件，应该使用 CPU_ZERO；其它两个巨集应该用以选择或取消选择个别的处理器核。CPU_ISSET 测试一个指定的处理器是否为集合的一部分。CPU_COUNT 回传集合中被选择的处理器核数量。cpu_set_t 型别为 CPU 数量的上限提供一个合理的预设值。随著时间推移，肯定会证实它太小；在这个时间点，这个型别将会被调整。这表示程序必须一直将这个大小放在心上。上述的便利巨集根据 cpu_set_t 的定义，隐式地处理这个大小。若是需要更动态的大小管理，应该使用一组扩充的巨集： #define _GNU_SOURCE #include #define CPU_SET_S(cpu, setsize, cpusetp) #define CPU_CLR_S(cpu, setsize, cpusetp) #define CPU_ZERO_S(setsize, cpusetp) #define CPU_ISSET_S(cpu, setsize, cpusetp) #define CPU_COUNT_S(setsize, cpusetp) 这些介面接收一个对应于这个大小的额外参数。为了能够分配动态大小的 CPU 集，提供三个巨集： #define _GNU_SOURCE #include #define CPU_ALLOC_SIZE(count) #define CPU_ALLOC(count) #define CPU_FREE(cpuset) CPU_ALLOC_SIZE 巨集的回传值为，必须为一个能够处理 CPU 计数的 cpu_set_t 结构而分配的位元组数量。为了分配这种区块，能够使用 CPU_ALLOC 巨集。以这种方式分配的memory必须使用一次 CPU_FREE 的呼叫来释放。这些巨集可能会在背后使用 malloc 与 free，但并不是非得要维持这种方式。 最后，定义了一些对 CPU 集物件的操作： #define _GNU_SOURCE #include #define CPU_EQUAL(cpuset1, cpuset2) #define CPU_AND(destset, cpuset1, cpuset2) #define CPU_OR(destset, cpuset1, cpuset2) #define CPU_XOR(destset, cpuset1, cpuset2) #define CPU_EQUAL_S(setsize, cpuset1, cpuset2) #define CPU_AND_S(setsize, destset, cpuset1, cpuset2) #define CPU_OR_S(setsize, destset, cpuset1, cpuset2) #define CPU_XOR_S(setsize, destset, cpuset1, cpuset2) 这两组四个巨集的集合能够检查两个集合的相等性，以及对集合执行逻辑 AND、OR、与 XOR 操作。这些操作在使用一些 libNUMA 函数（见附录 D）的时候会派上用场。 一个行程能够使用 sched_getcpu 介面来确定它目前跑在哪个处理器上： #define _GNU_SOURCE #include int sched_getcpu(void); 结果为 CPU 在 CPU 集中的索引。由于排程的本质，这个数字并不总是 100% 正确。在回传结果的时间、与执行绪回到使用者层级的时间之间，执行绪可能已经被移到一个不同的 CPU 上。程序必须总是将这种不正确的可能性纳入考量。在任何情况下，更为重要的是被允许执行执行绪的那组 CPU。这个集合能够使用 sched_getaffinity 来查询。集合会被子执行绪与行程继承。执行绪不能指望集合在生命周期中是稳定的。亲和性遮罩能够从外界设置（见上面原型中的 pid 参数）；Linux 也支援 CPU 热插拔（hot-plugging），这意味著 CPU 能够从系统中消失 –– 因此，也能从亲和 CPU 集消失。 在多执行绪程序中，个别的执行绪并没有如 POSIX 定义的正式行程 ID，因此无法使用上面两个函数。作为替代， 宣告四个不同的介面： #define _GNU_SOURCE #include int pthread_setaffinity_np(pthread_t th, size_t size, const cpu_set_t *cpuset); int pthread_getaffinity_np(pthread_t th, size_t size, cpu_set_t *cpuset); int pthread_attr_setaffinity_np( pthread_attr_t *at, size_t size, const cpu_set_t *cpuset); int pthread_attr_getaffinity_np( pthread_attr_t *at, size_t size, cpu_set_t *cpuset); 前两个介面基本上与我们已经看过的那两个相同，除了它们在第一个参数拿的是一个执行绪的控制柄（handle），而非一个行程 ID。这能够定址在一个行程中的个别执行绪。这也代表这些介面无法在另一个行程使用，它们完全是为了行程内部使用的。第三与第四个介面使用一个执行绪属性。这些属性是在建立一条新的执行绪的时候使用的。借由设置属性，一条执行绪能够在开始时就被排程在一个特定的 CPU 集合上。这么早选择目标处理器 –– 而非在执行绪已经启动之后 –– 能够在许多不同层面上受益，包含（而且尤其是）memory分配（见 6.5 节的 NUMA）。 说到 NUMA，亲和性介面在 NUMA 程序设计中也扮演著一个重要的角色。我们不久后就会回到这个案例。 目前为止，我们已经谈过两条执行绪的工作集重叠、使得在相同处理器核上拥有两条执行绪是合理的情况。反之亦然。假如两条执行绪在个别的资料集上运作，将它们排程在相同的处理器核上可能是个问题。两条执行绪为相同的cache斗争，因而相互减少cache的有效使用。其次，两个资料集都得被载入到相同的cache中；实际上，这增加必须载入的资料总量，因此可用的频宽被砍半。 在这种情况中的解法是，设置执行绪的亲和性，使得它们无法被排程在相同的处理器核上。这与先前的情况相反，所以在做出任何改变之前，理解试著要最佳化的情况是很重要的。 针对cache共享最佳化以最佳化频宽，实际上是将会在下一节涵盖的 NUMA 程序设计的一个面相。只要将「memory」的概念扩充至cache。一旦cache的层级数增加，这会变得越来越重要。基于这个理由，NUMA 支援函式库中提供一个多核排程的解决方法。在不写死系统细节、或是钻入 /sys 档案系统的深度的前提下，决定亲和性遮罩的方法请参阅附录 D 中的程序例子。 "},"what-programmers-can-do/numa-programming.html":{"url":"what-programmers-can-do/numa-programming.html","title":"6.5. NUMA 程序设计","keywords":"","body":"6.5. NUMA 程序设计 以 NUMA 程序设计而言，目前为止说过的有关cache最佳化的所有东西也都适用。差异仅在这个层级以下才开始。NUMA 在存取定址空间的不同部分时引入不同的成本。使用均匀memory存取的话，我们能够最佳化以最小化分页错误（见 7.5 节），但这是对它而言。所有建立的分页都是均等的。 NUMA 改变这点。存取成本可能取决于被存取的分页。存取成本的差异也增加针对memory分页的局部性进行最佳化的重要性。NUMA 对于大多 SMP 机器而言都是无可避免的，因为有著 CSI 的 Intel（for x86, x86-64, and IA-64）与 AMD（for Opteron）都会使用它。随著每个处理器核数量的增加，我们很可能会看到被使用的 SMP 系统急遽减少（至少除了资料中心与有著非常高 CPU 使用率需求的人们的办公室之外）。大多家用机器仅有一个处理器就很好，因此没有 NUMA 的问题。但这一来不是说程序开发者可忽略 NUMA，再者也不表示没有相关的问题。 假如理解 NUMA 的一般化的话，也能快速意识到拓展至处理器cache的概念。在处理器核上使用相同cache的两条执行绪，会合作得比在处理器核上不共享cache的执行绪还快。这不是个杜撰的状况： 早期的双核处理器没有 L2 共享。 举例来说，Intel 的 Core 2 QX 6700 与 QX 6800 四核晶片拥有两个独立的 L2 cache。 正如早先猜测的，由于一片晶片上的更多核、以及统一cache的渴望，我们将会有更多层的cache。 cache形成它们自己的阶层结构；执行绪在核上的摆放，对于许多cache的共享（或者没有）来说变得很重要。这与 NUMA 面对的问题并没有很大的不同，因此能够统一这两个概念。即使是只对非 SMP 机器感兴趣的人也该读一读本节。 在 5.3 节中，我们已经看到 Linux 系统核心提供许多在 NUMA 程序设计中有用 –– 而且需要 –– 的资讯。不过，收集这些资讯并没有这么简单。以这个目的而言，目前在 Linux 上可用的 NUMA 函式库是完全不足的。一个更为合适的版本正由本作者建造中。 现有的 NUMA 函式库，libnuma –– numactl 套件（package）的一部分 –– 并不提供对系统架构资讯的存取。它仅是一个对可用系统呼叫的包装（wrapper）、与针对常用操作的一些方便的介面。现今在 Linux 上可用的系统呼叫为： mbind 选择指定memory分页的连结（binding）。 set_mempolicy 设定预设的memory连结策略。 get_mempolicy 取得预设的memory连结策略。 migrate_pages 将一组给定节点上的一个行程的所有分页迁移到一组不同的节点上。 move_pages 将选择的分页移到给定的节点、或是请求关于分页的节点资讯。 这些介面被宣告在与 libnuma 函式库一起出现的 标头档中。在我们深入更多细节之前，我们必须理解memory策略的概念。 "},"what-programmers-can-do/numa-programming/memory-policy.html":{"url":"what-programmers-can-do/numa-programming/memory-policy.html","title":"6.5.1. memory策略","keywords":"","body":"6.5.1. memory策略 定义一个memory策略背后的构想是，令现有的程序在不大幅度修改的情况下，能够在一个 NUMA 环境中适度良好地运作。策略由子行程继承，这使得我们能够使用 numactl 工具。这个工具的用途之一是能够用来以给定的策略来启动一支程序。 Linux 系统核心支援下列策略： MPOL_BIND memory只会从一组给定的节点分配。假如不能做到，则分配失败。 MPOL_PREFERRED memory最好是从一组给定的节点分配。若是这失败，才考虑来自其它节点的memory。 MPOL_INTERLEAVE memory是平等地从指定的节点分配。节点要不是针对基于 VMA 的策略，以虚拟memory区域中的偏移量来选择、就是针对基于任务（task）的策略，透过自由执行的计数器来选择。 MPOL_DEFAULT 根据memory区域的预设值来选择分配方式。 图 6.15：memory策略阶层结构 这份清单似乎递回地定义策略。这对了一半。事实上，memory策略形成一个阶层结构（见图 6.15）。若是一个地址被一个 VMA 策略所涵盖，就会使用这个策略。一种特殊的策略被用在共享的memory区段上。假如没有针对特定地址的策略，就会使用任务的策略。若是连这也没有，便使用系统的预设策略。 系统预设是分配请求memory的那条执行绪本地的memory。预设不会提供任务与 VMA 策略。对于一个有著多条执行绪的行程，本地节点为首先执行行程的「家」节点。上面提到的系统呼叫能够用来选择不同的策略。 "},"what-programmers-can-do/numa-programming/specifying-policies.html":{"url":"what-programmers-can-do/numa-programming/specifying-policies.html","title":"6.5.2. 指定策略","keywords":"","body":"6.5.2. 指定策略 set_mempolicy 呼叫能够用以为目前的执行绪（对系统核心来说的任务）设定任务策略。仅有目前的执行绪会受影响，而非整个行程。 #include long set_mempolicy(int mode, unsigned long *nodemask, unsigned long maxnode); mode 参数必须为前一节介绍过的其中一个 MPOL_* 常数。nodemask 参数指定未来分配要使用的memory节点，而 maxnode 为 nodemask 中的节点（即位元）数量。若是 mode 为 MPOL_DEFAULT，就不需要指定memory节点，并且会忽略 nodemask 参数。若是为 MPOL_PREFERRED 传递一个空指标作为 nodemask，则会选择本地节点。否则，MPOL_PREFERRED 会使用 nodemask 中设置的位元所对应的最低的节点编号。 对于已经分配的memory，设定策略并没有任何影响。分页不会被自动迁移；只有未来的分配会受影响。注意到memory分配与预留定址空间之间的不同：一个使用 mmap 建立的定址空间区域通常不会被自动分配。在memory区域上首次的读取或写入操作会分配合适的分页。若是策略在存取相同定址空间区域的不同分页之间发生改变，或者策略允许memory的分配来自不同节点，那么一个看似均匀的定址空间区域可能会被分散在许多memory节点之中。 "},"what-programmers-can-do/numa-programming/swapping-and-policies.html":{"url":"what-programmers-can-do/numa-programming/swapping-and-policies.html","title":"6.5.3. 置换与策略","keywords":"","body":"6.5.3. 置换与策略 若是实体memory耗尽，系统就必须丢弃干净的分页，并将脏的分页储存到置换空间（swap）中。Linux 的置换实作会在将分页写入置换空间的时候丢弃节点资讯。这表示当分页被重复使用并载入分页（page in）时，将会从头开始选择被使用的节点。执行绪的策略很可能会导致一个靠近执行中处理器的节点被选到，但这个节点可能跟先前使用的节点不同。 这种变换的关联（association）意味著节点关联无法借由一支程序被储存为分页的一个属性。关联可能会随著时间改变。对于与其它行程共享的分页，这也可能会因为一个行程的请求而发生（见下面 mbind 的讨论）。系统核心本身能够在一个节点耗尽空间、而其它节点仍有閒置空间的时候迁移分页。 任何使用者层级程序得知的节点关联因而只能在一段很短的时间内为真。它比起纯粹的资讯，更像是一个提示。每当需要精确的消息时，应该使用 get_mempolicy 介面（见 6.5.5 节）。 "},"what-programmers-can-do/numa-programming/vma-policy.html":{"url":"what-programmers-can-do/numa-programming/vma-policy.html","title":"6.5.4. VMA 策略","keywords":"","body":"6.5.4. VMA 策略 要为一个地址范围设定 VMA 策略，必须使用一个不同的介面： #include long mbind(void *start, unsigned long len, int mode, unsigned long *nodemask, unsigned long maxnode, unsigned flags); 这个介面为地址范围 [start, start + len) 注册一个新的 VMA 策略。由于memory管理是在分页上操作，所以起始地址必须是对齐分页的。len 值会被无条件进位至下一个分页大小。 mode 参数再次指定策略；这个值必须从 6.5.1 节的清单中挑选。与使用 set_mempolicy 相同，nodemask 参数只会用在某些策略上。它的处理是一样的。 mbind 介面的语义取决于 flags 参数的值。预设情况下，若是 flags 为零，系统呼叫会为这个地址范围设定 VMA 策略。现有的对映不受影响。假如这还不够，目前有三种修改这种行为的旗标；它们能够被单独或者一起被选择： MPOL_MF_STRICT 假如并非所有分页都在由 nodemask 指定的节点上，对 mbind 的呼叫将会失败。在这个旗标与 MPOL_MF_MOVE 和／或 MPOL_MF_MOVEALL 一起使用的情况下，呼叫会在任何分页无法被移动的时候失败。 MPOL_MF_MOVE 系统核心将会试著移动地址范围中、任何分配在一个不在由 nodemask 指定的集合中的节点上的分页。预设情况下，仅有被目前行程的分页表专用的分页会被移动。 MPOL_MF_MOVEALL 如同 MPOL_MF_MOVE，但系统核心会试著移动所有分页，而非仅有那些独自被目前行程的分页表使用的分页。这个操作具有系统层面的影响，因为它也影响其它 –– 可能不是由相同使用者所拥有的 –– 行程的memory存取。因此 MPOL_MF_MOVEALL 是个特权操作（需要 CAP_NICE 的能力）。 注意到针对 MPOL_MF_MOVE 与 MPOL_MF_MOVEALL 的支援仅在 2.6.16 Linux 系统核心中才被加入。 在没有任何旗标的情况下呼叫 mbind，在任何分页真的被分配之前必须为一个新预留的地址范围指定策略的时候是最有用的。 void *p = mmap(NULL, len, PROT_READ|PROT_WRITE, MAP_ANON, -1, 0); if (p != MAP_FAILED) mbind(p, len, mode, nodemask, maxnode, 0); 这段程序序列保留一段 len 位元组的定址空间范围，并指定应该使用指涉 nodemask 中的memory节点的策略 mode。除非 MAP_POPULATE 旗标与 mmap 一起使用，否则在 mbind 呼叫的时候并不会分配任何memory，因此新的策略会套用在这个定址空间区域中的所有分页。 单独的 MPOL_MF_STRICT 旗标能用来确定，传给 mbind 的 start 与 len 参数所描述的地址范围中的任何分页，是否都被分配在那些由 nodemask 指定的那些节点以外的节点上。已分配的分页不会被改变。若是所有分页都被分配在指定的节点上，那么定址空间区域的 VMA 策略会根据 mode 改变。 有时候是需要memory的重新平衡的。在这种情况下，可能必须将一个节点上分配的分页移到另一个节点上。以设置 MPOL_MF_MOVE 呼叫的 mbind 会尽最大努力来达成这点。仅有单独被行程的分页表树指涉到的分页会被考虑是否移动。可能有多个以执行绪或其他行程的形式共享部分分页表树的使用者。不可能影响碰巧映射相同资料的其它行程。这些分页并不共享分页表项目。 若是传递给 mbind 的 flags 参数中设置 MPOL_MF_STRICT 与 MPOL_MF_MOVE 位元，系统核心会试著移动并非分配在指定节点上的所有分页。假如这无法做到，这个呼叫将会失败。这种呼叫可能有助于确定是否有个节点（或是一组节点）能够容纳所有的分页。可以连续尝试多种组合，直到找到一个合适的节点。 除非执行目前的行程是这台电脑的主要目的，否则 MPOL_MF_MOVEALL 的使用是较难以证明为正当的。理由是，即使是出现在多张分页表的分页也会被移动。这会轻易地以负面的方式影响其它行程。因而应该要谨慎地使用这个操作。 "},"what-programmers-can-do/numa-programming/querying-node-information.html":{"url":"what-programmers-can-do/numa-programming/querying-node-information.html","title":"6.5.5. 查询节点资讯","keywords":"","body":"6.5.5. 查询节点资讯 get_mempolicy 介面能用以查询关于一个给定地址的 NUMA 状态的各种事实。 #include long get_mempolicy(int *policy, const unsigned long *nmask, unsigned long maxnode, void *addr, int flags); 当 get_mempolicy 以 0 作为 flags 参数呼叫时，关于地址 addr 的策略资讯会被储存在由 policy 指到的字组、以及由 nmask 指到的节点的位元遮罩中。若是 addr 落在一段已经被指定一个 VMA 策略的定址空间范围中，就回传关于这个策略的资讯。否则，将会回传关于任务策略或者 –– 必要的话 –– 系统预设策略的资讯。 若是 flags 中设定 MPOL_F_NODE、并且管理 addr 的策略为 MPOL_INTERLEAVE，那么 policy 所指到的字组为要进行下一次分配的节点索引。这个资讯能够潜在地用来设定打算要在新分配的memory上运作的一条执行绪的亲和性。这可能是实现逼近的一个比较不昂贵的方法，尤其是在执行绪仍未被建立的情况。 MPOL_F_ADDR 旗标能用来检索另一个完全不同的资料项目。假如使用这个旗标，policy 所指到的字组为已经为包含 addr 的分页分配memory的memory节点索引。这个资讯能用来决定可能的分页迁移、决定哪条执行绪能够最有效率地运作在memory位置上、还有更多更多的事情。 一条执行绪正在使用的 CPU –– 以及因此用到的memory节点 –– 比起它的memory分配还要更加变化无常。在没有明确请求的情况下，memory分页只会在极端的情况下被移动。一条执行绪能被指派给另一个 CPU，作为重新平衡 CPU 负载的结果。关于当前 CPU 与节点的资讯可能因而仅在短期内有效。排程器会试著将执行绪维持在同一个 CPU 上，甚至可能在相同的核上，以最小化由于冷cache（cold cache）造成的效能损失。这表示，查看当前 CPU 与节点的资讯是有用的；只要避免假设关联性不会改变。 libNUMA 提供两个介面，以查询一段给定虚拟定址空间范围的节点资讯： #include int NUMA_mem_get_node_idx(void *addr); int NUMA_mem_get_node_mask(void *addr, size_t size, size_t __destsize, memnode_set_t *dest); NUMA_mem_get_node_mask 根据管理策略，在 dest 中设置代表所有分配（或者可能分配）范围 [addr, addr+size) 中的分页的memory节点的位元。NUMA_mem_get_node 只看地址 addr，并回传分配（或者可能分配）这个地址的memory节点的索引。这些介面比 get_mempolicy 还容易使用，而且应该是首选。 当前正由一条执行绪使用的 CPU 能够使用 sched_getcpu 来查询（见 6.4.3 节）。使用这个资讯，一支程序能够使用来自 libNUMA 的 NUMA_cpu_to_memnode 介面来确定 CPU 本地的memory节点（们）： #include int NUMA_cpu_to_memnode(size_t cpusetsize, const cpu_set_t *cpuset, size_t memnodesize, memnode_set_t * memnodeset); 对这个函式的一次呼叫会设置所有对应于任何在第二个参数指到的集合中的 CPU 本地的memory节点的位元。就如同 CPU 资讯本身，这个资讯直到机器的配置改变（例如，CPU 被移除或新增）时才会是正确的。 memnode_set_t 物件中的位元能被用在像 get_mempolicy 这种低阶函式的呼叫上。使用 libNUMA 中的其它函式会更加方便。反向映射能透过下述函式取得： #include int NUMA_memnode_to_cpu(size_t memnodesize, const memnode_set_t * memnodeset, size_t cpusetsize, cpu_set_t *cpuset) 在产生的 cpuset 中设置的位元为任何在 memnodeset 中设置的位元所对应的memory节点本地的那些 CPU。对于这两个介面，程序开发者都必须意识到，资讯可能随著时间改变（尤其是使用 CPU 热插拔的情况）。在许多情境中，在输入的位元集中只会设置单一个位元，但举例来说，将 sched_getaffinity 呼叫检索到的整个 CPU 集合传递到 NUMA_cpu_to_memnode，以确定哪些memory节点能够被执行绪直接存取到，也是有意义的。 "},"what-programmers-can-do/numa-programming/cpu-and-node-sets.html":{"url":"what-programmers-can-do/numa-programming/cpu-and-node-sets.html","title":"6.5.6. CPU 与节点集合","keywords":"","body":"6.5.6. CPU 与节点集合 借由将程序改变为使用目前为止所描述的介面来为 SMP 与 NUMA 环境调整程序，在来源译注无法取得的情况下可能会极为昂贵（或者不可能）。此外，系统管理员可能想要对使用者和／或行程能够使用的资源施加限制。对于这些情境，Linux 系统核心支援所谓的 CPU 集。这个名称有一点误导，因为memory节点也被涵盖其中。它们也与 cpu_set_t 资料型别无关。 此刻，CPU 集的介面为一个特殊的档案系统。它通常没有被挂载（mount）（至少到目前为止）。这能够使用 mount -t cpuset none /dev/cpuset 改变。挂载点 /dev/cpuset 在这个时间点当然必须存在。这个目录的内容为预设（根）CPU 集的描述。它起初由所有的 CPU 与所有的memory节点所构成。这个目录中的 cpus 档案显示在 CPU 集中的 CPU、mems 档案显示memory节点、tasks 档案显示行程。 为了建立一个新的 CPU 集，只要在阶层结构中的某个地方建立一个新的目录。新的 CPU 集会继承来自父集合的所有设定。接著，新的 CPU 集的 CPU 与memory节点能够借由将新值写到在新目录中的 cpus 与 mems 虚拟档案来更改。 若是一个行程属于一个 CPU 集，CPU 与memory节点的设定会被用作亲和性与memory策略位元遮罩的遮罩。这表示，程序无法在亲和性遮罩里选择不在行程正在使用的 CPU 集（即，它在 tasks 档案中列出的位置）的 cpus 档案中的任何 CPU。对于memory策略的节点遮罩与 mems 档案也是类似的。 除非位元遮罩在遮罩后为空，否则程序不会经历任何错误，因此 CPU 集是一种控制程序执行的近乎无形的手段。这种方法在有著大量 CPU 与／或memory节点时是尤其有效率的。将一个行程移到一个新的 CPU 集，就跟将行程 ID 写到合适 CPU 集的 tasks 档案一样简单。 CPU 集的目录包含许多其它档案，能用来指定像是memory压力下、以及独占存取 CPU 与memory节点时的行为。感兴趣的读者请参阅系统核心原始码树中的档案 Documentation/cpusets.txt。 译注. 根据前后文猜测，这里的「来源」指的应该是程序使用的 CPU 与memory节点。 ↩ "},"what-programmers-can-do/numa-programming/explicit-numa-optimizations.html":{"url":"what-programmers-can-do/numa-programming/explicit-numa-optimizations.html","title":"6.5.7. 明确的 NUMA 最佳化","keywords":"","body":"6.5.7. 明确的 NUMA 最佳化 假如所有节点上的所有执行绪都需要存取相同的memory区域时，所有的本地memory与亲和性规则都无法帮上忙。当然，简单地将执行绪的数量限制为直接连接到memory节点的处理器所能支援的数量是可能的。不过，这并没有善用 SMP NUMA 机器，因此并不是个实际的选项。 若是所需的资料是唯读的，有个简单的解法：复制（replication）。每个节点能够得到它自己的资料副本，这样就不必进行节点之间的存取。这样做的程序码看起来可能像这样： void *local_data(void) { static void *data[NNODES]; int node = NUMA_memnode_self_current_idx(); if (node == -1) /* Cannot get node, pick one. */ node = 0; if (data[node] == NULL) data[node] = allocate_data(); return data[node]; } void worker(void) { void *data = local_data(); for (...) compute using data } 在这段程序中，函式 worker 借由一次 local_data 的呼叫来取得一个资料的本地副本的指标来进行准备。接著它继续执行使用这个指标的回圈。local_data 函式保存一个已经被分配的资料副本的列表。每个系统拥有有限的memory节点，所以带有各节点memory副本的指标的阵列大小是受限的。来自 libNUMA 的 NUMA_memnode_system_count 函式回传这个数字。若是给定节点的memory还没被分配给当前节点（由 data 在 NUMA_memnode_self_current_idx 呼叫所回传的索引位置的空指标来识别），就分配一个新的副本。 重要的是要意识到，如果在 getcpu 系统呼叫之后，执行绪被排程在另一个连接到不同memory节点的 CPU 上时，是不会发生什么可怕的事情的。43它只代表在 worker 中使用 data 变数的存取，存取另一个memory节点上的memory。这直到 data 被重新计算为止会拖慢程序，但就这样。系统核心总是会避免不必要的、每颗 CPU 执行伫列的重新平衡。若是这种转移发生，通常是为了一个很好的理由，并且在不久的未来不会再次发生。 当处理中的memory区域是可写入的，事情会更为复杂。在这种情况下，单纯的复制是行不通的。根据具体情况，或许有一些可能的解法。 举例来说，若是可写入的memory区域是用来累积（accumulate）结果的，首先为结果被累积的每个memory节点建立一个分离的区域。接著，当这项工作完成时，所有的每节点的memory区域会被结合以得到全体的结果。即使运作从不真的停止，这项技术也能行得通，但中介结果是必要的。这个方法的必要条件是，结果的累积是无状态的（stateless）。即，它不会依赖先前收集起来的结果。 不过，拥有对可写入memory区域的直接存取总是比较好的。若是对memory区域的存取数量很可观，那么强迫系统核心将处理中的memory分页迁移到本地节点可能是个好点子。若是存取的数量非常高，并且在不同节点上的写入并未同时发生，这可能有帮助。但要留意，系统核心无法产生奇蹟：分页迁移是一个复制操作，并且以此而论并不便宜。这个成本必须被分期偿还。 43. 使用者层级的 sched_getcpu 介面是使用 getcpu 系统呼叫来实作的。后者不该被直接使用，并且有一个不同的介面。 ↩ "},"what-programmers-can-do/numa-programming/utilizing-all-bandwidth.html":{"url":"what-programmers-can-do/numa-programming/utilizing-all-bandwidth.html","title":"6.5.8. 利用所有频宽","keywords":"","body":"6.5.8. 利用所有频宽 在图 5.4 中的数据显示，当cache无效时，对远端memory的存取并不显著慢于对本地memory的存取。这表示，一支程序也许能借著将它不必再次读取的资料写入到附属于另一个处理器的memory中来节省频宽。到 DRAM 模组的连线频宽与交互连线的频宽大多数是独立的，所以平行使用能提升整体效能。 这是否真的可能，取决于许多因素。必须确保cache无效，否则与远端存取相关的减慢是很显著的。另一个大问题是，远端节点是否有任何它所拥有的memory频宽的需求。在采用这个方法之前，必须详加检验这种可能性。理论上，使用一个处理器可用的所有频宽可能有正面影响。一个 10h Opteron 家族的处理器能够直接连接到高达四个其它的处理器。假如系统的其余部分合作的话，利用所有这种额外频宽，也许结合合适的预取（尤其是 prefetchw），可能致使改进。 "},"examples-and-benchmark-programs.html":{"url":"examples-and-benchmark-programs.html","title":"A. 范例与基准测试程序","keywords":"","body":"A. 范例与基准测试程序 "},"examples-and-benchmark-programs/matrix-multiplication.html":{"url":"examples-and-benchmark-programs/matrix-multiplication.html","title":"A.1 矩阵乘法","keywords":"","body":"A.1 矩阵乘法 这是在 6.2.1 节的矩阵乘法的完整基准测试程序。有关使用的 intrinsic 函式的细节，请读者参阅 Intel 的参考手册。 #include #include #include #define N 1000 double res[N][N] __attribute__ ((aligned (64))); double mul1[N][N] __attribute__ ((aligned (64))); double mul2[N][N] __attribute__ ((aligned (64))); #define SM (CLS / sizeof (double)) int main (void) { // ... Initialize mul1 and mul2 int i, i2, j, j2, k, k2; double *restrict rres; double *restrict rmul1; double *restrict rmul2; for (i = 0; i 回圈的结构跟 6.2.1 节的最终型态几乎完全相同。唯一的大改变是 rmul1[k2] 值的载入被拉出内部回圈了，因为我们必须建立一个拥有两个相同元素值的向量。这即是 _mm_unpacklo_pd() intrinsic 函式所为。 其余唯一值得注意的事情是，我们明确地对齐了三个阵列，以令我们预期会位在同个cache行的值真的在那里找到。 "},"bibliography.html":{"url":"bibliography.html","title":"参考书目","keywords":"","body":"参考书目 Performance Guidelines for AMD AthlonTM 64 and AMD OpteronTM ccNUMA Multiprocessor Systems. Advanced Micro Devices, June 2006. 5.4 Jennifer M. Anderson, Lance M. Berc, Jeffrey Dean, Sanjay Ghemawat, Monika R. Henzinger, Shun-Tak A. Leung, Richard L. Sites, Mark T. Vandevoorde, Carl A. Waldspurger, and William E. Weihl. Continuous profiling: Where have all the cycles gone. In Proceedings of the 16th ACM Symposium of Operating Systems Principles, pages 1–14, October 1997. 7.1 Vinodh Cuppu, Bruce Jacob, Brian Davis, and Trevor Mudge. High-Performance DRAMs in Workstation Environments. IEEE Transactions on Computers, 50(11):1133–1153, November 2001. 2.1.2, 2.2, 2.2.1, 2.2.3, 10 Arnaldo Carvalho de Melo. The 7 dwarves: debugging information beyond gdb. In Proceedings of the Linux Symposium, 2007. 6.2.1 Simon Doherty, David L. Detlefs, Lindsay Grove, Christine H. Flood, Victor Luchangco, Paul A. Martin, Mark Moir, Nir Shavit, and Jr. Guy L. Steele. DCAS is not a Silver Bullet for Nonblocking Algorithm Design. In SPAA ’04: Proceedings of the Sixteenth Annual ACM Symposium on Parallelism in Algorithms and Architectures, pages 216–224, New York, NY, USA, 2004. ACM Press. ISBN 1-58113-840-7. 8.1 M. Dowler. Introduction to DDR-2: The DDR Memory Replacement, May 2004. 2.2.1 Ulrich Drepper. Futexes Are Tricky, December 2005. URL http://people.redhat.com/drepper/futex.pdf. 6.3.4 Ulrich Drepper. ELF Handling For Thread-Local Storage. Technical report, Red Hat, Inc., 2003. URL http://people.redhat.com/drepper/tls.pdf. 6.4.1 Ulrich Drepper. Security Enhancements in Red Hat Enterprise Linux, 2004. URL http://people.redhat.com/drepper/nonselsec.pdf. 4.2 Dominique Fober, Yann Orlarey, and Stephane Letz. Lock-Free Techniques for Concurrent Access to Shared Objects. In GMEM, editor, Actes des Journes d’Informatique Musicale JIM2002, Marseille, pages 143–150, 2002. 8.1, 8.1 Joe Gebis and David Patterson. Embracing and Extending 20th-Century Instruction Set Architectures. Computer, 40(4):68–75, April 2007. 8.4 David Goldberg. What Every Computer Scientist Should Know About Floating-Point Arithmetic. ACM Computing Surveys, 23(1):5–48, March 1991. 1 Maurice Herlihy and J. Eliot B. Moss. Transactional memory: Architectural support for lock-free data structures. In Proceedings of 20th International Symposium on Computer Architecture, 1993. 8.2, 8.2.2, 8.2.3, 8.2.4 Ram Huggahalli, Ravi Iyer, and Scott Tetrick. Direct Cache Access for High Bandwidth Network I/O, 2005. 6.3.5 Intel R 64 and IA-32 Architectures Optimization Reference Manual. Intel Corporation, May 2007. B.3 William Margo, Paul Petersen, and Sanjiv Shah. Hyper-Threading Technology: Impact on Compute-Intensive Workloads. Intel Technology Journal, 6(1), 2002. URL ftp://download.intel.com/technology/itj/2002/volume06issue01/art06_computeintensive/vol6iss1_art06. 3.3.4 Caola ́n McNamara. Controlling symbol ordering. http://blogs.linux.ie/caolan/2007/04/24/controlling-symbol-ordering/, April 2007. 7.5 Double Data Rate (DDR) SDRAM MT46V. Micron Technology, 2003. Rev. L 6/06 EN. 2.2.2, 10 Jon “Hannibal” Stokes. Ars Technica RAM Guide, Part II: Asynchronous and Synchronous DRAM. http://arstechnica.com/paedia/r/ram_guide/ram_guide.part2-1.html, 2004. 2.2 Wikipedia. Static random access memory, 2006. 2.1.1 "}}